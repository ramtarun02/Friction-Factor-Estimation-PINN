{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.train' from '/Users/ramtarun/Desktop/Cambridge/Friction-Factor-Estimation-PINN/src/train.py'>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import scipy.io\n",
    "import torch\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import src.HyperParameters as hp \n",
    "import src.data as dt \n",
    "import src.model3 as Model \n",
    "import src.train as trainer\n",
    "\n",
    "importlib.reload(hp)\n",
    "importlib.reload(dt)\n",
    "importlib.reload(Model)\n",
    "importlib.reload(trainer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices:\n",
      "1. cpu\n",
      "2. tpu\n",
      "3. mps\n",
      "Selected device: mps\n"
     ]
    }
   ],
   "source": [
    "def list_available_devices():\n",
    "    devices = []\n",
    "    if torch.cuda.is_available():\n",
    "        devices.append('cuda')\n",
    "    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "        devices.append('multi-gpu')\n",
    "    if torch.cuda.is_available() and torch.cuda.nccl.version() > 0:\n",
    "        devices.append('nccl')\n",
    "    if torch.cuda.is_available() and torch.cuda.is_initialized():\n",
    "        devices.append('initialized')\n",
    "    if torch.cuda.is_available() and torch.cuda.memory_allocated() > 0:\n",
    "        devices.append('memory-allocated')\n",
    "    if torch.cuda.is_available() and torch.cuda.memory_reserved() > 0:\n",
    "        devices.append('memory-reserved')\n",
    "    devices.append('cpu')\n",
    "    devices.append('mps')\n",
    "\n",
    "    return devices\n",
    "\n",
    "def get_device_choice():\n",
    "    devices = list_available_devices()\n",
    "    print(\"Available devices:\")\n",
    "    for i, device in enumerate(devices):\n",
    "        print(f\"{i + 1}. {device}\")\n",
    "    while True:\n",
    "        choice = input(\"Choose the device for model training (enter the corresponding number): \")\n",
    "        if choice.isdigit() and 1 <= int(choice) <= len(devices):\n",
    "            return devices[int(choice) - 1]\n",
    "        print(\"Invalid choice. Please enter a valid device number.\")\n",
    "\n",
    "# Usage\n",
    "device = get_device_choice()\n",
    "print(f\"Selected device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('/Users/ramtarun/Desktop/Cambridge/Indirect-Noise-in-Nozzles/Data/Data_PINN_subsonic_geom_linvelsup_f0-0.1.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PINN(\n",
       "  (loss_function): MSELoss()\n",
       "  (rnn): PhyGRU(\n",
       "    (gru): GRU(2, 4, num_layers=2, batch_first=True, dropout=0.5)\n",
       "    (output_layer): Linear(in_features=4, out_features=4, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PINN_model = Model.PINN(hp.input_size, hp.output_size, hp.hidden_size, hp.num_layers, hp.lda)\n",
    "PINN_model.to(device, non_blocking=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(PINN_model.parameters())\n",
    "# optimizer = torch.optim.Adam([{'params' : params[1::]},{'params' : params[-1], 'lr': hp.ff_learning_rate}], lr = hp.learning_rate, amsgrad = True)   \n",
    "optimizer = torch.optim.Adam(params=params, lr = hp.learning_rate, amsgrad = True)   \n",
    "# optimizer = torch.optim.LBFGS(params, hp.ff_learning_rate, \n",
    "#                               max_iter = hp.epochs, \n",
    "#                               max_eval = None, \n",
    "#                               tolerance_grad = 1e-11, \n",
    "#                               tolerance_change = 1e-11, \n",
    "#                               history_size = 100, \n",
    "#                               line_search_fn = 'strong_wolfe')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, targets, meanflow =  dt.DataPreprocessing(data, ff=0.01, device = device)\n",
    "inputs.to(device)\n",
    "targets.to(device)\n",
    "meanflow.to(device)\n",
    "\n",
    "N = inputs.shape[1]\n",
    "train_loader, val_loader = dt.DataTransformer(inputs, targets, meanflow, TrainingSet=True)\n",
    "\n",
    "meanflow.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/500 - Train Loss: 16.207254 Val Loss: 0.534402 f_train: -0.1256 f_test: -0.1226\n",
      "Epoch 10/500 - Train Loss: 14.792564 Val Loss: 0.488329 f_train: -0.1152 f_test: -0.1104\n",
      "Epoch 15/500 - Train Loss: 13.387316 Val Loss: 0.441396 f_train: -0.1185 f_test: -0.1163\n",
      "Epoch 20/500 - Train Loss: 12.044244 Val Loss: 0.398381 f_train: -0.1189 f_test: -0.1098\n",
      "Epoch 25/500 - Train Loss: 10.789747 Val Loss: 0.357223 f_train: -0.1132 f_test: -0.1068\n",
      "Epoch 30/500 - Train Loss: 9.647078 Val Loss: 0.316306 f_train: -0.0959 f_test: -0.0991\n",
      "Epoch 35/500 - Train Loss: 8.640535 Val Loss: 0.281520 f_train: -0.0861 f_test: -0.0838\n",
      "Epoch 40/500 - Train Loss: 7.846201 Val Loss: 0.257969 f_train: -0.0689 f_test: -0.0573\n",
      "Epoch 45/500 - Train Loss: 7.408281 Val Loss: 0.244384 f_train: -0.0250 f_test: -0.0298\n",
      "Epoch 50/500 - Train Loss: 7.626735 Val Loss: 0.246262 f_train: 0.0114 f_test: 0.0039\n",
      "Epoch 55/500 - Train Loss: 96.269034 Val Loss: 0.251805 f_train: 0.0249 f_test: 0.0327\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The total norm of order 2.0 for gradients from `parameters` is non-finite, so it cannot be clipped. To disable this error and scale the gradients by the non-finite norm anyway, set `error_if_nonfinite=False`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[130], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m fval \u001b[39m=\u001b[39m [\u001b[39m0.1\u001b[39m  , \u001b[39m0.06\u001b[39m , \u001b[39m0.04\u001b[39m, \u001b[39m0.01\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[39m# for f in fval:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# ### Model 3\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# batch_size = 32\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39m#     input_data = batch.view(batch_size, sequence_length, 1)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m#     train_loader.append(input_data)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m train_loss, val_loss, f_train, f_test, f_dist \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain(train_loader, val_loader, hp\u001b[39m.\u001b[39;49mepochs, optimizer, PINN_model, N)\n\u001b[1;32m     15\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     16\u001b[0m     plt\u001b[39m.\u001b[39mfigure() \n",
      "File \u001b[0;32m~/Desktop/Cambridge/Friction-Factor-Estimation-PINN/src/train.py:48\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, val_loader, epochs, optimizer, PINN_model, N)\u001b[0m\n\u001b[1;32m     46\u001b[0m loss1 \u001b[39m=\u001b[39m PINN_model\u001b[39m.\u001b[39mLoss(X, Y, MF)\n\u001b[1;32m     47\u001b[0m loss1\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 48\u001b[0m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mclip_grad_norm_(PINN_model\u001b[39m.\u001b[39;49mparameters(), max_norm\u001b[39m=\u001b[39;49m\u001b[39m2.0\u001b[39;49m, norm_type\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, error_if_nonfinite\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     49\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     50\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss1\u001b[39m.\u001b[39mitem() \n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/utils/clip_grad.py:64\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     61\u001b[0m     total_norm \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mvector_norm(torch\u001b[39m.\u001b[39mstack([norm\u001b[39m.\u001b[39mto(first_device) \u001b[39mfor\u001b[39;00m norm \u001b[39min\u001b[39;00m norms]), norm_type)\n\u001b[1;32m     63\u001b[0m \u001b[39mif\u001b[39;00m error_if_nonfinite \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39mlogical_or(total_norm\u001b[39m.\u001b[39misnan(), total_norm\u001b[39m.\u001b[39misinf()):\n\u001b[0;32m---> 64\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mThe total norm of order \u001b[39m\u001b[39m{\u001b[39;00mnorm_type\u001b[39m}\u001b[39;00m\u001b[39m for gradients from \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     66\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m`parameters` is non-finite, so it cannot be clipped. To disable \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     67\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mthis error and scale the gradients by the non-finite norm anyway, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     68\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mset `error_if_nonfinite=False`\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     69\u001b[0m clip_coef \u001b[39m=\u001b[39m max_norm \u001b[39m/\u001b[39m (total_norm \u001b[39m+\u001b[39m \u001b[39m1e-6\u001b[39m)\n\u001b[1;32m     70\u001b[0m \u001b[39m# Note: multiplying by the clamped coef is redundant when the coef is clamped to 1, but doing so\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m# avoids a `if clip_coef < 1:` conditional which can require a CPU <=> device synchronization\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[39m# when the gradients do not reside in CPU memory.\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The total norm of order 2.0 for gradients from `parameters` is non-finite, so it cannot be clipped. To disable this error and scale the gradients by the non-finite norm anyway, set `error_if_nonfinite=False`"
     ]
    }
   ],
   "source": [
    "fval = [0.1  , 0.06 , 0.04, 0.01]\n",
    "\n",
    "# for f in fval:\n",
    "# ### Model 3\n",
    "# batch_size = 32\n",
    "# sequence_length = len(inputs) // batch_size\n",
    "# # Reshape the input tensors within the DataLoader\n",
    "# train_loader = []\n",
    "# for batch in Train_loader:\n",
    "#     input_data = batch.view(batch_size, sequence_length, 1)\n",
    "#     train_loader.append(input_data)\n",
    "\n",
    "train_loss, val_loss, f_train, f_test, f_dist = trainer.train(train_loader, val_loader, hp.epochs, optimizer, PINN_model, N)\n",
    "\n",
    "with torch.no_grad():\n",
    "    plt.figure() \n",
    "    plt.plot(train_loss.keys(), train_loss.values(), 'r-', label='Training Loss')\n",
    "    plt.plot(val_loss.keys(), val_loss.values(), 'g-', label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    plt.figure()\n",
    "    plt.plot(f_train.keys(), f_train.values(), 'r-', label='Training Friction Factor')\n",
    "    plt.plot(f_test.keys(), f_test.values(), 'g-', label='Validation Friction Factor')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Friction Factor')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    plt.figure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_sequence(tensor, seq_length):\n",
    "    # tensor shape: (N, 2)\n",
    "    # seq_length: desired sequence length\n",
    "  \n",
    "\n",
    "    # Add a new dimension of size 1 to the tensor\n",
    "    tensor = torch.unsqueeze(tensor, dim=1)\n",
    "\n",
    "    # Repeat the tensor along the added dimension to match the desired sequence length\n",
    "    tensor = tensor.repeat(1, seq_length, 1)\n",
    "\n",
    "    # tensor shape: (N, L, 2)\n",
    "    return tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
