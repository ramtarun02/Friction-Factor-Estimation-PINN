{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import torch\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.train' from 'c:\\\\Users\\\\s420553\\\\Downloads\\\\F_PINNN\\\\Friction-Factor-Estimation-PINN\\\\src\\\\train.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import src.HyperParameters as hp \n",
    "import src.data as dt \n",
    "import src.model3 as Model \n",
    "import src.train as trainer\n",
    "\n",
    "importlib.reload(hp)\n",
    "importlib.reload(dt)\n",
    "importlib.reload(Model)\n",
    "importlib.reload(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('/Users/ramtarun/Desktop/Cambridge/Indirect-Noise-in-Nozzles/Data/Data_PINN_subsonic_geom_linvelsup_f0-0.1.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINN_model = Model.PINN(hp.input_size, hp.output_size, hp.hidden_size, hp.num_layers)\n",
    "# PINN_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(PINN_model.parameters())\n",
    "# optimizer = torch.optim.Adam([{'params' : params[1::]},{'params' : params[-1], 'lr': hp.ff_learning_rate}], lr = hp.learning_rate, amsgrad = True)   \n",
    "optimizer = torch.optim.Adam(params=params, lr = hp.learning_rate, amsgrad = True)   \n",
    "# optimizer = torch.optim.LBFGS(params, hp.ff_learning_rate, \n",
    "#                               max_iter = hp.epochs, \n",
    "#                               max_eval = None, \n",
    "#                               tolerance_grad = 1e-11, \n",
    "#                               tolerance_change = 1e-11, \n",
    "#                               history_size = 100, \n",
    "#                               line_search_fn = 'strong_wolfe')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000 - Train Loss: 0.192116 Val Loss: 0.002611 f_train: 0.0643 f_test: 0.0679\n",
      "Epoch 20/1000 - Train Loss: 0.179700 Val Loss: 0.002292 f_train: 0.0674 f_test: 0.0776\n",
      "Epoch 30/1000 - Train Loss: 0.175881 Val Loss: 0.002326 f_train: 0.0782 f_test: 0.0781\n",
      "Epoch 40/1000 - Train Loss: 0.168891 Val Loss: 0.002599 f_train: 0.0868 f_test: 0.0866\n",
      "Epoch 50/1000 - Train Loss: 0.170819 Val Loss: 0.002577 f_train: 0.0835 f_test: 0.0850\n",
      "Epoch 60/1000 - Train Loss: 0.166151 Val Loss: 0.002398 f_train: 0.0978 f_test: 0.0977\n",
      "Epoch 70/1000 - Train Loss: 0.161913 Val Loss: 0.002385 f_train: 0.0918 f_test: 0.0889\n",
      "Epoch 80/1000 - Train Loss: 0.163672 Val Loss: 0.002350 f_train: 0.0825 f_test: 0.0815\n",
      "Epoch 90/1000 - Train Loss: 0.162211 Val Loss: 0.002544 f_train: 0.0935 f_test: 0.0929\n",
      "Epoch 100/1000 - Train Loss: 0.159608 Val Loss: 0.002264 f_train: 0.0955 f_test: 0.0961\n",
      "Epoch 110/1000 - Train Loss: 0.159599 Val Loss: 0.002099 f_train: 0.1003 f_test: 0.0952\n",
      "Epoch 120/1000 - Train Loss: 0.158260 Val Loss: 0.002031 f_train: 0.0892 f_test: 0.0928\n",
      "Epoch 130/1000 - Train Loss: 0.160447 Val Loss: 0.002041 f_train: 0.1016 f_test: 0.0999\n",
      "Epoch 140/1000 - Train Loss: 0.158514 Val Loss: 0.002122 f_train: 0.0937 f_test: 0.0938\n",
      "Epoch 150/1000 - Train Loss: 0.159416 Val Loss: 0.002035 f_train: 0.1118 f_test: 0.1076\n",
      "Epoch 160/1000 - Train Loss: 0.158629 Val Loss: 0.002321 f_train: 0.0803 f_test: 0.0866\n",
      "Epoch 170/1000 - Train Loss: 0.158995 Val Loss: 0.002102 f_train: 0.1028 f_test: 0.1050\n",
      "Epoch 180/1000 - Train Loss: 0.156883 Val Loss: 0.002106 f_train: 0.0955 f_test: 0.1065\n",
      "Epoch 190/1000 - Train Loss: 0.157515 Val Loss: 0.002126 f_train: 0.1057 f_test: 0.1001\n",
      "Epoch 200/1000 - Train Loss: 0.159889 Val Loss: 0.002155 f_train: 0.0781 f_test: 0.0855\n",
      "Epoch 210/1000 - Train Loss: 0.156827 Val Loss: 0.002137 f_train: 0.0905 f_test: 0.0909\n",
      "Epoch 220/1000 - Train Loss: 0.156666 Val Loss: 0.002171 f_train: 0.0968 f_test: 0.1044\n",
      "Epoch 230/1000 - Train Loss: 0.157256 Val Loss: 0.002062 f_train: 0.1136 f_test: 0.1083\n",
      "Epoch 240/1000 - Train Loss: 0.156965 Val Loss: 0.002047 f_train: 0.0937 f_test: 0.0965\n",
      "Epoch 250/1000 - Train Loss: 0.156503 Val Loss: 0.002059 f_train: 0.0787 f_test: 0.0903\n",
      "Epoch 260/1000 - Train Loss: 0.157926 Val Loss: 0.002012 f_train: 0.1052 f_test: 0.1092\n",
      "Epoch 270/1000 - Train Loss: 0.153751 Val Loss: 0.002044 f_train: 0.0900 f_test: 0.0888\n",
      "Epoch 280/1000 - Train Loss: 0.156159 Val Loss: 0.002281 f_train: 0.0724 f_test: 0.0790\n",
      "Epoch 290/1000 - Train Loss: 0.154171 Val Loss: 0.002039 f_train: 0.1257 f_test: 0.1166\n",
      "Epoch 300/1000 - Train Loss: 0.155961 Val Loss: 0.001960 f_train: 0.1172 f_test: 0.1024\n",
      "Epoch 310/1000 - Train Loss: 0.157087 Val Loss: 0.002181 f_train: 0.0853 f_test: 0.0975\n",
      "Epoch 320/1000 - Train Loss: 0.157816 Val Loss: 0.002008 f_train: 0.0982 f_test: 0.0994\n",
      "Epoch 330/1000 - Train Loss: 0.153337 Val Loss: 0.002058 f_train: 0.0711 f_test: 0.0991\n",
      "Epoch 340/1000 - Train Loss: 0.154345 Val Loss: 0.002091 f_train: 0.1051 f_test: 0.1005\n",
      "Epoch 350/1000 - Train Loss: 0.153130 Val Loss: 0.002000 f_train: 0.0956 f_test: 0.0980\n",
      "Epoch 360/1000 - Train Loss: 0.154429 Val Loss: 0.002094 f_train: 0.0870 f_test: 0.0875\n",
      "Epoch 370/1000 - Train Loss: 0.153205 Val Loss: 0.002025 f_train: 0.1043 f_test: 0.1048\n",
      "Epoch 380/1000 - Train Loss: 0.153879 Val Loss: 0.002093 f_train: 0.1016 f_test: 0.1130\n",
      "Epoch 390/1000 - Train Loss: 0.154058 Val Loss: 0.002042 f_train: 0.0927 f_test: 0.1017\n",
      "Epoch 400/1000 - Train Loss: 0.154702 Val Loss: 0.002177 f_train: 0.0842 f_test: 0.0953\n",
      "Epoch 410/1000 - Train Loss: 0.152410 Val Loss: 0.002166 f_train: 0.0976 f_test: 0.0952\n",
      "Epoch 420/1000 - Train Loss: 0.152937 Val Loss: 0.002013 f_train: 0.0820 f_test: 0.0924\n",
      "Epoch 430/1000 - Train Loss: 0.152560 Val Loss: 0.002143 f_train: 0.0842 f_test: 0.0959\n",
      "Epoch 440/1000 - Train Loss: 0.152284 Val Loss: 0.001973 f_train: 0.1105 f_test: 0.1067\n",
      "Epoch 450/1000 - Train Loss: 0.153657 Val Loss: 0.002172 f_train: 0.1092 f_test: 0.1084\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "fval = [0.1  , 0.06 , 0.04, 0.01]\n",
    "\n",
    "# for f in fval:\n",
    "inputs, targets, meanflow =  dt.DataPreprocessing(data, ff=0.01)\n",
    "N = inputs.shape[1]\n",
    "train_loader, val_loader = dt.DataTransformer(inputs, targets, meanflow, TrainingSet=True)\n",
    "# ### Model 3\n",
    "# batch_size = 32\n",
    "# sequence_length = len(inputs) // batch_size\n",
    "# # Reshape the input tensors within the DataLoader\n",
    "# train_loader = []\n",
    "# for batch in Train_loader:\n",
    "#     input_data = batch.view(batch_size, sequence_length, 1)\n",
    "#     train_loader.append(input_data)\n",
    "\n",
    "train_loss, val_loss, f_train, f_test, f_dist = trainer.train(train_loader, val_loader, hp.epochs, optimizer, PINN_model, N)\n",
    "with torch.no_grad():\n",
    "    plt.figure() \n",
    "    plt.plot(train_loss.keys(), train_loss.values(), 'r-', label='Training Loss')\n",
    "    plt.plot(val_loss.keys(), val_loss.values(), 'g-', label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    plt.figure()\n",
    "    plt.plot(f_train.keys(), f_train.values(), 'r-', label='Training Friction Factor')\n",
    "    plt.plot(f_test.keys(), f_test.values(), 'g-', label='Validation Friction Factor')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Friction Factor')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    plt.figure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(inputs[:,1], targets)\n",
    "preds = PINN_model.rnn(inputs).detach()\n",
    "plt.plot(inputs[:,1], preds[:,:3], 'k-.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
