{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataPreprocessing(data, ff):\n",
    "\n",
    "    fmat = {0.1:data['sbsl_PINN'][0][0], 0.08:data['sbsl_PINN'][1][0], 0.06:data['sbsl_PINN'][2][0], 0.04:data['sbsl_PINN'][3][0], 0.02:data['sbsl_PINN'][4][0], 0.01:data['sbsl_PINN'][5][0], 0.005:data['sbsl_PINN'][6][0], 0.0:data['sbsl_PINN'][7][0]}\n",
    "    fval = [0.1  , 0.08 , 0.06 , 0.04 , 0.02 , 0.01 , 0.005, 0.000]\n",
    "    i = ff\n",
    "    N = fmat[i].shape[1]\n",
    "    U = np.zeros((N,4))\n",
    "    U[:,0] = data['eta']\n",
    "    U[:,1] = fmat[i][0,:]\n",
    "    U[:,2] = fmat[i][1,:]\n",
    "    U[:,3] = fmat[i][2,:]\n",
    "    #U[:,4] = torch.from_numpy(np.array([0.06]))\n",
    "\n",
    "    D = np.array(data['D'])\n",
    "    M = np.array(data['M'])\n",
    "    u = np.array(data['ubar'])\n",
    "    dMdx = np.array(data['dMdx'])\n",
    "    dudx = np.array(data['dudx'])\n",
    "    alpha = np.array(data['alp'])\n",
    "    eta1 = np.array(data['eta'])   \n",
    "\n",
    "    meanflow = np.zeros((D.shape[1],7))\n",
    "    meanflow[:,0] = D\n",
    "    meanflow[:,1] = M\n",
    "    meanflow[:,2] = u\n",
    "    meanflow[:,3] = dMdx\n",
    "    meanflow[:,4] = dudx\n",
    "    meanflow[:,5] = alpha\n",
    "    meanflow[:,6] = eta1\n",
    "    #meanflow = torch.tile(meanflow, (8,1))\n",
    "\n",
    "    he = np.array([0.00])\n",
    "    HE = np.tile(he, (N,1)) \n",
    "    #He = HE.flatten()[:,None]\n",
    "\n",
    "    uu = U[:, 1:4]\n",
    "    uu0 = U[:,0]\n",
    "    uu0 = uu0.flatten()[:, None]\n",
    "\n",
    "    input_set = np.concatenate([HE, uu0],1)\n",
    "\n",
    "    return input_set, uu, meanflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('/Users/ramtarun/Desktop/Cambridge/Indirect-Noise-in-Nozzles/Data/Data_PINN_subsonic_geom_linvelsup_f0-0.1.mat')\n",
    "inputs, targets, meanflow = DataPreprocessing(data, ff=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_data(U, b_size, n_batches):\n",
    "    '''\n",
    "    Splits the data in batches. Each batch is created by sampling the signal with interval\n",
    "    equal to n_batches.\n",
    "    '''\n",
    "    data   = np.zeros((n_batches, b_size, U.shape[1]), dtype=float)    \n",
    "    for j in range(n_batches):\n",
    "        data[j,:b_size] = U[::skip][j::n_batches].copy()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip = 2\n",
    "b_size = 30\n",
    "n_batches = 5\n",
    "val_batches = n_batches//2   #validation batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tt = inputs[:b_size*n_batches*skip].copy()\n",
    "y_tt = targets[:b_size*n_batches*skip].copy()\n",
    "m_tt = meanflow[:b_size*n_batches*skip].copy()\n",
    "\n",
    "Y_train     = split_data(y_tt, b_size, n_batches).astype(dtype=np.float32)\n",
    "X_train     = split_data(x_tt, b_size, n_batches).astype(dtype=np.float32)\n",
    "Meanflow_train = split_data(m_tt, b_size, n_batches).astype(dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vv        = inputs[b_size*n_batches*skip:b_size*n_batches*skip+b_size*val_batches*skip:].copy()\n",
    "Y_vv        = targets[b_size*n_batches*skip:b_size*n_batches*skip+b_size*val_batches*skip:].copy()\n",
    "meanflow_vv = meanflow[b_size*n_batches*skip:b_size*n_batches*skip+b_size*val_batches*skip:].copy()\n",
    "Y_val       = split_data(Y_vv, b_size, val_batches).astype(dtype=np.float32) \n",
    "X_val       = split_data(x_vv, b_size, val_batches).astype(dtype=np.float32)\n",
    "Meanflow_val = split_data(meanflow_vv, b_size, val_batches).astype(dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RHS_ff_t(x,y, baseflow, f):\n",
    "    eta = x\n",
    "    pi_p, pi_m, sig = tf.unstack(y,axis=1)\n",
    "\n",
    "    D = baseflow[:,0]\n",
    "    M = baseflow[:,1]\n",
    "    u = baseflow[:,2]\n",
    "    dMdx = baseflow[:,3]\n",
    "    dudx = baseflow[:,4]\n",
    "    alpha = baseflow[:,5]\n",
    "    eta1 = baseflow[:,6]    \n",
    "    \n",
    "    He = 0#a constant input\n",
    "\n",
    "    gamma = 1.4#constant\n",
    "    \n",
    "    Msq = tf.math.square(M)\n",
    "\n",
    "    Lambda = 1 + Msq * (gamma-1)/2\n",
    "    zeta = f*gamma*Msq - 2*tf.math.tan(alpha)\n",
    "    C1= ((gamma - 1)*(1-Msq)*f)/(2*Lambda*zeta)\n",
    "    Ca = -C1*M*u*dMdx*(2-(2*Msq/(1-Msq)) - (2*gamma*Msq*(-2*f*Lambda - (gamma-1)/gamma *zeta)/(2*Lambda*zeta)))\n",
    "    Ff = -(dudx + (4*f*u/(2*D)))\n",
    "\n",
    "    denom = (M**2*u - u + C1*Msq *u + C1*Msq*Msq*gamma*u) #M**4\n",
    "    vrh_p = M*(2*(1-M) + C1*M*(M-2+M*gamma*(1-2*M)))/(2*denom)\n",
    "    vrh_m = -M*(2*(1+M) + C1*M*(M+2+M*gamma*(1+2*M)))/(2*denom)\n",
    "    vkp_p = Msq*C1*(2+M*(gamma-1))/(2*denom)\n",
    "    vkp_m = Msq*C1*(2-M*(gamma-1))/(2*denom)\n",
    "    vth_p = C1*M*(M*(1+gamma) + M**2*(1-gamma) - 2)/denom\n",
    "    vth_m = C1*M*(M*(1+gamma) - M**2*(1-gamma) + 2)/denom\n",
    "    vsig = -(C1*Msq*(1+gamma*Msq) + Msq - 1)/denom\n",
    "    calM = dMdx/(2*M)\n",
    "    kp_p = (gamma - 1) + (2/M)\n",
    "    kp_m = (gamma - 1) - (2/M)\n",
    "    Gm_p = M*(Ca*(M + 1) + Ff*M*(C1*gamma*M*Msq + M + (1 - C1*Msq)))/(2*denom)\n",
    "    Gm_m = M*(Ca*(M - 1) - Ff*M*(C1*gamma*M*Msq + M - (1 - C1*Msq)))/(2*denom)\n",
    "    Ups = (Ca*(Msq - 1) - C1*Ff*Msq*Msq *(1+gamma))/denom\n",
    "    \n",
    "#     eq1 = - (2*np.pi*1j*He*vrh_p + Gm_m*kp_m + calM)*pi_p + (2*np.pi*1j*He*vkp_p + Gm_m*kp_p + calM)*pi_m - Gm_m*sig\n",
    "#     eq2 = - (2*np.pi*1j*He*vkp_m + Gm_p*kp_m - calM)*pi_p - (2*np.pi*1j*He*vrh_m + Gm_p*kp_p + calM)*pi_m - Gm_m*sig\n",
    "#     eq3 = - (2*np.pi*1j*He*vth_p + Ups*kp_m)*pi_p - (2*np.pi*1j*He*vth_m + Ups*kp_p)*pi_m - (2*np.pi*1j*He*vsig + Ups)*sig\n",
    "    \n",
    "#     eq1 = - tf.multiply((Gm_m*kp_m + calM),pi_p) + tf.multiply(( Gm_m*kp_p + calM),pi_m) - tf.multiply(Gm_m,sig)\n",
    "    eq1 =  (Gm_m*kp_m + calM)*pi_p + ( Gm_m*kp_p - calM)*pi_m + Gm_m*sig\n",
    "    eq2 =  ( Gm_p*kp_m - calM)*pi_p + ( Gm_p*kp_p + calM)*pi_m + Gm_m*sig\n",
    "    eq3 =  ( Ups*kp_m)*pi_p + ( Ups*kp_p)*pi_m + (Ups)*sig\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    return tf.stack([-eq1, -eq2, -eq3],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def pde(r, n, baseflow, f):\n",
    "    dr_dn = tf.gradients(r,n)\n",
    "    gr = RHS_ff_t(n, r, baseflow, f)\n",
    "    pde = dr_dn  + gr\n",
    "    return pde\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def loss(outs, targets, pde):\n",
    "    l1 = tf.keras.losses.MSE(targets, outs)\n",
    "    l2 = tf.reduce_mean(pde**2)\n",
    "    return l1+l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2023-07-21 20:59:50.471576: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-07-21 20:59:50.628047: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_14/lstm_cell/kernel/Assign' id:12281 op device:{requested: '', assigned: ''} def:{{{node lstm_14/lstm_cell/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_14/lstm_cell/kernel, lstm_14/lstm_cell/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-07-21 20:59:52.004852: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-07-21 20:59:52.374985: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.5       , 0.5       , 0.5       ],\n",
       "        [0.50012755, 0.50005364, 0.50008917],\n",
       "        [0.5003494 , 0.5001536 , 0.50025094],\n",
       "        [0.50064117, 0.5002918 , 0.50047064],\n",
       "        [0.50098455, 0.50046057, 0.50073576],\n",
       "        [0.50136584, 0.50065273, 0.501036  ],\n",
       "        [0.50177443, 0.5008623 , 0.50136274],\n",
       "        [0.5022025 , 0.50108445, 0.5017094 ],\n",
       "        [0.50264394, 0.5013151 , 0.5020705 ],\n",
       "        [0.5030944 , 0.50155145, 0.50244206],\n",
       "        [0.5035504 , 0.5017912 , 0.50282085],\n",
       "        [0.5040097 , 0.5020327 , 0.50320446],\n",
       "        [0.5044706 , 0.50227505, 0.50359106],\n",
       "        [0.5049319 , 0.5025174 , 0.5039794 ],\n",
       "        [0.50539297, 0.5027594 , 0.50436854],\n",
       "        [0.5058532 , 0.50300086, 0.5047578 ],\n",
       "        [0.5063124 , 0.50324154, 0.50514674],\n",
       "        [0.5067704 , 0.50348157, 0.50553507],\n",
       "        [0.5072271 , 0.50372094, 0.5059225 ],\n",
       "        [0.5076827 , 0.5039599 , 0.50630903],\n",
       "        [0.50813717, 0.5041984 , 0.5066945 ],\n",
       "        [0.5085907 , 0.5044365 , 0.507079  ],\n",
       "        [0.5090433 , 0.50467443, 0.50746256],\n",
       "        [0.50949496, 0.5049122 , 0.5078451 ],\n",
       "        [0.5099459 , 0.50514984, 0.50822675],\n",
       "        [0.5103963 , 0.50538754, 0.50860757],\n",
       "        [0.5108461 , 0.5056252 , 0.50898755],\n",
       "        [0.5112953 , 0.50586295, 0.50936675],\n",
       "        [0.5117441 , 0.5061007 , 0.5097452 ],\n",
       "        [0.5121925 , 0.50633866, 0.51012295]],\n",
       "\n",
       "       [[0.5000256 , 0.5000107 , 0.5000178 ],\n",
       "        [0.50017196, 0.5000736 , 0.50012153],\n",
       "        [0.50040776, 0.50018126, 0.50029486],\n",
       "        [0.50070983, 0.50032556, 0.5005236 ],\n",
       "        [0.5010608 , 0.500499  , 0.5007958 ],\n",
       "        [0.5014475 , 0.50069463, 0.5011013 ],\n",
       "        [0.5018601 , 0.50090677, 0.5014321 ],\n",
       "        [0.50229084, 0.5011305 , 0.50178164],\n",
       "        [0.50273407, 0.5013624 , 0.5021449 ],\n",
       "        [0.5031856 , 0.5015994 , 0.5025179 ],\n",
       "        [0.50364226, 0.50183946, 0.50289756],\n",
       "        [0.5041019 , 0.5020812 , 0.5032818 ],\n",
       "        [0.5045629 , 0.50232357, 0.5036688 ],\n",
       "        [0.5050242 , 0.5025658 , 0.5040572 ],\n",
       "        [0.50548506, 0.5028077 , 0.50444645],\n",
       "        [0.505945  , 0.50304896, 0.5048356 ],\n",
       "        [0.506404  , 0.5032895 , 0.50522447],\n",
       "        [0.50686175, 0.5035294 , 0.50561255],\n",
       "        [0.50731826, 0.5037688 , 0.5059998 ],\n",
       "        [0.50777364, 0.5040076 , 0.5063861 ],\n",
       "        [0.50822794, 0.504246  , 0.50677145],\n",
       "        [0.5086812 , 0.5044841 , 0.5071558 ],\n",
       "        [0.5091336 , 0.50472194, 0.5075391 ],\n",
       "        [0.5095852 , 0.50495976, 0.50792146],\n",
       "        [0.510036  , 0.5051974 , 0.508303  ],\n",
       "        [0.51048625, 0.5054351 , 0.50868356],\n",
       "        [0.5109359 , 0.50567275, 0.50906336],\n",
       "        [0.5113851 , 0.5059105 , 0.50944245],\n",
       "        [0.5118338 , 0.50614834, 0.50982076],\n",
       "        [0.51228213, 0.50638616, 0.5101984 ]],\n",
       "\n",
       "       [[0.500051  , 0.50002146, 0.5000357 ],\n",
       "        [0.5002163 , 0.50009364, 0.5001539 ],\n",
       "        [0.5004661 , 0.5002089 , 0.5003388 ],\n",
       "        [0.50077856, 0.50035936, 0.5005767 ],\n",
       "        [0.5011371 , 0.5005374 , 0.50085586],\n",
       "        [0.5015293 , 0.50073653, 0.5011667 ],\n",
       "        [0.5019457 , 0.5009512 , 0.50150144],\n",
       "        [0.50237906, 0.5011767 , 0.5018539 ],\n",
       "        [0.5028241 , 0.50140965, 0.5022192 ],\n",
       "        [0.50327677, 0.5016473 , 0.50259364],\n",
       "        [0.5037342 , 0.5018878 , 0.5029743 ],\n",
       "        [0.5041941 , 0.5021297 , 0.5033591 ],\n",
       "        [0.5046552 , 0.502372  , 0.50374645],\n",
       "        [0.5051164 , 0.5026142 , 0.5041351 ],\n",
       "        [0.5055771 , 0.50285596, 0.5045243 ],\n",
       "        [0.50603694, 0.5030971 , 0.5049134 ],\n",
       "        [0.5064956 , 0.50333756, 0.50530213],\n",
       "        [0.50695306, 0.50357735, 0.50569004],\n",
       "        [0.5074094 , 0.50381655, 0.5060772 ],\n",
       "        [0.50786453, 0.5040553 , 0.5064632 ],\n",
       "        [0.5083186 , 0.5042936 , 0.5068484 ],\n",
       "        [0.5087717 , 0.5045317 , 0.5072324 ],\n",
       "        [0.50922394, 0.5047695 , 0.5076156 ],\n",
       "        [0.5096754 , 0.50500727, 0.5079979 ],\n",
       "        [0.5101261 , 0.505245  , 0.5083791 ],\n",
       "        [0.51057625, 0.5054826 , 0.5087596 ],\n",
       "        [0.5110258 , 0.5057203 , 0.5091393 ],\n",
       "        [0.51147485, 0.5059581 , 0.50951815],\n",
       "        [0.5119235 , 0.5061959 , 0.5098964 ],\n",
       "        [0.5123717 , 0.50643384, 0.5102739 ]],\n",
       "\n",
       "       [[0.50007653, 0.5000322 , 0.50005347],\n",
       "        [0.5002607 , 0.5001136 , 0.5001862 ],\n",
       "        [0.5005244 , 0.5002366 , 0.5003827 ],\n",
       "        [0.50084716, 0.50039303, 0.5006297 ],\n",
       "        [0.5012133 , 0.50057584, 0.5009159 ],\n",
       "        [0.501611  , 0.5007785 , 0.5012321 ],\n",
       "        [0.50203127, 0.5009955 , 0.50157076],\n",
       "        [0.50246733, 0.5012228 , 0.5019261 ],\n",
       "        [0.50291425, 0.5014569 , 0.5022935 ],\n",
       "        [0.503368  , 0.5016952 , 0.50266933],\n",
       "        [0.503826  , 0.50193614, 0.50305104],\n",
       "        [0.5042862 , 0.50217813, 0.50343645],\n",
       "        [0.50474745, 0.5024205 , 0.50382406],\n",
       "        [0.5052086 , 0.5026626 , 0.5042129 ],\n",
       "        [0.5056692 , 0.5029043 , 0.50460213],\n",
       "        [0.5061287 , 0.5031453 , 0.50499123],\n",
       "        [0.50658715, 0.50338554, 0.50537974],\n",
       "        [0.50704443, 0.5036252 , 0.5057675 ],\n",
       "        [0.5075005 , 0.5038643 , 0.5061544 ],\n",
       "        [0.50795543, 0.50410295, 0.50654036],\n",
       "        [0.5084093 , 0.50434124, 0.5069252 ],\n",
       "        [0.50886226, 0.50457925, 0.5073092 ],\n",
       "        [0.5093143 , 0.50481707, 0.50769216],\n",
       "        [0.50976557, 0.5050548 , 0.50807416],\n",
       "        [0.5102162 , 0.5052925 , 0.5084553 ],\n",
       "        [0.5106662 , 0.5055302 , 0.50883555],\n",
       "        [0.5111156 , 0.5057679 , 0.5092151 ],\n",
       "        [0.5115646 , 0.50600564, 0.50959384],\n",
       "        [0.51201314, 0.50624347, 0.5099719 ],\n",
       "        [0.5124613 , 0.5064814 , 0.51034933]],\n",
       "\n",
       "       [[0.5001021 , 0.5000429 , 0.5000713 ],\n",
       "        [0.50030506, 0.50013363, 0.5002186 ],\n",
       "        [0.5005828 , 0.5002642 , 0.50042665],\n",
       "        [0.5009159 , 0.5004268 , 0.5006827 ],\n",
       "        [0.50128955, 0.5006143 , 0.5009759 ],\n",
       "        [0.5016927 , 0.5008204 , 0.5012974 ],\n",
       "        [0.50211686, 0.50104   , 0.5016401 ],\n",
       "        [0.50255567, 0.501269  , 0.50199836],\n",
       "        [0.50300425, 0.5015042 , 0.5023678 ],\n",
       "        [0.5034592 , 0.5017432 , 0.5027451 ],\n",
       "        [0.5039179 , 0.5019844 , 0.5031277 ],\n",
       "        [0.50437844, 0.5022266 , 0.50351375],\n",
       "        [0.5048397 , 0.50246894, 0.5039017 ],\n",
       "        [0.50530076, 0.50271106, 0.50429076],\n",
       "        [0.50576115, 0.5029526 , 0.50468   ],\n",
       "        [0.5062206 , 0.5031934 , 0.505069  ],\n",
       "        [0.50667876, 0.5034336 , 0.5054574 ],\n",
       "        [0.5071358 , 0.50367314, 0.505845  ],\n",
       "        [0.5075916 , 0.50391215, 0.5062317 ],\n",
       "        [0.5080463 , 0.5041507 , 0.5066174 ],\n",
       "        [0.50850004, 0.50438887, 0.5070022 ],\n",
       "        [0.50895274, 0.5046268 , 0.5073859 ],\n",
       "        [0.5094046 , 0.50486463, 0.50776863],\n",
       "        [0.5098558 , 0.50510234, 0.5081505 ],\n",
       "        [0.51030624, 0.50534004, 0.50853145],\n",
       "        [0.51075613, 0.5055777 , 0.50891155],\n",
       "        [0.5112055 , 0.5058154 , 0.50929093],\n",
       "        [0.5116544 , 0.50605315, 0.50966954],\n",
       "        [0.51210284, 0.50629103, 0.51004744],\n",
       "        [0.5125509 , 0.506529  , 0.51042473]]], dtype=float32)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Model\n",
    "model = keras.Sequential(name='PhyLSTM')\n",
    "model.add(keras.layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True,input_shape=(None, 2)))\n",
    "model.add(keras.layers.Dense(3, activation='sigmoid')) \n",
    "\n",
    "model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.keras.optimizers.legacy.Adam(amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train(Xtrain, ytrain, baseflow, model, train = True):\n",
    "    \n",
    "    n = Xtrain[:,1]\n",
    "    xtrain = tf.expand_dims(Xtrain, axis=1)\n",
    "    outs = model(xtrain) [:,:3]\n",
    "    f = tf.reduce_mean(outs[:,-1])\n",
    "    print(f)\n",
    "    PDE = pde(outs, n, baseflow, f)\n",
    "    print(PDE)\n",
    "    trn_loss = loss(outs, ytrain, pde)\n",
    "    \n",
    "\n",
    "    if train:\n",
    "        param = model.trainable_weights + [f]\n",
    "                #compute and apply gradients\n",
    "        grads   = tf.gradients(loss, param)\n",
    "        # grads   = tf.gradients(loss, varss, grad_ys= tf.complex128)\n",
    "        optim.apply_gradients(zip(grads, param))\n",
    "\n",
    "\n",
    "    return outs, trn_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mean:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/var/folders/rc/z9d7mxns5jgd6ggy35fswxnw0000gn/T/ipykernel_20304/1775593817.py\", line 9, in train  *\n        PDE = pde(outs, n, baseflow, f)\n    File \"/var/folders/rc/z9d7mxns5jgd6ggy35fswxnw0000gn/T/ipykernel_20304/2385997753.py\", line 4, in pde  *\n        gr = RHS_ff_t(n, r, baseflow, f)\n    File \"/var/folders/rc/z9d7mxns5jgd6ggy35fswxnw0000gn/T/ipykernel_20304/3186833219.py\", line 3, in RHS_ff_t  *\n        pi_p, pi_m, sig = tf.unstack(y,axis=1)\n\n    ValueError: not enough values to unpack (expected 3, got 1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[214], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39m#     rng.shuffle(t_train, axis=0) #shuffle batches\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_batches\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m):\n\u001b[0;32m---> 37\u001b[0m         outs, loss  \u001b[39m=\u001b[39m train(X_train[j], Y_train[j], Meanflow_train[j], model)\n\u001b[1;32m     38\u001b[0m         loss_training \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[1;32m     40\u001b[0m \u001b[39m#     #save training loss each epoch\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/rc/z9d7mxns5jgd6ggy35fswxnw0000gn/T/__autograph_generated_filex4d0jprh.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train\u001b[0;34m(Xtrain, ytrain, baseflow, model, train)\u001b[0m\n\u001b[1;32m     13\u001b[0m f \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mreduce_mean, (ag__\u001b[39m.\u001b[39mld(outs)[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m],), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     14\u001b[0m ag__\u001b[39m.\u001b[39mld(\u001b[39mprint\u001b[39m)(ag__\u001b[39m.\u001b[39mld(f))\n\u001b[0;32m---> 15\u001b[0m PDE \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(pde), (ag__\u001b[39m.\u001b[39;49mld(outs), ag__\u001b[39m.\u001b[39;49mld(n), ag__\u001b[39m.\u001b[39;49mld(baseflow), ag__\u001b[39m.\u001b[39;49mld(f)), \u001b[39mNone\u001b[39;49;00m, fscope)\n\u001b[1;32m     16\u001b[0m ag__\u001b[39m.\u001b[39mld(\u001b[39mprint\u001b[39m)(ag__\u001b[39m.\u001b[39mld(PDE))\n\u001b[1;32m     17\u001b[0m trn_loss \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(loss), (ag__\u001b[39m.\u001b[39mld(outs), ag__\u001b[39m.\u001b[39mld(ytrain), ag__\u001b[39m.\u001b[39mld(pde)), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/var/folders/rc/z9d7mxns5jgd6ggy35fswxnw0000gn/T/__autograph_generated_filen2xr_vu1.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__pde\u001b[0;34m(r, n, baseflow, f)\u001b[0m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m     10\u001b[0m dr_dn \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mgradients, (ag__\u001b[39m.\u001b[39mld(r), ag__\u001b[39m.\u001b[39mld(n)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 11\u001b[0m gr \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(RHS_ff_t), (ag__\u001b[39m.\u001b[39;49mld(n), ag__\u001b[39m.\u001b[39;49mld(r), ag__\u001b[39m.\u001b[39;49mld(baseflow), ag__\u001b[39m.\u001b[39;49mld(f)), \u001b[39mNone\u001b[39;49;00m, fscope)\n\u001b[1;32m     12\u001b[0m pde \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(dr_dn) \u001b[39m+\u001b[39m ag__\u001b[39m.\u001b[39mld(gr)\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/var/folders/rc/z9d7mxns5jgd6ggy35fswxnw0000gn/T/__autograph_generated_file50s_uyt8.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__RHS_ff_t\u001b[0;34m(x, y, baseflow, f)\u001b[0m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m     10\u001b[0m eta \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(x)\n\u001b[0;32m---> 11\u001b[0m (pi_p, pi_m, sig) \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39munstack, (ag__\u001b[39m.\u001b[39mld(y),), \u001b[39mdict\u001b[39m(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), fscope)\n\u001b[1;32m     12\u001b[0m D \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(baseflow)[:, \u001b[39m0\u001b[39m]\n\u001b[1;32m     13\u001b[0m M \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(baseflow)[:, \u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/var/folders/rc/z9d7mxns5jgd6ggy35fswxnw0000gn/T/ipykernel_20304/1775593817.py\", line 9, in train  *\n        PDE = pde(outs, n, baseflow, f)\n    File \"/var/folders/rc/z9d7mxns5jgd6ggy35fswxnw0000gn/T/ipykernel_20304/2385997753.py\", line 4, in pde  *\n        gr = RHS_ff_t(n, r, baseflow, f)\n    File \"/var/folders/rc/z9d7mxns5jgd6ggy35fswxnw0000gn/T/ipykernel_20304/3186833219.py\", line 3, in RHS_ff_t  *\n        pi_p, pi_m, sig = tf.unstack(y,axis=1)\n\n    ValueError: not enough values to unpack (expected 3, got 1)\n"
     ]
    }
   ],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15,4)\n",
    "plt.rcParams[\"font.size\"]  = 20\n",
    "\n",
    "Loss_Mse    = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "n_epochs    = 5001 #number of epochs\n",
    "\n",
    "#define optimizer and initial learning rate   \n",
    "optimizer   = tf.keras.optimizers.legacy.Adam(amsgrad=True) #amsgrad True for better convergence\n",
    "# optimizer   = tf.keras.optimizers.Adam #amsgrad True for better convergence\n",
    "\n",
    "l_rate                  = 0.01\n",
    "optimizer.learning_rate = l_rate\n",
    "\n",
    "# quantities to check and store the training and validation loss and the training goes on\n",
    "old_loss      = np.zeros(n_epochs) #needed to evaluate training loss convergence\n",
    "tloss_plot    = np.zeros(n_epochs) #training loss\n",
    "vloss_plot    = np.zeros(n_epochs) #validation loss\n",
    "tloss1_plot   = np.zeros(n_epochs) #training_der loss\n",
    "vloss1_plot   = np.zeros(n_epochs) #validation_der loss\n",
    "old_loss[0]  = 1e6 #initial value has to be high\n",
    "N_check      = 5   #each N_check epochs we check convergence and validation loss\n",
    "patience     = 200 #if the val_loss has not gone down in the last patience epochs, early stop\n",
    "last_save    = patience\n",
    "\n",
    "t            = 1 # initial (not important value) to monitor the time of the training\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    if epoch - last_save > patience: break #early stop\n",
    "                \n",
    "    #Perform gradient descent for all the batches every epoch\n",
    "    loss_training = 0\n",
    "#     rng.shuffle(t_train, axis=0) #shuffle batches\n",
    "\n",
    "    for j in range(n_batches-2):\n",
    "        outs, loss  = train(X_train[j], Y_train[j], Meanflow_train[j], model)\n",
    "        loss_training += loss\n",
    "    \n",
    "#     #save training loss each epoch\n",
    "   \n",
    "    tloss_plot[epoch]  = loss_training/n_batches\n",
    "        \n",
    "    if (epoch%100==0) and epoch != 0: \n",
    "        f = tf.mean(outs[:,-1])\n",
    "        print(f.numpy())\n",
    "        #plot convergence of training and validation loss\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.title('MSE convergence')\n",
    "        plt.yscale('log')\n",
    "        plt.grid(True, axis=\"both\", which='both', ls=\"-\", alpha=0.3)\n",
    "        plt.plot(tloss_plot[np.nonzero(tloss_plot)], 'g', label='Train loss')\n",
    "\n",
    "        plt.xlabel('epochs')\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(Y_train[-1], 'black')\n",
    "        plt.plot(outs[:,:3].numpy(), 'b--')\n",
    "\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
