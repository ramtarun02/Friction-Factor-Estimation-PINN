{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import torch\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.train' from '/Users/ramtarun/Desktop/Cambridge/Friction-Factor-Estimation-PINN/src/train.py'>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import src.HyperParameters as hp \n",
    "import src.data as dt \n",
    "import src.model3 as Model \n",
    "import src.train as trainer\n",
    "\n",
    "importlib.reload(hp)\n",
    "importlib.reload(dt)\n",
    "importlib.reload(Model)\n",
    "importlib.reload(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('/Users/ramtarun/Desktop/Cambridge/Indirect-Noise-in-Nozzles/Data/Data_PINN_subsonic_geom_linvelsup_f0-0.1.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PINN(\n",
       "  (loss_function): MSELoss()\n",
       "  (rnn): PhyGRU(\n",
       "    (activation): ModuleList(\n",
       "      (0): ReLU()\n",
       "      (1): Tanh()\n",
       "      (2): ReLU()\n",
       "      (3): Tanh()\n",
       "      (4): ReLU()\n",
       "      (5): Tanh()\n",
       "      (6): ReLU()\n",
       "      (7): Sigmoid()\n",
       "    )\n",
       "    (gru): GRU(2, 6, batch_first=True)\n",
       "    (hidden_layers): ModuleList(\n",
       "      (0): Linear(in_features=6, out_features=6, bias=True)\n",
       "      (1): Linear(in_features=6, out_features=6, bias=True)\n",
       "      (2): Linear(in_features=6, out_features=6, bias=True)\n",
       "      (3): Linear(in_features=6, out_features=6, bias=True)\n",
       "      (4): Linear(in_features=6, out_features=6, bias=True)\n",
       "      (5): Linear(in_features=6, out_features=6, bias=True)\n",
       "      (6): Linear(in_features=6, out_features=6, bias=True)\n",
       "      (7): Linear(in_features=6, out_features=6, bias=True)\n",
       "    )\n",
       "    (output_layer): Linear(in_features=6, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PINN_model = Model.PINN(hp.input_size, hp.output_size, hp.hidden_sizes, hp.activations)\n",
    "PINN_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(PINN_model.parameters())\n",
    "# optimizer = torch.optim.Adam([{'params' : params[1::]},{'params' : params[-1], 'lr': hp.ff_learning_rate}], lr = hp.learning_rate, amsgrad = True)   \n",
    "optimizer = torch.optim.Adam(params=params, lr = hp.learning_rate, amsgrad = True)   \n",
    "# optimizer = torch.optim.LBFGS(params, hp.ff_learning_rate, \n",
    "#                               max_iter = hp.epochs, \n",
    "#                               max_eval = None, \n",
    "#                               tolerance_grad = 1e-11, \n",
    "#                               tolerance_change = 1e-11, \n",
    "#                               history_size = 100, \n",
    "#                               line_search_fn = 'strong_wolfe')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/7500 - Train Loss: 10.712062 Val Loss: 0.508682 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 150/7500 - Train Loss: 3.404699 Val Loss: 0.283232 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 225/7500 - Train Loss: 1.150948 Val Loss: 0.159419 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 300/7500 - Train Loss: 0.443331 Val Loss: 0.092198 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 375/7500 - Train Loss: 0.220936 Val Loss: 0.060958 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 450/7500 - Train Loss: 0.150507 Val Loss: 0.054185 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 525/7500 - Train Loss: 0.127975 Val Loss: 0.052806 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 600/7500 - Train Loss: 0.121062 Val Loss: 0.052525 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 675/7500 - Train Loss: 0.118786 Val Loss: 0.052517 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 750/7500 - Train Loss: 0.117940 Val Loss: 0.052565 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 825/7500 - Train Loss: 0.117689 Val Loss: 0.052610 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 900/7500 - Train Loss: 0.117532 Val Loss: 0.052643 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 975/7500 - Train Loss: 0.117608 Val Loss: 0.052663 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 1050/7500 - Train Loss: 0.117423 Val Loss: 0.052676 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 1125/7500 - Train Loss: 0.117653 Val Loss: 0.052685 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 1200/7500 - Train Loss: 0.117379 Val Loss: 0.052690 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 1275/7500 - Train Loss: 0.117405 Val Loss: 0.052693 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 1350/7500 - Train Loss: 0.117588 Val Loss: 0.052696 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 1425/7500 - Train Loss: 0.117582 Val Loss: 0.052695 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 1500/7500 - Train Loss: 0.117529 Val Loss: 0.052697 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 1575/7500 - Train Loss: 0.117650 Val Loss: 0.052698 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 1650/7500 - Train Loss: 0.117677 Val Loss: 0.052697 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 1725/7500 - Train Loss: 0.117413 Val Loss: 0.052698 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 1800/7500 - Train Loss: 0.117560 Val Loss: 0.052698 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 1875/7500 - Train Loss: 0.117404 Val Loss: 0.052698 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 1950/7500 - Train Loss: 0.117650 Val Loss: 0.052697 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 2025/7500 - Train Loss: 0.117701 Val Loss: 0.052697 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 2100/7500 - Train Loss: 0.117246 Val Loss: 0.052697 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 2175/7500 - Train Loss: 0.117552 Val Loss: 0.052699 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 2250/7500 - Train Loss: 0.117664 Val Loss: 0.052697 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 2325/7500 - Train Loss: 0.117513 Val Loss: 0.052699 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 2400/7500 - Train Loss: 0.117556 Val Loss: 0.052700 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 2475/7500 - Train Loss: 0.117496 Val Loss: 0.052699 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 2550/7500 - Train Loss: 0.117420 Val Loss: 0.052698 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 2625/7500 - Train Loss: 0.117656 Val Loss: 0.052698 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 2700/7500 - Train Loss: 0.117567 Val Loss: 0.052697 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 2775/7500 - Train Loss: 0.117626 Val Loss: 0.052698 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 2850/7500 - Train Loss: 0.117671 Val Loss: 0.052698 f_train: -0.4848 f_test: -0.4848\n",
      "Epoch 2925/7500 - Train Loss: 0.117512 Val Loss: 0.052698 f_train: -0.4848 f_test: -0.4848\n"
     ]
    }
   ],
   "source": [
    "fval = [0.1  , 0.06 , 0.04, 0.01]\n",
    "\n",
    "# for f in fval:\n",
    "inputs, targets, meanflow =  dt.DataPreprocessing(data, ff=0.01)\n",
    "N = inputs.shape[1]\n",
    "train_loader, val_loader = dt.DataTransformer(inputs, targets, meanflow, TrainingSet=True)\n",
    "# ### Model 3\n",
    "# batch_size = 32\n",
    "# sequence_length = len(inputs) // batch_size\n",
    "# # Reshape the input tensors within the DataLoader\n",
    "# train_loader = []\n",
    "# for batch in Train_loader:\n",
    "#     input_data = batch.view(batch_size, sequence_length, 1)\n",
    "#     train_loader.append(input_data)\n",
    "\n",
    "train_loss, val_loss, f_train, f_test, f_dist = trainer.train(train_loader, val_loader, hp.epochs, optimizer, PINN_model, N)\n",
    "with torch.no_grad():\n",
    "    plt.figure() \n",
    "    plt.plot(train_loss.keys(), train_loss.values(), 'r-', label='Training Loss')\n",
    "    plt.plot(val_loss.keys(), val_loss.values(), 'g-', label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    plt.figure()\n",
    "    plt.plot(f_train.keys(), f_train.values(), 'r-', label='Training Friction Factor')\n",
    "    plt.plot(f_test.keys(), f_test.values(), 'g-', label='Validation Friction Factor')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Friction Factor')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    plt.figure()\n",
    "    ff_distribution = {}\n",
    "    for key_tensor, value_tensor in f_dist.items():\n",
    "    # Convert the tensors to numpy arrays\n",
    "        key_array = np.array(key_tensor.detach())\n",
    "        value_array = np.array(value_tensor.detach())\n",
    "        # Unpack the elements from the tensors\n",
    "        for key_elem, value_elem in zip(key_array, value_array):\n",
    "        # Add the unpacked elements to the new dictionary\n",
    "            ff_distribution[key_elem] = value_elem\n",
    "        \n",
    "        \n",
    "ff_distribution = dict(sorted(ff_distribution.items()))\n",
    "plt.plot(ff_distribution.keys(), ff_distribution.values(), label='Friction Factor Variation along the Nozzle')\n",
    "plt.ylabel('Friction Factor')\n",
    "plt.xlabel('Eta')\n",
    "plt.ylim([0,1]) \n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(inputs[:,1], targets)\n",
    "plt.plot(inputs[:,1],PINN_model.rnn(inputs).detach(), 'k-.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
