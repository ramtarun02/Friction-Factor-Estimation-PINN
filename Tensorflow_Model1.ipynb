{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataPreprocessing(data, ff):\n",
    "\n",
    "    fmat = {0.1:data['sbsl_PINN'][0][0], 0.08:data['sbsl_PINN'][1][0], 0.06:data['sbsl_PINN'][2][0], 0.04:data['sbsl_PINN'][3][0], 0.02:data['sbsl_PINN'][4][0], 0.01:data['sbsl_PINN'][5][0], 0.005:data['sbsl_PINN'][6][0], 0.0:data['sbsl_PINN'][7][0]}\n",
    "    fval = [0.1  , 0.08 , 0.06 , 0.04 , 0.02 , 0.01 , 0.005, 0.000]\n",
    "    i = ff\n",
    "    N = fmat[i].shape[1]\n",
    "    U = np.zeros((N,4))\n",
    "    U[:,0] = data['eta']\n",
    "    U[:,1] = fmat[i][0,:]\n",
    "    U[:,2] = fmat[i][1,:]\n",
    "    U[:,3] = fmat[i][2,:]\n",
    "    #U[:,4] = torch.from_numpy(np.array([0.06]))\n",
    "\n",
    "    D = np.array(data['D'])\n",
    "    M = np.array(data['M'])\n",
    "    u = np.array(data['ubar'])\n",
    "    dMdx = np.array(data['dMdx'])\n",
    "    dudx = np.array(data['dudx'])\n",
    "    alpha = np.array(data['alp'])\n",
    "    eta1 = np.array(data['eta'])   \n",
    "\n",
    "    meanflow = np.zeros((D.shape[1],7))\n",
    "    meanflow[:,0] = D\n",
    "    meanflow[:,1] = M\n",
    "    meanflow[:,2] = u\n",
    "    meanflow[:,3] = dMdx\n",
    "    meanflow[:,4] = dudx\n",
    "    meanflow[:,5] = alpha\n",
    "    meanflow[:,6] = eta1\n",
    "    #meanflow = torch.tile(meanflow, (8,1))\n",
    "\n",
    "    he = np.array([0.00])\n",
    "    HE = np.tile(he, (N,1)) \n",
    "    #He = HE.flatten()[:,None]\n",
    "\n",
    "    uu = U[:, 1:4]\n",
    "    uu0 = U[:,0]\n",
    "    uu0 = uu0.flatten()[:, None]\n",
    "\n",
    "    input_set = np.concatenate([HE, uu0],1)\n",
    "\n",
    "    return input_set, uu, meanflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('/Users/ramtarun/Desktop/Cambridge/Indirect-Noise-in-Nozzles/Data/Data_PINN_subsonic_geom_linvelsup_f0-0.1.mat')\n",
    "inputs, targets, meanflow = DataPreprocessing(data, ff=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_data(U, b_size, n_batches):\n",
    "    '''\n",
    "    Splits the data in batches. Each batch is created by sampling the signal with interval\n",
    "    equal to n_batches.\n",
    "    '''\n",
    "    data   = np.zeros((n_batches, b_size, U.shape[1]), dtype=float)    \n",
    "    for j in range(n_batches):\n",
    "        data[j,:b_size] = U[::skip][j::n_batches].copy()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip = 2\n",
    "b_size = 30\n",
    "n_batches = 5\n",
    "val_batches = n_batches//2   #validation batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tt = inputs[:b_size*n_batches*skip].copy()\n",
    "y_tt = targets[:b_size*n_batches*skip].copy()\n",
    "m_tt = meanflow[:b_size*n_batches*skip].copy()\n",
    "\n",
    "Y_train     = split_data(y_tt, b_size, n_batches).astype(dtype=np.float32)\n",
    "X_train     = split_data(x_tt, b_size, n_batches).astype(dtype=np.float32)\n",
    "Meanflow_train = split_data(m_tt, b_size, n_batches).astype(dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vv        = inputs[b_size*n_batches*skip:b_size*n_batches*skip+b_size*val_batches*skip:].copy()\n",
    "Y_vv        = targets[b_size*n_batches*skip:b_size*n_batches*skip+b_size*val_batches*skip:].copy()\n",
    "meanflow_vv = meanflow[b_size*n_batches*skip:b_size*n_batches*skip+b_size*val_batches*skip:].copy()\n",
    "Y_val       = split_data(Y_vv, b_size, val_batches).astype(dtype=np.float32) \n",
    "X_val       = split_data(x_vv, b_size, val_batches).astype(dtype=np.float32)\n",
    "Meanflow_val = split_data(meanflow_vv, b_size, val_batches).astype(dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RHS_ff_t(y, baseflow, f):\n",
    "\n",
    "    pi_p, pi_m, sig = tf.unstack(y, axis=1)\n",
    "\n",
    "    D = baseflow[:,0]\n",
    "    M = baseflow[:,1]\n",
    "    u = baseflow[:,2]\n",
    "    dMdx = baseflow[:,3]\n",
    "    dudx = baseflow[:,4]\n",
    "    alpha = baseflow[:,5]\n",
    "    eta1 = baseflow[:,6]    \n",
    "    \n",
    "    He = 0#a constant input\n",
    "\n",
    "    gamma = 1.4#constant\n",
    "    \n",
    "    Msq = tf.math.square(M)\n",
    "\n",
    "    Lambda = 1 + Msq * (gamma-1)/2\n",
    "    zeta = f*gamma*Msq - 2*tf.math.tan(alpha)\n",
    "    C1= ((gamma - 1)*(1-Msq)*f)/(2*Lambda*zeta)\n",
    "    Ca = -C1*M*u*dMdx*(2-(2*Msq/(1-Msq)) - (2*gamma*Msq*(-2*f*Lambda - (gamma-1)/gamma *zeta)/(2*Lambda*zeta)))\n",
    "    Ff = -(dudx + (4*f*u/(2*D)))\n",
    "\n",
    "    denom = (M**2*u - u + C1*Msq *u + C1*Msq*Msq*gamma*u) #M**4\n",
    "    vrh_p = M*(2*(1-M) + C1*M*(M-2+M*gamma*(1-2*M)))/(2*denom)\n",
    "    vrh_m = -M*(2*(1+M) + C1*M*(M+2+M*gamma*(1+2*M)))/(2*denom)\n",
    "    vkp_p = Msq*C1*(2+M*(gamma-1))/(2*denom)\n",
    "    vkp_m = Msq*C1*(2-M*(gamma-1))/(2*denom)\n",
    "    vth_p = C1*M*(M*(1+gamma) + M**2*(1-gamma) - 2)/denom\n",
    "    vth_m = C1*M*(M*(1+gamma) - M**2*(1-gamma) + 2)/denom\n",
    "    vsig = -(C1*Msq*(1+gamma*Msq) + Msq - 1)/denom\n",
    "    calM = dMdx/(2*M)\n",
    "    kp_p = (gamma - 1) + (2/M)\n",
    "    kp_m = (gamma - 1) - (2/M)\n",
    "    Gm_p = M*(Ca*(M + 1) + Ff*M*(C1*gamma*M*Msq + M + (1 - C1*Msq)))/(2*denom)\n",
    "    Gm_m = M*(Ca*(M - 1) - Ff*M*(C1*gamma*M*Msq + M - (1 - C1*Msq)))/(2*denom)\n",
    "    Ups = (Ca*(Msq - 1) - C1*Ff*Msq*Msq *(1+gamma))/denom\n",
    "    \n",
    "#     eq1 = - (2*np.pi*1j*He*vrh_p + Gm_m*kp_m + calM)*pi_p + (2*np.pi*1j*He*vkp_p + Gm_m*kp_p + calM)*pi_m - Gm_m*sig\n",
    "#     eq2 = - (2*np.pi*1j*He*vkp_m + Gm_p*kp_m - calM)*pi_p - (2*np.pi*1j*He*vrh_m + Gm_p*kp_p + calM)*pi_m - Gm_m*sig\n",
    "#     eq3 = - (2*np.pi*1j*He*vth_p + Ups*kp_m)*pi_p - (2*np.pi*1j*He*vth_m + Ups*kp_p)*pi_m - (2*np.pi*1j*He*vsig + Ups)*sig\n",
    "    \n",
    "#     eq1 = - tf.multiply((Gm_m*kp_m + calM),pi_p) + tf.multiply(( Gm_m*kp_p + calM),pi_m) - tf.multiply(Gm_m,sig)\n",
    "    eq1 =  (Gm_m*kp_m + calM)*pi_p + ( Gm_m*kp_p - calM)*pi_m + Gm_m*sig\n",
    "    eq2 =  ( Gm_p*kp_m - calM)*pi_p + ( Gm_p*kp_p + calM)*pi_m + Gm_m*sig\n",
    "    eq3 =  ( Ups*kp_m)*pi_p + ( Ups*kp_p)*pi_m + (Ups)*sig\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    return tf.stack([-eq1, -eq2, -eq3],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.5        0.5        0.5        0.5       ]\n",
      " [0.49974158 0.5012792  0.5007597  0.49872464]\n",
      " [0.49948317 0.5025582  0.50151926 0.49744937]\n",
      " [0.4992248  0.50383687 0.5022788  0.4961744 ]\n",
      " [0.49896657 0.5051152  0.5030381  0.49489987]\n",
      " [0.4987085  0.50639296 0.5037971  0.49362582]\n",
      " [0.4984507  0.50767004 0.5045558  0.49235243]\n",
      " [0.49819303 0.50894636 0.5053142  0.4910799 ]\n",
      " [0.49793562 0.51022166 0.50607216 0.48980832]\n",
      " [0.49767864 0.5114959  0.50682956 0.48853773]\n",
      " [0.49742195 0.5127689  0.5075864  0.4872684 ]\n",
      " [0.49716562 0.5140406  0.5083426  0.4860003 ]\n",
      " [0.49690977 0.51531076 0.5090982  0.4847338 ]\n",
      " [0.49665427 0.51657933 0.509853   0.48346886]\n",
      " [0.4963994  0.5178462  0.51060694 0.48220557]\n",
      " [0.49614504 0.5191111  0.51136005 0.48094416]\n",
      " [0.49589127 0.52037406 0.51211226 0.47968474]\n",
      " [0.49563807 0.5216349  0.5128635  0.47842744]\n",
      " [0.49538553 0.52289337 0.51361364 0.47717234]\n",
      " [0.49513367 0.52414954 0.5143627  0.4759197 ]\n",
      " [0.49488258 0.52540314 0.51511055 0.4746695 ]\n",
      " [0.4946322  0.52665406 0.51585716 0.4734219 ]\n",
      " [0.49438268 0.52790225 0.5166025  0.47217703]\n",
      " [0.49413398 0.5291475  0.5173465  0.47093496]\n",
      " [0.4938861  0.5303898  0.51808906 0.46969596]\n",
      " [0.4936391  0.53162897 0.51883024 0.46846   ]\n",
      " [0.4933931  0.53286475 0.5195698  0.46722728]\n",
      " [0.49314794 0.53409725 0.52030784 0.4659979 ]\n",
      " [0.4929039  0.53532624 0.5210442  0.464772  ]\n",
      " [0.49266082 0.53655165 0.5217789  0.46354955]], shape=(30, 4), dtype=float32)\n",
      "Model: \"PhyLSTM\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_43 (Dense)            (None, None, 4)           12        \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, None, 4)           20        \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, None, 4)           20        \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, None, 4)           20        \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, None, 4)           20        \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, None, 4)           20        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 112 (448.00 Byte)\n",
      "Trainable params: 112 (448.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Model\n",
    "model = keras.Sequential(name='PhyLSTM')\n",
    "model.add(keras.layers.Dense(4, activation='linear', input_shape=(None, 2)))\n",
    "# model.add(keras.layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True,input_shape=(None, 2)))\n",
    "model.add(keras.layers.Dense(4, activation='relu')) \n",
    "model.add(keras.layers.Dense(4, activation='tanh'))\n",
    "model.add(keras.layers.Dense(4, activation='relu'))\n",
    "model.add(keras.layers.Dense(4, activation='tanh'))\n",
    "model.add(keras.layers.Dense(4, activation='sigmoid'))    \n",
    "\n",
    "# xtrain = tf.expand_dims(X_train, axis=1)\n",
    "print(model(X_train[0]))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute '_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     pde \u001b[39m=\u001b[39m dy_dn  \u001b[39m+\u001b[39m gr\n\u001b[1;32m     11\u001b[0m     \u001b[39mreturn\u001b[39;00m pde\n\u001b[0;32m---> 13\u001b[0m DiffEq(X_train[\u001b[39m0\u001b[39;49m], Meanflow_train[\u001b[39m0\u001b[39;49m], f\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/tensorflow/python/ops/custom_gradient.py:343\u001b[0m, in \u001b[0;36mBind.__call__\u001b[0;34m(self, *a, **k)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mk):\n\u001b[0;32m--> 343\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_d(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_f, a, k)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/tensorflow/python/ops/custom_gradient.py:297\u001b[0m, in \u001b[0;36mcustom_gradient.<locals>.decorated\u001b[0;34m(wrapped, args, kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Decorated function with custom gradient.\"\"\"\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 297\u001b[0m   \u001b[39mreturn\u001b[39;00m _eager_mode_decorator(wrapped, args, kwargs)\n\u001b[1;32m    298\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m   \u001b[39mreturn\u001b[39;00m _graph_mode_decorator(wrapped, args, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/tensorflow/python/ops/custom_gradient.py:543\u001b[0m, in \u001b[0;36m_eager_mode_decorator\u001b[0;34m(f, args, kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Implement custom gradient decorator for eager mode.\"\"\"\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mVariableWatcher() \u001b[39mas\u001b[39;00m variable_watcher:\n\u001b[0;32m--> 543\u001b[0m   result, grad_fn \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    544\u001b[0m flat_args \u001b[39m=\u001b[39m composite_tensor_gradient\u001b[39m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m    545\u001b[0m     nest\u001b[39m.\u001b[39mflatten(args))\n\u001b[1;32m    546\u001b[0m flat_kwargs \u001b[39m=\u001b[39m composite_tensor_gradient\u001b[39m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m    547\u001b[0m     nest\u001b[39m.\u001b[39mflatten(kwargs))\n",
      "Cell \u001b[0;32mIn[94], line 8\u001b[0m, in \u001b[0;36mDiffEq\u001b[0;34m(x, baseflow, f)\u001b[0m\n\u001b[1;32m      5\u001b[0m     n \u001b[39m=\u001b[39m x[:,\u001b[39m1\u001b[39m]\n\u001b[1;32m      6\u001b[0m     r \u001b[39m=\u001b[39m model(X_train[\u001b[39m0\u001b[39m])\n\u001b[0;32m----> 8\u001b[0m dy_dn \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39;49mgradient(r,n)\n\u001b[1;32m      9\u001b[0m gr \u001b[39m=\u001b[39m RHS_ff_t(r, baseflow, f)\n\u001b[1;32m     10\u001b[0m pde \u001b[39m=\u001b[39m dy_dn  \u001b[39m+\u001b[39m gr\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:1063\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1057\u001b[0m   output_gradients \u001b[39m=\u001b[39m (\n\u001b[1;32m   1058\u001b[0m       composite_tensor_gradient\u001b[39m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1059\u001b[0m           output_gradients))\n\u001b[1;32m   1060\u001b[0m   output_gradients \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m ops\u001b[39m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1061\u001b[0m                       \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1063\u001b[0m flat_grad \u001b[39m=\u001b[39m imperative_grad\u001b[39m.\u001b[39;49mimperative_grad(\n\u001b[1;32m   1064\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tape,\n\u001b[1;32m   1065\u001b[0m     flat_targets,\n\u001b[1;32m   1066\u001b[0m     flat_sources,\n\u001b[1;32m   1067\u001b[0m     output_gradients\u001b[39m=\u001b[39;49moutput_gradients,\n\u001b[1;32m   1068\u001b[0m     sources_raw\u001b[39m=\u001b[39;49mflat_sources_raw,\n\u001b[1;32m   1069\u001b[0m     unconnected_gradients\u001b[39m=\u001b[39;49munconnected_gradients)\n\u001b[1;32m   1071\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent:\n\u001b[1;32m   1072\u001b[0m   \u001b[39m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_watched_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape\u001b[39m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mUnknown value for unconnected_gradients: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_TapeGradient(\n\u001b[1;32m     68\u001b[0m     tape\u001b[39m.\u001b[39;49m_tape,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m     target,\n\u001b[1;32m     70\u001b[0m     sources,\n\u001b[1;32m     71\u001b[0m     output_gradients,\n\u001b[1;32m     72\u001b[0m     sources_raw,\n\u001b[1;32m     73\u001b[0m     compat\u001b[39m.\u001b[39;49mas_str(unconnected_gradients\u001b[39m.\u001b[39;49mvalue))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute '_id'"
     ]
    }
   ],
   "source": [
    "@tf.custom_gradient\n",
    "def DiffEq(x, baseflow, f):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        n = x[:,1]\n",
    "        r = model(X_train[0])\n",
    "\n",
    "    dy_dn = tape.gradient(r,n)\n",
    "    gr = RHS_ff_t(r, baseflow, f)\n",
    "    pde = dy_dn  + gr\n",
    "    return pde\n",
    "\n",
    "DiffEq(X_train[0], Meanflow_train[0], f=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 55\u001b[0m\n\u001b[1;32m     50\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39madam\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[39m# # Just use `fit` as usual -- you can use callbacks, etc.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[39m# x = np.random.random((1000, 32))\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39m# y = np.random.random((1000, 1))\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, Y_train, Meanflow_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/keras/src/engine/data_adapter.py:272\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m _check_data_cardinality(inputs)\n\u001b[1;32m    270\u001b[0m \u001b[39m# If batch_size is not passed but steps is, calculate from the input\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[39m# data.  Defaults to `32` for backwards compatibility.\u001b[39;00m\n\u001b[0;32m--> 272\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m batch_size:\n\u001b[1;32m    273\u001b[0m     batch_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(math\u001b[39m.\u001b[39mceil(num_samples \u001b[39m/\u001b[39m steps)) \u001b[39mif\u001b[39;00m steps \u001b[39melse\u001b[39;00m \u001b[39m32\u001b[39m\n\u001b[1;32m    275\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(math\u001b[39m.\u001b[39mceil(num_samples \u001b[39m/\u001b[39m batch_size))\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "class CustomModel(keras.Model):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "        self.mae_metric = keras.metrics.MeanSquaredError(name=\"mse\")\n",
    "\n",
    "    def train_step(self, xtrain, ytrain, baseflowtrain):\n",
    "        x = xtrain\n",
    "        y = ytrain\n",
    "        bf = baseflowtrain\n",
    "        eta = xtrain[:,1]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)  # Forward pass\n",
    "            # Compute our own loss\n",
    "            r = y_pred[:,:-1]\n",
    "            f = tf.reduce_mean(y_pred[:,-1])\n",
    "            loss1 = keras.losses.mean_squared_error(y, r)\n",
    "            loss2 = DiffEq(r, eta, bf , f)\n",
    "            loss = loss1 + loss2\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables + [f]\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Compute our own metrics\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.mae_metric.update_state(y, r)\n",
    "        return {\"loss\": self.loss_tracker.result(), \"mse\": self.mae_metric.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We list our `Metric` objects here so that `reset_states()` can be\n",
    "        # called automatically at the start of each epoch\n",
    "        # or at the start of `evaluate()`.\n",
    "        # If you don't implement this property, you have to call\n",
    "        # `reset_states()` yourself at the time of your choosing.\n",
    "        return [self.loss_tracker, self.mae_metric]\n",
    "# Construct an instance of CustomModel\n",
    "# inputs = keras.Input(shape=(32,))\n",
    "# outputs = keras.layers.Dense(1)(inputs)\n",
    "model = CustomModel(model)\n",
    "\n",
    "# We don't passs a loss or metrics here.\n",
    "model.compile(optimizer=\"adam\")\n",
    "\n",
    "# # Just use `fit` as usual -- you can use callbacks, etc.\n",
    "# x = np.random.random((1000, 32))\n",
    "# y = np.random.random((1000, 1))\n",
    "model.fit(X_train, Y_train, Meanflow_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outs, targets, pde):\n",
    "    l1 = tf.keras.losses.MSE(targets, outs)\n",
    "    l2 = tf.reduce_mean(tf.square(pde))\n",
    "    return l1+l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.keras.optimizers.legacy.Adam(amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = loss_fn, optimizer = optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Model.fit() got multiple values for argument 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, Y_train, Meanflow_train, epochs \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m, batch_size \u001b[39m=\u001b[39;49m \u001b[39m30\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "\u001b[0;31mTypeError\u001b[0m: Model.fit() got multiple values for argument 'batch_size'"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, Meanflow_train, epochs = 100, batch_size = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train(Xtrain, ytrain, baseflow, model, train = True):\n",
    "    \n",
    "    n = Xtrain[:,1]\n",
    "    xtrain = tf.expand_dims(Xtrain, axis=1)\n",
    "    outs = model(xtrain) [:,:3]\n",
    "    f = tf.reduce_mean(outs[:,-1])\n",
    "    print(f)\n",
    "    PDE = DiffEq(outs, n, baseflow, f)\n",
    "    print(PDE)\n",
    "    trn_loss = loss_fn(outs, ytrain, pde=)\n",
    "    \n",
    "\n",
    "\n",
    "    if train:\n",
    "        param = model.trainable_weights + [f]\n",
    "        #compute and apply gradients\n",
    "        grads   = tf.gradients(trn_loss, param)\n",
    "        optim.apply_gradients(zip(grads, param))\n",
    "\n",
    "\n",
    "    return  outs, trn_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15,4)\n",
    "plt.rcParams[\"font.size\"]  = 20\n",
    "\n",
    "Loss_Mse    = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "n_epochs    = 5001 #number of epochs\n",
    "\n",
    "#define optimizer and initial learning rate   \n",
    "optimizer   = tf.keras.optimizers.legacy.Adam(amsgrad=True) #amsgrad True for better convergence\n",
    "# optimizer   = tf.keras.optimizers.Adam #amsgrad True for better convergence\n",
    "\n",
    "l_rate                  = 0.01\n",
    "optimizer.learning_rate = l_rate\n",
    "\n",
    "# quantities to check and store the training and validation loss and the training goes on\n",
    "old_loss      = np.zeros(n_epochs) #needed to evaluate training loss convergence\n",
    "tloss_plot    = np.zeros(n_epochs) #training loss\n",
    "vloss_plot    = np.zeros(n_epochs) #validation loss\n",
    "tloss1_plot   = np.zeros(n_epochs) #training_der loss\n",
    "vloss1_plot   = np.zeros(n_epochs) #validation_der loss\n",
    "old_loss[0]  = 1e6 #initial value has to be high\n",
    "N_check      = 5   #each N_check epochs we check convergence and validation loss\n",
    "patience     = 200 #if the val_loss has not gone down in the last patience epochs, early stop\n",
    "last_save    = patience\n",
    "\n",
    "t            = 1 # initial (not important value) to monitor the time of the training\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    if epoch - last_save > patience: break #early stop\n",
    "                \n",
    "    #Perform gradient descent for all the batches every epoch\n",
    "    loss_training = 0\n",
    "#     rng.shuffle(t_train, axis=0) #shuffle batches\n",
    "\n",
    "    for j in range(n_batches-2):\n",
    "        temp = train(X_train[j], Y_train[j], Meanflow_train[j], model)\n",
    "        # loss_training += loss\n",
    "        print(temp)\n",
    "    \n",
    "#     #save training loss each epoch\n",
    "   \n",
    "    # tloss_plot[epoch]  = loss_training/n_batches\n",
    "        \n",
    "    # if (epoch%100==0) and epoch != 0: \n",
    "    #     f = tf.mean(outs[:,-1])\n",
    "    #     print(f.numpy())\n",
    "    #     #plot convergence of training and validation loss\n",
    "    #     plt.subplot(1,2,1)\n",
    "    #     plt.title('MSE convergence')\n",
    "    #     plt.yscale('log')\n",
    "    #     plt.grid(True, axis=\"both\", which='both', ls=\"-\", alpha=0.3)\n",
    "    #     plt.plot(tloss_plot[np.nonzero(tloss_plot)], 'g', label='Train loss')\n",
    "\n",
    "    #     plt.xlabel('epochs')\n",
    "    #     plt.subplot(1,2,2)\n",
    "    #     plt.plot(Y_train[-1], 'black')\n",
    "    #     plt.plot(outs[:,:3].numpy(), 'b--')\n",
    "\n",
    "        \n",
    "    #     plt.tight_layout()\n",
    "    #     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
