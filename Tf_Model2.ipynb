{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "# tf.config.threading.set_inter_op_parallelism_threads(1) #set cores for TF\n",
    "# tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "# print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# print(gpus)\n",
    "# for gpu in gpus:\n",
    "#     tf.config.experimental.set_memory_growth(gpu, True)\n",
    "import numpy as np\n",
    "import keras\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from keras.src.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataPreprocessing(data, ff):\n",
    "\n",
    "    fmat = {0.1:data['sbsl_PINN'][0][0], 0.08:data['sbsl_PINN'][1][0], 0.06:data['sbsl_PINN'][2][0], 0.04:data['sbsl_PINN'][3][0], 0.02:data['sbsl_PINN'][4][0], 0.01:data['sbsl_PINN'][5][0], 0.005:data['sbsl_PINN'][6][0], 0.0:data['sbsl_PINN'][7][0]}\n",
    "    fval = [0.1  , 0.08 , 0.06 , 0.04 , 0.02 , 0.01 , 0.005, 0.000]\n",
    "    i = ff\n",
    "    N = fmat[i].shape[1]\n",
    "    U = np.zeros((N,4))\n",
    "    U[:,0] = data['eta']\n",
    "    U[:,1] = fmat[i][0,:]\n",
    "    U[:,2] = fmat[i][1,:]\n",
    "    U[:,3] = fmat[i][2,:]\n",
    "    #U[:,4] = torch.from_numpy(np.array([0.06]))\n",
    "\n",
    "    D = np.array(data['D'])\n",
    "    M = np.array(data['M'])\n",
    "    u = np.array(data['ubar'])\n",
    "    dMdx = np.array(data['dMdx'])\n",
    "    dudx = np.array(data['dudx'])\n",
    "    alpha = np.array(data['alp'])\n",
    "    eta1 = np.array(data['eta'])   \n",
    "\n",
    "    meanflow = np.zeros((D.shape[1],7))\n",
    "    meanflow[:,0] = D\n",
    "    meanflow[:,1] = M\n",
    "    meanflow[:,2] = u\n",
    "    meanflow[:,3] = dMdx\n",
    "    meanflow[:,4] = dudx\n",
    "    meanflow[:,5] = alpha\n",
    "    meanflow[:,6] = eta1\n",
    "    #meanflow = torch.tile(meanflow, (8,1))\n",
    "\n",
    "    he = np.array([0.00])\n",
    "    HE = np.tile(he, (N,1)) \n",
    "    #He = HE.flatten()[:,None]\n",
    "\n",
    "    uu = U[:, 1:4]\n",
    "    uu0 = U[:,0]\n",
    "    uu0 = uu0.flatten()[:, None]\n",
    "\n",
    "    input_set = tf.concat([HE, uu0],1)\n",
    "\n",
    "    return input_set, tf.convert_to_tensor(uu), tf.convert_to_tensor(meanflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('/Users/ramtarun/Desktop/Cambridge/Indirect-Noise-in-Nozzles/Data/Data_PINN_subsonic_geom_linvelsup_f0-0.1.mat')\n",
    "# inp, targets, meanflow = DataPreprocessing(data, ff=0.01)\n",
    "inp2, Y, meanflow2 = DataPreprocessing(data, ff=0.1)\n",
    "X = tf.concat([inp2, meanflow2],1)\n",
    "ftrain = [0.08 , 0.06 , 0.04 , 0.02]\n",
    "fval = [0.00, 0.01, 0.005]\n",
    "for i in ftrain:\n",
    "    inp, targets, meanflow = DataPreprocessing(data, ff=i)\n",
    "    x = tf.concat([inp, meanflow], axis=1)\n",
    "    X = tf.concat([X, x], axis=0)\n",
    "    Y = tf.concat([Y, targets], axis = 0)\n",
    "\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "inp, Y_test, meanflow = DataPreprocessing(data, ff=0.00)\n",
    "X_test = tf.concat([inp, meanflow],1)\n",
    "for i in fval[1:]:\n",
    "    t1, t2, t3 = DataPreprocessing(data, ff=i)\n",
    "    x = tf.concat([t1, t3], axis=1)\n",
    "    X_test = tf.concat([X_test, x], axis=0)\n",
    "    Y_test = tf.concat([Y_test, t2], axis = 0)\n",
    "\n",
    "\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxilary Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_sequence(tensor, seq_length):\n",
    "    # tensor shape: (N, 2)\n",
    "    # seq_length: desired sequence length\n",
    "\n",
    "    N = tensor.shape[0]\n",
    "    transformed_sequences = []\n",
    "\n",
    "    for i in range(N - seq_length + 1):\n",
    "        sequence = tensor[i : i + seq_length]\n",
    "        transformed_sequences.append(sequence)\n",
    "\n",
    "    transformed_tensor = tf.stack(transformed_sequences)\n",
    "\n",
    "    # transformed_tensor shape: (N - seq_length + 1, seq_length, 2)\n",
    "    return tf.cast(transformed_tensor, dtype=tf.float32)\n",
    "\n",
    "def transform_original(tensor):\n",
    "    new_tensor = tensor[:, 0, :]\n",
    "    temp2 = tf.concat([new_tensor, tensor[-1][1:,:]], axis=0)\n",
    "    return tf.cast(temp2, dtype=tf.float32)\n",
    "\n",
    "def pde_residual(r_n, gr):\n",
    "    return r_n + gr\n",
    "\n",
    "def RHS_ff_t(y, baseflow, f):\n",
    "\n",
    "    y = tf.cast(y, dtype=tf.float32)\n",
    "    f = tf.cast(f, dtype=tf.float32)\n",
    "    baseflow = tf.cast(baseflow, dtype=tf.float32)\n",
    "\n",
    "    pi_p, pi_m, sig = tf.unstack(y,axis=1)\n",
    "\n",
    "    D = baseflow[:,0]\n",
    "    M = baseflow[:,1]\n",
    "    u = baseflow[:,2]\n",
    "    dMdx = baseflow[:,3]\n",
    "    dudx = baseflow[:,4]\n",
    "    alpha = baseflow[:,5]\n",
    "    eta1 = baseflow[:,6]    \n",
    "    \n",
    "    He = 0\n",
    "    # He = tf.constant(0.0, dtype=tf.float32) #a constant input\n",
    "\n",
    "    gamma = 1.4#constant\n",
    "    \n",
    "    Msq = tf.math.square(M)\n",
    "\n",
    "    Lambda = 1 + Msq * (gamma-1)/2\n",
    "    zeta = f*gamma*Msq - 2*tf.math.tan(alpha)\n",
    "    C1= ((gamma - 1)*(1-Msq)*f)/(2*Lambda*zeta)\n",
    "    Ca = -C1*M*u*dMdx*(2-(2*Msq/(1-Msq)) - (2*gamma*Msq*(-2*f*Lambda - (gamma-1)/gamma *zeta)/(2*Lambda*zeta)))\n",
    "    Ff = -(dudx + (4*f*u/(2*D)))\n",
    "\n",
    "    denom = (M**2*u - u + C1*Msq *u + C1*Msq*Msq*gamma*u) #M**4\n",
    "    vrh_p = M*(2*(1-M) + C1*M*(M-2+M*gamma*(1-2*M)))/(2*denom)\n",
    "    vrh_m = -M*(2*(1+M) + C1*M*(M+2+M*gamma*(1+2*M)))/(2*denom)\n",
    "    vkp_p = Msq*C1*(2+M*(gamma-1))/(2*denom)\n",
    "    vkp_m = Msq*C1*(2-M*(gamma-1))/(2*denom)\n",
    "    vth_p = C1*M*(M*(1+gamma) + M**2*(1-gamma) - 2)/denom\n",
    "    vth_m = C1*M*(M*(1+gamma) - M**2*(1-gamma) + 2)/denom\n",
    "    vsig = -(C1*Msq*(1+gamma*Msq) + Msq - 1)/denom\n",
    "    calM = dMdx/(2*M)\n",
    "    kp_p = (gamma - 1) + (2/M)\n",
    "    kp_m = (gamma - 1) - (2/M)\n",
    "    Gm_p = M*(Ca*(M + 1) + Ff*M*(C1*gamma*M*Msq + M + (1 - C1*Msq)))/(2*denom)\n",
    "    Gm_m = M*(Ca*(M - 1) - Ff*M*(C1*gamma*M*Msq + M - (1 - C1*Msq)))/(2*denom)\n",
    "    Ups = (Ca*(Msq - 1) - C1*Ff*Msq*Msq *(1+gamma))/denom\n",
    "    \n",
    "#     eq1 = - (2*np.pi*1j*He*vrh_p + Gm_m*kp_m + calM)*pi_p + (2*np.pi*1j*He*vkp_p + Gm_m*kp_p + calM)*pi_m - Gm_m*sig\n",
    "#     eq2 = - (2*np.pi*1j*He*vkp_m + Gm_p*kp_m - calM)*pi_p - (2*np.pi*1j*He*vrh_m + Gm_p*kp_p + calM)*pi_m - Gm_m*sig\n",
    "#     eq3 = - (2*np.pi*1j*He*vth_p + Ups*kp_m)*pi_p - (2*np.pi*1j*He*vth_m + Ups*kp_p)*pi_m - (2*np.pi*1j*He*vsig + Ups)*sig\n",
    "    \n",
    "#     eq1 = - tf.multiply((Gm_m*kp_m + calM),pi_p) + tf.multiply(( Gm_m*kp_p + calM),pi_m) - tf.multiply(Gm_m,sig)\n",
    "    eq1 =  (Gm_m*kp_m + calM)*pi_p + ( Gm_m*kp_p - calM)*pi_m + Gm_m*sig\n",
    "    eq2 =  ( Gm_p*kp_m - calM)*pi_p + ( Gm_p*kp_p + calM)*pi_m + Gm_m*sig\n",
    "    eq3 =  ( Ups*kp_m)*pi_p + ( Ups*kp_p)*pi_m + (Ups)*sig\n",
    "    \n",
    "\n",
    "    rhs = tf.stack([-eq1, -eq2, -eq3],axis=1)\n",
    "\n",
    "    return tf.cast(rhs, dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model():\n",
    "\n",
    "    ### Keras Functional API\n",
    "    inputs = keras.Input(shape= (None, 9))\n",
    "    # o1 = keras.layers.LSTM(16, activation='tanh', return_sequences=True)(inputs)\n",
    "    # o1 = keras.layers.Dense(1, activation='sigmoid')(o1)\n",
    "\n",
    "\n",
    "    o2 = keras.layers.Dense(200, activation='tanh')(inputs)\n",
    "    o2 = keras.layers.Dense(200, activation='tanh')(o2)\n",
    "    o2 = keras.layers.Dense(200, activation='tanh')(o2)\n",
    "    o2 = keras.layers.Dense(200, activation='tanh')(o2)\n",
    "    o2 = keras.layers.Dense(200, activation='tanh')(o2)\n",
    "    o2 = keras.layers.Dense(200, activation='tanh')(o2)\n",
    "    o2 = keras.layers.Dense(3, activation='sigmoid')(o2)\n",
    "    \n",
    "    model = keras.Model(inputs = inputs, outputs=o2)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, None, 9)]         0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, None, 200)         2000      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, None, 200)         40200     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, None, 200)         40200     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, None, 200)         40200     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, None, 200)         40200     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, None, 200)         40200     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, None, 3)           603       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 203603 (795.32 KB)\n",
      "Trainable params: 203603 (795.32 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = init_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.keras.optimizers.legacy.Adam(\n",
    "    learning_rate=0.01,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.9,\n",
    "    epsilon=1e-07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer=optim)\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.000001, patience=500, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "  1/313 [..............................] - ETA: 1:22 - loss: 4.2386e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-12 20:26:09.501782: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - ETA: 0s - loss: 4.9445e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-12 20:26:12.036332: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 9ms/step - loss: 4.9445e-04 - val_loss: 7.8847e-04\n",
      "Epoch 2/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.4410e-04 - val_loss: 3.9459e-04\n",
      "Epoch 3/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.8668e-04 - val_loss: 7.4989e-04\n",
      "Epoch 4/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.7019e-04 - val_loss: 4.3187e-04\n",
      "Epoch 5/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.7081e-04 - val_loss: 5.9660e-04\n",
      "Epoch 6/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.1779e-04 - val_loss: 8.1844e-04\n",
      "Epoch 7/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.7117e-04 - val_loss: 7.1484e-04\n",
      "Epoch 8/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5851e-04 - val_loss: 4.0553e-04\n",
      "Epoch 9/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.7179e-04 - val_loss: 7.9602e-04\n",
      "Epoch 10/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8412e-04 - val_loss: 4.9465e-04\n",
      "Epoch 11/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.9937e-04 - val_loss: 7.0530e-04\n",
      "Epoch 12/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7080e-04 - val_loss: 9.0556e-04\n",
      "Epoch 13/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5992e-04 - val_loss: 6.9652e-04\n",
      "Epoch 14/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5204e-04 - val_loss: 3.7300e-04\n",
      "Epoch 15/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6341e-04 - val_loss: 6.8700e-04\n",
      "Epoch 16/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4060e-04 - val_loss: 3.6765e-04\n",
      "Epoch 17/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5371e-04 - val_loss: 3.7211e-04\n",
      "Epoch 18/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6723e-04 - val_loss: 3.6155e-04\n",
      "Epoch 19/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.0331e-04 - val_loss: 8.9548e-04\n",
      "Epoch 20/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4712e-04 - val_loss: 5.3227e-04\n",
      "Epoch 21/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4166e-04 - val_loss: 3.3031e-04\n",
      "Epoch 22/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8351e-04 - val_loss: 5.9540e-04\n",
      "Epoch 23/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.0369e-04 - val_loss: 4.9171e-04\n",
      "Epoch 24/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.8345e-04 - val_loss: 6.3600e-04\n",
      "Epoch 25/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.6229e-04 - val_loss: 7.8887e-04\n",
      "Epoch 26/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.4733e-04 - val_loss: 3.3371e-04\n",
      "Epoch 27/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8546e-04 - val_loss: 4.5859e-04\n",
      "Epoch 28/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.0795e-04 - val_loss: 4.9667e-04\n",
      "Epoch 29/5000\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 5.0182e-04 - val_loss: 3.2854e-04\n",
      "Epoch 30/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.0013e-04 - val_loss: 3.7260e-04\n",
      "Epoch 31/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.7981e-04 - val_loss: 7.0758e-04\n",
      "Epoch 32/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.2628e-04 - val_loss: 3.7488e-04\n",
      "Epoch 33/5000\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 4.7381e-04 - val_loss: 5.2128e-04\n",
      "Epoch 34/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.9709e-04 - val_loss: 3.9620e-04\n",
      "Epoch 35/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8275e-04 - val_loss: 5.0248e-04\n",
      "Epoch 36/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.0521e-04 - val_loss: 5.1205e-04\n",
      "Epoch 37/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9847e-04 - val_loss: 4.0260e-04\n",
      "Epoch 38/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9696e-04 - val_loss: 4.6209e-04\n",
      "Epoch 39/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.5700e-04 - val_loss: 3.3480e-04\n",
      "Epoch 40/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.2389e-04 - val_loss: 3.6865e-04\n",
      "Epoch 41/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7524e-04 - val_loss: 3.3063e-04\n",
      "Epoch 42/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.7217e-04 - val_loss: 8.6366e-04\n",
      "Epoch 43/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.9559e-04 - val_loss: 6.6816e-04\n",
      "Epoch 44/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.2960e-04 - val_loss: 7.9222e-04\n",
      "Epoch 45/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.7004e-04 - val_loss: 5.2386e-04\n",
      "Epoch 46/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.1717e-04 - val_loss: 4.3325e-04\n",
      "Epoch 47/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.0208e-04 - val_loss: 3.9131e-04\n",
      "Epoch 48/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.0578e-04 - val_loss: 7.3656e-04\n",
      "Epoch 49/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.3893e-04 - val_loss: 8.1218e-04\n",
      "Epoch 50/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5610e-04 - val_loss: 4.3627e-04\n",
      "Epoch 51/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.8146e-04 - val_loss: 4.7423e-04\n",
      "Epoch 52/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9519e-04 - val_loss: 6.4823e-04\n",
      "Epoch 53/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.7606e-04 - val_loss: 7.8816e-04\n",
      "Epoch 54/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.7730e-04 - val_loss: 3.9578e-04\n",
      "Epoch 55/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.8920e-04 - val_loss: 2.9148e-04\n",
      "Epoch 56/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.9990e-04 - val_loss: 7.2652e-04\n",
      "Epoch 57/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.5324e-04 - val_loss: 8.1889e-04\n",
      "Epoch 58/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.3012e-04 - val_loss: 6.5535e-04\n",
      "Epoch 59/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.7419e-04 - val_loss: 4.2717e-04\n",
      "Epoch 60/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.2601e-04 - val_loss: 4.5451e-04\n",
      "Epoch 61/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.3416e-04 - val_loss: 6.8536e-04\n",
      "Epoch 62/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.6163e-04 - val_loss: 7.7883e-04\n",
      "Epoch 63/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.5431e-04 - val_loss: 8.4602e-04\n",
      "Epoch 64/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.2551e-04 - val_loss: 8.0354e-04\n",
      "Epoch 65/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.3558e-04 - val_loss: 5.5822e-04\n",
      "Epoch 66/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9767e-04 - val_loss: 6.9420e-04\n",
      "Epoch 67/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9338e-04 - val_loss: 4.7663e-04\n",
      "Epoch 68/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7216e-04 - val_loss: 3.5638e-04\n",
      "Epoch 69/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9774e-04 - val_loss: 7.8775e-04\n",
      "Epoch 70/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.8169e-04 - val_loss: 5.3383e-04\n",
      "Epoch 71/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4194e-04 - val_loss: 2.6366e-04\n",
      "Epoch 72/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.0400e-04 - val_loss: 4.5373e-04\n",
      "Epoch 73/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.4752e-04 - val_loss: 4.9146e-04\n",
      "Epoch 74/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.4143e-04 - val_loss: 6.3877e-04\n",
      "Epoch 75/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.6111e-04 - val_loss: 3.6999e-04\n",
      "Epoch 76/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.8619e-04 - val_loss: 7.1571e-04\n",
      "Epoch 77/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6362e-04 - val_loss: 3.8599e-04\n",
      "Epoch 78/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.3545e-04 - val_loss: 5.8314e-04\n",
      "Epoch 79/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.0865e-04 - val_loss: 7.9949e-04\n",
      "Epoch 80/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.0315e-04 - val_loss: 4.6137e-04\n",
      "Epoch 81/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.1459e-04 - val_loss: 4.4201e-04\n",
      "Epoch 82/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.0044e-04 - val_loss: 3.4465e-04\n",
      "Epoch 83/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.3212e-04 - val_loss: 4.1961e-04\n",
      "Epoch 84/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6226e-04 - val_loss: 4.0350e-04\n",
      "Epoch 85/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6106e-04 - val_loss: 3.3299e-04\n",
      "Epoch 86/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4417e-04 - val_loss: 5.8953e-04\n",
      "Epoch 87/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.0993e-04 - val_loss: 7.1509e-04\n",
      "Epoch 88/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.7701e-04 - val_loss: 6.9336e-04\n",
      "Epoch 89/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4013e-04 - val_loss: 6.7162e-04\n",
      "Epoch 90/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4412e-04 - val_loss: 2.5761e-04\n",
      "Epoch 91/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6566e-04 - val_loss: 6.9475e-04\n",
      "Epoch 92/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.8980e-04 - val_loss: 2.6545e-04\n",
      "Epoch 93/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.8157e-04 - val_loss: 3.7319e-04\n",
      "Epoch 94/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5547e-04 - val_loss: 3.7723e-04\n",
      "Epoch 95/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.0194e-04 - val_loss: 2.8924e-04\n",
      "Epoch 96/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4478e-04 - val_loss: 2.7335e-04\n",
      "Epoch 97/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9378e-04 - val_loss: 4.0630e-04\n",
      "Epoch 98/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6724e-04 - val_loss: 5.0673e-04\n",
      "Epoch 99/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6421e-04 - val_loss: 7.4758e-04\n",
      "Epoch 100/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.5941e-04 - val_loss: 4.7713e-04\n",
      "Epoch 101/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.4896e-04 - val_loss: 8.3682e-04\n",
      "Epoch 102/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.8131e-04 - val_loss: 3.0914e-04\n",
      "Epoch 103/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7492e-04 - val_loss: 4.2443e-04\n",
      "Epoch 104/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.2762e-04 - val_loss: 6.4715e-04\n",
      "Epoch 105/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.7680e-04 - val_loss: 2.8069e-04\n",
      "Epoch 106/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.2151e-04 - val_loss: 6.6332e-04\n",
      "Epoch 107/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.6003e-04 - val_loss: 7.5240e-04\n",
      "Epoch 108/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.6065e-04 - val_loss: 8.6104e-04\n",
      "Epoch 109/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 7.2628e-04 - val_loss: 8.9994e-04\n",
      "Epoch 110/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 7.6392e-04 - val_loss: 9.6162e-04\n",
      "Epoch 111/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 8.2338e-04 - val_loss: 9.8836e-04\n",
      "Epoch 112/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 8.1672e-04 - val_loss: 9.2809e-04\n",
      "Epoch 113/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 7.3907e-04 - val_loss: 7.6638e-04\n",
      "Epoch 114/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.2287e-04 - val_loss: 5.4090e-04\n",
      "Epoch 115/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.2685e-04 - val_loss: 7.5044e-04\n",
      "Epoch 116/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.4907e-04 - val_loss: 8.1914e-04\n",
      "Epoch 117/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.9395e-04 - val_loss: 6.3046e-04\n",
      "Epoch 118/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.9159e-04 - val_loss: 6.0686e-04\n",
      "Epoch 119/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.2763e-04 - val_loss: 3.3518e-04\n",
      "Epoch 120/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.1397e-04 - val_loss: 6.1167e-04\n",
      "Epoch 121/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.4628e-04 - val_loss: 4.3287e-04\n",
      "Epoch 122/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.2965e-04 - val_loss: 3.6762e-04\n",
      "Epoch 123/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.0096e-04 - val_loss: 7.3441e-04\n",
      "Epoch 124/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9260e-04 - val_loss: 4.8158e-04\n",
      "Epoch 125/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5071e-04 - val_loss: 3.3876e-04\n",
      "Epoch 126/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.1241e-04 - val_loss: 5.3793e-04\n",
      "Epoch 127/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.2801e-04 - val_loss: 5.5817e-04\n",
      "Epoch 128/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.2738e-04 - val_loss: 5.8860e-04\n",
      "Epoch 129/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.6605e-04 - val_loss: 2.5705e-04\n",
      "Epoch 130/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7581e-04 - val_loss: 7.3087e-04\n",
      "Epoch 131/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6280e-04 - val_loss: 7.9278e-04\n",
      "Epoch 132/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4966e-04 - val_loss: 3.4090e-04\n",
      "Epoch 133/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.3933e-04 - val_loss: 5.7746e-04\n",
      "Epoch 134/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6109e-04 - val_loss: 7.2297e-04\n",
      "Epoch 135/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.3930e-04 - val_loss: 7.0711e-04\n",
      "Epoch 136/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4612e-04 - val_loss: 5.0020e-04\n",
      "Epoch 137/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.8156e-04 - val_loss: 2.9437e-04\n",
      "Epoch 138/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5304e-04 - val_loss: 2.8649e-04\n",
      "Epoch 139/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5034e-04 - val_loss: 7.8520e-04\n",
      "Epoch 140/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6975e-04 - val_loss: 3.7361e-04\n",
      "Epoch 141/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4525e-04 - val_loss: 3.9365e-04\n",
      "Epoch 142/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7343e-04 - val_loss: 7.8935e-04\n",
      "Epoch 143/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.2056e-04 - val_loss: 4.6785e-04\n",
      "Epoch 144/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5517e-04 - val_loss: 2.7873e-04\n",
      "Epoch 145/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.2132e-04 - val_loss: 4.8114e-04\n",
      "Epoch 146/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9840e-04 - val_loss: 4.5304e-04\n",
      "Epoch 147/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5265e-04 - val_loss: 4.7202e-04\n",
      "Epoch 148/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6364e-04 - val_loss: 4.5238e-04\n",
      "Epoch 149/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4682e-04 - val_loss: 5.8758e-04\n",
      "Epoch 150/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7225e-04 - val_loss: 7.5588e-04\n",
      "Epoch 151/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6530e-04 - val_loss: 3.5854e-04\n",
      "Epoch 152/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.3860e-04 - val_loss: 3.4524e-04\n",
      "Epoch 153/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.4942e-04 - val_loss: 3.4354e-04\n",
      "Epoch 154/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5643e-04 - val_loss: 3.1130e-04\n",
      "Epoch 155/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6806e-04 - val_loss: 7.0556e-04\n",
      "Epoch 156/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6332e-04 - val_loss: 3.5106e-04\n",
      "Epoch 157/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.6797e-04 - val_loss: 9.1459e-04\n",
      "Epoch 158/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9375e-04 - val_loss: 3.5332e-04\n",
      "Epoch 159/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6223e-04 - val_loss: 3.1107e-04\n",
      "Epoch 160/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8738e-04 - val_loss: 8.2215e-04\n",
      "Epoch 161/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.0235e-04 - val_loss: 7.9004e-04\n",
      "Epoch 162/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.1104e-04 - val_loss: 5.8913e-04\n",
      "Epoch 163/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.7052e-04 - val_loss: 6.5329e-04\n",
      "Epoch 164/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.5501e-04 - val_loss: 4.1530e-04\n",
      "Epoch 165/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.2040e-04 - val_loss: 7.8936e-04\n",
      "Epoch 166/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.3316e-04 - val_loss: 3.3213e-04\n",
      "Epoch 167/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5455e-04 - val_loss: 6.1239e-04\n",
      "Epoch 168/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5794e-04 - val_loss: 6.1785e-04\n",
      "Epoch 169/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4391e-04 - val_loss: 3.9264e-04\n",
      "Epoch 170/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6503e-04 - val_loss: 7.3461e-04\n",
      "Epoch 171/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5665e-04 - val_loss: 3.5891e-04\n",
      "Epoch 172/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6580e-04 - val_loss: 4.1789e-04\n",
      "Epoch 173/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.3026e-04 - val_loss: 3.0469e-04\n",
      "Epoch 174/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5286e-04 - val_loss: 7.0084e-04\n",
      "Epoch 175/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6316e-04 - val_loss: 5.5200e-04\n",
      "Epoch 176/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9658e-04 - val_loss: 0.0012\n",
      "Epoch 177/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.9441e-04 - val_loss: 7.8107e-04\n",
      "Epoch 178/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.2216e-04 - val_loss: 8.3711e-04\n",
      "Epoch 179/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6881e-04 - val_loss: 3.1886e-04\n",
      "Epoch 180/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7072e-04 - val_loss: 5.2123e-04\n",
      "Epoch 181/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4477e-04 - val_loss: 4.8165e-04\n",
      "Epoch 182/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7840e-04 - val_loss: 6.2682e-04\n",
      "Epoch 183/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.3950e-04 - val_loss: 4.8323e-04\n",
      "Epoch 184/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5299e-04 - val_loss: 3.9078e-04\n",
      "Epoch 185/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.2350e-04 - val_loss: 7.6460e-04\n",
      "Epoch 186/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7400e-04 - val_loss: 4.0400e-04\n",
      "Epoch 187/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5137e-04 - val_loss: 3.7029e-04\n",
      "Epoch 188/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9109e-04 - val_loss: 4.2168e-04\n",
      "Epoch 189/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4052e-04 - val_loss: 3.2128e-04\n",
      "Epoch 190/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4083e-04 - val_loss: 5.0364e-04\n",
      "Epoch 191/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5752e-04 - val_loss: 4.2603e-04\n",
      "Epoch 192/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4669e-04 - val_loss: 7.4843e-04\n",
      "Epoch 193/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.5143e-04 - val_loss: 5.1115e-04\n",
      "Epoch 194/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.7981e-04 - val_loss: 5.5873e-04\n",
      "Epoch 195/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.0302e-04 - val_loss: 7.9489e-04\n",
      "Epoch 196/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.0842e-04 - val_loss: 4.3560e-04\n",
      "Epoch 197/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.8322e-04 - val_loss: 4.4464e-04\n",
      "Epoch 198/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.8235e-04 - val_loss: 6.8648e-04\n",
      "Epoch 199/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.1533e-04 - val_loss: 3.8217e-04\n",
      "Epoch 200/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6275e-04 - val_loss: 4.4561e-04\n",
      "Epoch 201/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7592e-04 - val_loss: 5.0161e-04\n",
      "Epoch 202/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.0059e-04 - val_loss: 4.5496e-04\n",
      "Epoch 203/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9223e-04 - val_loss: 4.1036e-04\n",
      "Epoch 204/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7138e-04 - val_loss: 5.9190e-04\n",
      "Epoch 205/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.8620e-04 - val_loss: 7.1660e-04\n",
      "Epoch 206/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.2417e-04 - val_loss: 2.8064e-04\n",
      "Epoch 207/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.2246e-04 - val_loss: 7.3147e-04\n",
      "Epoch 208/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.2177e-04 - val_loss: 8.5586e-04\n",
      "Epoch 209/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6016e-04 - val_loss: 3.3824e-04\n",
      "Epoch 210/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6358e-04 - val_loss: 3.3658e-04\n",
      "Epoch 211/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.8665e-04 - val_loss: 8.0965e-04\n",
      "Epoch 212/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.4938e-04 - val_loss: 4.4879e-04\n",
      "Epoch 213/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.3131e-04 - val_loss: 2.9446e-04\n",
      "Epoch 214/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6194e-04 - val_loss: 7.9123e-04\n",
      "Epoch 215/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5631e-04 - val_loss: 3.8038e-04\n",
      "Epoch 216/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.8403e-04 - val_loss: 3.1929e-04\n",
      "Epoch 217/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6229e-04 - val_loss: 5.3449e-04\n",
      "Epoch 218/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9299e-04 - val_loss: 3.1311e-04\n",
      "Epoch 219/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.0330e-04 - val_loss: 3.1960e-04\n",
      "Epoch 220/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6042e-04 - val_loss: 3.4234e-04\n",
      "Epoch 221/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7555e-04 - val_loss: 7.0320e-04\n",
      "Epoch 222/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5438e-04 - val_loss: 4.7697e-04\n",
      "Epoch 223/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6218e-04 - val_loss: 4.5491e-04\n",
      "Epoch 224/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.8117e-04 - val_loss: 2.8402e-04\n",
      "Epoch 225/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6541e-04 - val_loss: 3.6762e-04\n",
      "Epoch 226/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.0575e-04 - val_loss: 4.9409e-04\n",
      "Epoch 227/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7709e-04 - val_loss: 3.2042e-04\n",
      "Epoch 228/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7189e-04 - val_loss: 4.5631e-04\n",
      "Epoch 229/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.9501e-04 - val_loss: 2.7586e-04\n",
      "Epoch 230/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.5422e-04 - val_loss: 6.0958e-04\n",
      "Epoch 231/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7771e-04 - val_loss: 4.5805e-04\n",
      "Epoch 232/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7672e-04 - val_loss: 4.6967e-04\n",
      "Epoch 233/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.2721e-04 - val_loss: 5.5701e-04\n",
      "Epoch 234/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.2625e-04 - val_loss: 4.8445e-04\n",
      "Epoch 235/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.8363e-04 - val_loss: 4.7117e-04\n",
      "Epoch 236/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.8345e-04 - val_loss: 5.3953e-04\n",
      "Epoch 237/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7170e-04 - val_loss: 7.1874e-04\n",
      "Epoch 238/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.8088e-04 - val_loss: 6.4519e-04\n",
      "Epoch 239/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.3418e-04 - val_loss: 4.1657e-04\n",
      "Epoch 240/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.9683e-04 - val_loss: 7.0985e-04\n",
      "Epoch 241/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.8264e-04 - val_loss: 4.7614e-04\n",
      "Epoch 242/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6214e-04 - val_loss: 3.0279e-04\n",
      "Epoch 243/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.1267e-04 - val_loss: 8.6855e-04\n",
      "Epoch 244/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.1705e-04 - val_loss: 4.6565e-04\n",
      "Epoch 245/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6474e-04 - val_loss: 5.8869e-04\n",
      "Epoch 246/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.6555e-04 - val_loss: 4.1377e-04\n",
      "Epoch 247/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.7872e-04 - val_loss: 6.1975e-04\n",
      "Epoch 248/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.9822e-04 - val_loss: 2.8547e-04\n",
      "Epoch 249/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.9506e-04 - val_loss: 7.2769e-04\n",
      "Epoch 250/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.7564e-04 - val_loss: 5.4293e-04\n",
      "Epoch 251/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.9490e-04 - val_loss: 8.1804e-04\n",
      "Epoch 252/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.1316e-04 - val_loss: 4.5477e-04\n",
      "Epoch 253/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6240e-04 - val_loss: 8.9115e-04\n",
      "Epoch 254/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.3658e-04 - val_loss: 4.4744e-04\n",
      "Epoch 255/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.4968e-04 - val_loss: 3.4017e-04\n",
      "Epoch 256/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.2931e-04 - val_loss: 6.0794e-04\n",
      "Epoch 257/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.6446e-04 - val_loss: 4.9030e-04\n",
      "Epoch 258/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.4411e-04 - val_loss: 5.3535e-04\n",
      "Epoch 259/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.2905e-04 - val_loss: 4.8164e-04\n",
      "Epoch 260/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.3353e-04 - val_loss: 5.6584e-04\n",
      "Epoch 261/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.3445e-04 - val_loss: 7.4608e-04\n",
      "Epoch 262/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.0853e-04 - val_loss: 3.6784e-04\n",
      "Epoch 263/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7942e-04 - val_loss: 5.7435e-04\n",
      "Epoch 264/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.8894e-04 - val_loss: 9.6430e-04\n",
      "Epoch 265/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.1831e-04 - val_loss: 4.0465e-04\n",
      "Epoch 266/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.6498e-04 - val_loss: 8.0617e-04\n",
      "Epoch 267/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.5513e-04 - val_loss: 6.0455e-04\n",
      "Epoch 268/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.0444e-04 - val_loss: 7.7929e-04\n",
      "Epoch 269/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.4687e-04 - val_loss: 8.2691e-04\n",
      "Epoch 270/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.7546e-04 - val_loss: 8.0649e-04\n",
      "Epoch 271/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.3159e-04 - val_loss: 6.8004e-04\n",
      "Epoch 272/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.5548e-04 - val_loss: 8.1751e-04\n",
      "Epoch 273/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.4095e-04 - val_loss: 5.1091e-04\n",
      "Epoch 274/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8245e-04 - val_loss: 8.1659e-04\n",
      "Epoch 275/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.2677e-04 - val_loss: 6.9177e-04\n",
      "Epoch 276/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.7763e-04 - val_loss: 5.7038e-04\n",
      "Epoch 277/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.6182e-04 - val_loss: 6.2190e-04\n",
      "Epoch 278/5000\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 4.7205e-04 - val_loss: 4.7666e-04\n",
      "Epoch 279/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.7058e-04 - val_loss: 8.3231e-04\n",
      "Epoch 280/5000\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 5.4048e-04 - val_loss: 4.9526e-04\n",
      "Epoch 281/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.0187e-04 - val_loss: 5.0177e-04\n",
      "Epoch 282/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.1972e-04 - val_loss: 6.4387e-04\n",
      "Epoch 283/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.1722e-04 - val_loss: 5.8337e-04\n",
      "Epoch 284/5000\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 5.1466e-04 - val_loss: 5.2132e-04\n",
      "Epoch 285/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.0527e-04 - val_loss: 6.6887e-04\n",
      "Epoch 286/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 6.0236e-04 - val_loss: 5.9903e-04\n",
      "Epoch 287/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5811e-04 - val_loss: 6.3724e-04\n",
      "Epoch 288/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.0830e-04 - val_loss: 7.1136e-04\n",
      "Epoch 289/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.1944e-04 - val_loss: 5.0941e-04\n",
      "Epoch 290/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8582e-04 - val_loss: 2.8174e-04\n",
      "Epoch 291/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.0101e-04 - val_loss: 5.8190e-04\n",
      "Epoch 292/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5718e-04 - val_loss: 3.6911e-04\n",
      "Epoch 293/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.6929e-04 - val_loss: 4.3719e-04\n",
      "Epoch 294/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 6.1261e-04 - val_loss: 9.0331e-04\n",
      "Epoch 295/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 8.7136e-04 - val_loss: 4.0632e-04\n",
      "Epoch 296/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.9127e-04 - val_loss: 6.8141e-04\n",
      "Epoch 297/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.8511e-04 - val_loss: 9.0160e-04\n",
      "Epoch 298/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 7.3927e-04 - val_loss: 8.3667e-04\n",
      "Epoch 299/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.9539e-04 - val_loss: 7.9959e-04\n",
      "Epoch 300/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 6.4549e-04 - val_loss: 7.9105e-04\n",
      "Epoch 301/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.6506e-04 - val_loss: 3.8056e-04\n",
      "Epoch 302/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.9325e-04 - val_loss: 4.6556e-04\n",
      "Epoch 303/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4642e-04 - val_loss: 3.0232e-04\n",
      "Epoch 304/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8669e-04 - val_loss: 6.3936e-04\n",
      "Epoch 305/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8796e-04 - val_loss: 4.4363e-04\n",
      "Epoch 306/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8251e-04 - val_loss: 8.1131e-04\n",
      "Epoch 307/5000\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 6.7640e-04 - val_loss: 7.9615e-04\n",
      "Epoch 308/5000\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 5.6382e-04 - val_loss: 3.8330e-04\n",
      "Epoch 309/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 6.1660e-04 - val_loss: 7.3984e-04\n",
      "Epoch 310/5000\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 5.0804e-04 - val_loss: 2.9899e-04\n",
      "Epoch 311/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.9276e-04 - val_loss: 6.7905e-04\n",
      "Epoch 312/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.7378e-04 - val_loss: 3.5770e-04\n",
      "Epoch 313/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.6515e-04 - val_loss: 7.8677e-04\n",
      "Epoch 314/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.6798e-04 - val_loss: 6.5596e-04\n",
      "Epoch 315/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.1553e-04 - val_loss: 4.9378e-04\n",
      "Epoch 316/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5031e-04 - val_loss: 7.8533e-04\n",
      "Epoch 317/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.6401e-04 - val_loss: 7.1610e-04\n",
      "Epoch 318/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7225e-04 - val_loss: 8.0360e-04\n",
      "Epoch 319/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9228e-04 - val_loss: 4.5213e-04\n",
      "Epoch 320/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.3804e-04 - val_loss: 3.8940e-04\n",
      "Epoch 321/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.2206e-04 - val_loss: 7.8749e-04\n",
      "Epoch 322/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.6002e-04 - val_loss: 7.0562e-04\n",
      "Epoch 323/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.5900e-04 - val_loss: 7.5580e-04\n",
      "Epoch 324/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.7532e-04 - val_loss: 7.6898e-04\n",
      "Epoch 325/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.4288e-04 - val_loss: 9.0131e-04\n",
      "Epoch 326/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 7.9370e-04 - val_loss: 9.7137e-04\n",
      "Epoch 327/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 8.3924e-04 - val_loss: 9.5083e-04\n",
      "Epoch 328/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 7.5334e-04 - val_loss: 6.9152e-04\n",
      "Epoch 329/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.9641e-04 - val_loss: 8.2245e-04\n",
      "Epoch 330/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 7.3196e-04 - val_loss: 0.0014\n",
      "Epoch 331/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.6967e-04 - val_loss: 6.2216e-04\n",
      "Epoch 332/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.3775e-04 - val_loss: 5.3910e-04\n",
      "Epoch 333/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.8062e-04 - val_loss: 3.8562e-04\n",
      "Epoch 334/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.3895e-04 - val_loss: 4.9053e-04\n",
      "Epoch 335/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.3788e-04 - val_loss: 4.7942e-04\n",
      "Epoch 336/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6142e-04 - val_loss: 3.9335e-04\n",
      "Epoch 337/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4978e-04 - val_loss: 6.1328e-04\n",
      "Epoch 338/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.0965e-04 - val_loss: 5.1415e-04\n",
      "Epoch 339/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.3070e-04 - val_loss: 3.8018e-04\n",
      "Epoch 340/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.9510e-04 - val_loss: 3.1215e-04\n",
      "Epoch 341/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7211e-04 - val_loss: 2.7805e-04\n",
      "Epoch 342/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.5926e-04 - val_loss: 3.3413e-04\n",
      "Epoch 343/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.3597e-04 - val_loss: 2.7567e-04\n",
      "Epoch 344/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4332e-04 - val_loss: 2.8112e-04\n",
      "Epoch 345/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6398e-04 - val_loss: 6.2673e-04\n",
      "Epoch 346/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.3958e-04 - val_loss: 3.5224e-04\n",
      "Epoch 347/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4805e-04 - val_loss: 4.8872e-04\n",
      "Epoch 348/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9858e-04 - val_loss: 8.4030e-04\n",
      "Epoch 349/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5907e-04 - val_loss: 4.0725e-04\n",
      "Epoch 350/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.2299e-04 - val_loss: 5.1265e-04\n",
      "Epoch 351/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.7425e-04 - val_loss: 8.4157e-04\n",
      "Epoch 352/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5318e-04 - val_loss: 3.4557e-04\n",
      "Epoch 353/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4973e-04 - val_loss: 5.0215e-04\n",
      "Epoch 354/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5407e-04 - val_loss: 5.2088e-04\n",
      "Epoch 355/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6347e-04 - val_loss: 7.2718e-04\n",
      "Epoch 356/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5181e-04 - val_loss: 3.2788e-04\n",
      "Epoch 357/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7969e-04 - val_loss: 4.2809e-04\n",
      "Epoch 358/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.4258e-04 - val_loss: 4.6388e-04\n",
      "Epoch 359/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7973e-04 - val_loss: 6.8232e-04\n",
      "Epoch 360/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7187e-04 - val_loss: 4.6553e-04\n",
      "Epoch 361/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.6335e-04 - val_loss: 4.1066e-04\n",
      "Epoch 362/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6714e-04 - val_loss: 3.0941e-04\n",
      "Epoch 363/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5320e-04 - val_loss: 6.1620e-04\n",
      "Epoch 364/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5425e-04 - val_loss: 2.6945e-04\n",
      "Epoch 365/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7588e-04 - val_loss: 7.1886e-04\n",
      "Epoch 366/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.8292e-04 - val_loss: 3.2651e-04\n",
      "Epoch 367/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.6031e-04 - val_loss: 4.6436e-04\n",
      "Epoch 368/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.3604e-04 - val_loss: 6.1904e-04\n",
      "Epoch 369/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.8756e-04 - val_loss: 3.7923e-04\n",
      "Epoch 370/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7001e-04 - val_loss: 5.6206e-04\n",
      "Epoch 371/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.3101e-04 - val_loss: 9.0168e-04\n",
      "Epoch 372/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.4572e-04 - val_loss: 3.9916e-04\n",
      "Epoch 373/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4220e-04 - val_loss: 4.1975e-04\n",
      "Epoch 374/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4900e-04 - val_loss: 7.9452e-04\n",
      "Epoch 375/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.3552e-04 - val_loss: 6.8770e-04\n",
      "Epoch 376/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7817e-04 - val_loss: 3.4819e-04\n",
      "Epoch 377/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5291e-04 - val_loss: 7.5966e-04\n",
      "Epoch 378/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7648e-04 - val_loss: 5.8326e-04\n",
      "Epoch 379/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.1755e-04 - val_loss: 7.5107e-04\n",
      "Epoch 380/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.5370e-04 - val_loss: 8.7294e-04\n",
      "Epoch 381/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.7196e-04 - val_loss: 3.4141e-04\n",
      "Epoch 382/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9001e-04 - val_loss: 3.5062e-04\n",
      "Epoch 383/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9423e-04 - val_loss: 5.5489e-04\n",
      "Epoch 384/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4519e-04 - val_loss: 3.8778e-04\n",
      "Epoch 385/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5356e-04 - val_loss: 3.2761e-04\n",
      "Epoch 386/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5348e-04 - val_loss: 9.8711e-04\n",
      "Epoch 387/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7403e-04 - val_loss: 7.9199e-04\n",
      "Epoch 388/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5748e-04 - val_loss: 5.3619e-04\n",
      "Epoch 389/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5575e-04 - val_loss: 3.3994e-04\n",
      "Epoch 390/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6964e-04 - val_loss: 5.0802e-04\n",
      "Epoch 391/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.2464e-04 - val_loss: 9.2782e-04\n",
      "Epoch 392/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5626e-04 - val_loss: 2.6521e-04\n",
      "Epoch 393/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6446e-04 - val_loss: 5.8393e-04\n",
      "Epoch 394/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.6015e-04 - val_loss: 6.9338e-04\n",
      "Epoch 395/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9285e-04 - val_loss: 4.1487e-04\n",
      "Epoch 396/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.0342e-04 - val_loss: 6.0191e-04\n",
      "Epoch 397/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.3805e-04 - val_loss: 3.8742e-04\n",
      "Epoch 398/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.8262e-04 - val_loss: 3.5689e-04\n",
      "Epoch 399/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7739e-04 - val_loss: 5.2456e-04\n",
      "Epoch 400/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.3532e-04 - val_loss: 3.6016e-04\n",
      "Epoch 401/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.3788e-04 - val_loss: 8.5885e-04\n",
      "Epoch 402/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6907e-04 - val_loss: 3.5221e-04\n",
      "Epoch 403/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7946e-04 - val_loss: 5.6442e-04\n",
      "Epoch 404/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5048e-04 - val_loss: 2.8675e-04\n",
      "Epoch 405/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4377e-04 - val_loss: 3.8067e-04\n",
      "Epoch 406/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.3502e-04 - val_loss: 4.9005e-04\n",
      "Epoch 407/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.2056e-04 - val_loss: 7.2237e-04\n",
      "Epoch 408/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.5946e-04 - val_loss: 6.3856e-04\n",
      "Epoch 409/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.0814e-04 - val_loss: 7.5296e-04\n",
      "Epoch 410/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5968e-04 - val_loss: 2.7840e-04\n",
      "Epoch 411/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.3902e-04 - val_loss: 4.6152e-04\n",
      "Epoch 412/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6624e-04 - val_loss: 7.8732e-04\n",
      "Epoch 413/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.0859e-04 - val_loss: 3.7219e-04\n",
      "Epoch 414/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5589e-04 - val_loss: 4.1845e-04\n",
      "Epoch 415/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5565e-04 - val_loss: 4.1815e-04\n",
      "Epoch 416/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.2018e-04 - val_loss: 4.5566e-04\n",
      "Epoch 417/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.4685e-04 - val_loss: 4.1087e-04\n",
      "Epoch 418/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.2683e-04 - val_loss: 8.6454e-04\n",
      "Epoch 419/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.5884e-04 - val_loss: 3.3685e-04\n",
      "Epoch 420/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5736e-04 - val_loss: 3.2419e-04\n",
      "Epoch 421/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7520e-04 - val_loss: 5.8197e-04\n",
      "Epoch 422/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5384e-04 - val_loss: 3.8935e-04\n",
      "Epoch 423/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9939e-04 - val_loss: 6.8542e-04\n",
      "Epoch 424/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.8331e-04 - val_loss: 4.2699e-04\n",
      "Epoch 425/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.9705e-04 - val_loss: 8.8202e-04\n",
      "Epoch 426/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.1325e-04 - val_loss: 7.9913e-04\n",
      "Epoch 427/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.4932e-04 - val_loss: 7.4280e-04\n",
      "Epoch 428/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.1501e-04 - val_loss: 4.8308e-04\n",
      "Epoch 429/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.7846e-04 - val_loss: 8.4003e-04\n",
      "Epoch 430/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.0396e-04 - val_loss: 8.3364e-04\n",
      "Epoch 431/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.4884e-04 - val_loss: 4.3381e-04\n",
      "Epoch 432/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.6927e-04 - val_loss: 6.5047e-04\n",
      "Epoch 433/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.9824e-04 - val_loss: 0.0010\n",
      "Epoch 434/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 7.8546e-04 - val_loss: 7.3382e-04\n",
      "Epoch 435/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 7.0628e-04 - val_loss: 8.4212e-04\n",
      "Epoch 436/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.8184e-04 - val_loss: 5.2388e-04\n",
      "Epoch 437/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.4077e-04 - val_loss: 5.6532e-04\n",
      "Epoch 438/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.8715e-04 - val_loss: 8.0959e-04\n",
      "Epoch 439/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 7.2666e-04 - val_loss: 6.4555e-04\n",
      "Epoch 440/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.4347e-04 - val_loss: 7.9617e-04\n",
      "Epoch 441/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.6736e-04 - val_loss: 5.9443e-04\n",
      "Epoch 442/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.7877e-04 - val_loss: 7.3588e-04\n",
      "Epoch 443/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.0842e-04 - val_loss: 6.2517e-04\n",
      "Epoch 444/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.4837e-04 - val_loss: 6.1650e-04\n",
      "Epoch 445/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.5051e-04 - val_loss: 7.5024e-04\n",
      "Epoch 446/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5956e-04 - val_loss: 3.1594e-04\n",
      "Epoch 447/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.3243e-04 - val_loss: 4.3887e-04\n",
      "Epoch 448/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7135e-04 - val_loss: 4.0516e-04\n",
      "Epoch 449/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.3590e-04 - val_loss: 3.5762e-04\n",
      "Epoch 450/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4328e-04 - val_loss: 6.0563e-04\n",
      "Epoch 451/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.3607e-04 - val_loss: 7.8259e-04\n",
      "Epoch 452/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7927e-04 - val_loss: 2.7130e-04\n",
      "Epoch 453/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.2614e-04 - val_loss: 0.0012\n",
      "Epoch 454/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.0334e-04 - val_loss: 8.2343e-04\n",
      "Epoch 455/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 7.1334e-04 - val_loss: 7.9157e-04\n",
      "Epoch 456/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.3196e-04 - val_loss: 6.7668e-04\n",
      "Epoch 457/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9281e-04 - val_loss: 5.2765e-04\n",
      "Epoch 458/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9069e-04 - val_loss: 2.8490e-04\n",
      "Epoch 459/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.2117e-04 - val_loss: 8.8842e-04\n",
      "Epoch 460/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.0100e-04 - val_loss: 4.0980e-04\n",
      "Epoch 461/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.0630e-04 - val_loss: 8.2509e-04\n",
      "Epoch 462/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9501e-04 - val_loss: 4.3969e-04\n",
      "Epoch 463/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5932e-04 - val_loss: 5.5172e-04\n",
      "Epoch 464/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.8055e-04 - val_loss: 7.8257e-04\n",
      "Epoch 465/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5418e-04 - val_loss: 3.6448e-04\n",
      "Epoch 466/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6360e-04 - val_loss: 5.2912e-04\n",
      "Epoch 467/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4969e-04 - val_loss: 3.8802e-04\n",
      "Epoch 468/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.3926e-04 - val_loss: 5.3175e-04\n",
      "Epoch 469/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4285e-04 - val_loss: 4.2401e-04\n",
      "Epoch 470/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7263e-04 - val_loss: 3.9779e-04\n",
      "Epoch 471/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.3126e-04 - val_loss: 0.0010\n",
      "Epoch 472/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9550e-04 - val_loss: 3.5550e-04\n",
      "Epoch 473/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4388e-04 - val_loss: 7.5054e-04\n",
      "Epoch 474/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.8092e-04 - val_loss: 6.0156e-04\n",
      "Epoch 475/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.0710e-04 - val_loss: 5.2443e-04\n",
      "Epoch 476/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4404e-04 - val_loss: 6.0722e-04\n",
      "Epoch 477/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.3029e-04 - val_loss: 7.8687e-04\n",
      "Epoch 478/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7627e-04 - val_loss: 9.0629e-04\n",
      "Epoch 479/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5303e-04 - val_loss: 4.1838e-04\n",
      "Epoch 480/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4689e-04 - val_loss: 5.8764e-04\n",
      "Epoch 481/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4558e-04 - val_loss: 4.8545e-04\n",
      "Epoch 482/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.3323e-04 - val_loss: 4.1511e-04\n",
      "Epoch 483/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.2096e-04 - val_loss: 6.7743e-04\n",
      "Epoch 484/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4764e-04 - val_loss: 3.8957e-04\n",
      "Epoch 485/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.3712e-04 - val_loss: 6.6957e-04\n",
      "Epoch 486/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.2109e-04 - val_loss: 4.2709e-04\n",
      "Epoch 487/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5895e-04 - val_loss: 4.2859e-04\n",
      "Epoch 488/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.3052e-04 - val_loss: 3.6122e-04\n",
      "Epoch 489/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.3645e-04 - val_loss: 3.5678e-04\n",
      "Epoch 490/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7527e-04 - val_loss: 4.8176e-04\n",
      "Epoch 491/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.3975e-04 - val_loss: 6.3369e-04\n",
      "Epoch 492/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5340e-04 - val_loss: 3.0007e-04\n",
      "Epoch 493/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6003e-04 - val_loss: 7.9845e-04\n",
      "Epoch 494/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6991e-04 - val_loss: 5.8547e-04\n",
      "Epoch 495/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.8049e-04 - val_loss: 6.6358e-04\n",
      "Epoch 496/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7544e-04 - val_loss: 4.4056e-04\n",
      "Epoch 497/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7926e-04 - val_loss: 4.0062e-04\n",
      "Epoch 498/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7905e-04 - val_loss: 3.7330e-04\n",
      "Epoch 499/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6059e-04 - val_loss: 7.0453e-04\n",
      "Epoch 500/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.3765e-04 - val_loss: 3.9422e-04\n",
      "Epoch 501/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.2621e-04 - val_loss: 5.6247e-04\n",
      "Epoch 502/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4374e-04 - val_loss: 3.8851e-04\n",
      "Epoch 503/5000\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 4.6628e-04 - val_loss: 7.1089e-04\n",
      "Epoch 504/5000\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 5.4838e-04 - val_loss: 8.2290e-04\n",
      "Epoch 505/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.4430e-04 - val_loss: 4.8854e-04\n",
      "Epoch 506/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5335e-04 - val_loss: 4.4762e-04\n",
      "Epoch 507/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.7377e-04 - val_loss: 4.3063e-04\n",
      "Epoch 508/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.0250e-04 - val_loss: 4.7432e-04\n",
      "Epoch 509/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.6021e-04 - val_loss: 5.0302e-04\n",
      "Epoch 510/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.9056e-04 - val_loss: 4.2400e-04\n",
      "Epoch 511/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.1828e-04 - val_loss: 7.2854e-04\n",
      "Epoch 512/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5226e-04 - val_loss: 5.9758e-04\n",
      "Epoch 513/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.6712e-04 - val_loss: 5.2599e-04\n",
      "Epoch 514/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4337e-04 - val_loss: 8.7138e-04\n",
      "Epoch 515/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.7891e-04 - val_loss: 5.4790e-04\n",
      "Epoch 516/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5474e-04 - val_loss: 3.5837e-04\n",
      "Epoch 517/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.8240e-04 - val_loss: 4.7274e-04\n",
      "Epoch 518/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.2095e-04 - val_loss: 3.8430e-04\n",
      "Epoch 519/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5055e-04 - val_loss: 4.9994e-04\n",
      "Epoch 520/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.3838e-04 - val_loss: 4.9173e-04\n",
      "Epoch 521/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.8902e-04 - val_loss: 9.2669e-04\n",
      "Epoch 522/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.8743e-04 - val_loss: 2.8620e-04\n",
      "Epoch 523/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.6970e-04 - val_loss: 3.8048e-04\n",
      "Epoch 524/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.3234e-04 - val_loss: 4.6102e-04\n",
      "Epoch 525/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4944e-04 - val_loss: 3.2936e-04\n",
      "Epoch 526/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.8227e-04 - val_loss: 4.0869e-04\n",
      "Epoch 527/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.2671e-04 - val_loss: 8.7667e-04\n",
      "Epoch 528/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.8837e-04 - val_loss: 7.4621e-04\n",
      "Epoch 529/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.9489e-04 - val_loss: 7.6048e-04\n",
      "Epoch 530/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 6.3320e-04 - val_loss: 7.6106e-04\n",
      "Epoch 531/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 6.7626e-04 - val_loss: 7.9562e-04\n",
      "Epoch 532/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 6.2404e-04 - val_loss: 8.1589e-04\n",
      "Epoch 533/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 6.1356e-04 - val_loss: 6.8688e-04\n",
      "Epoch 534/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 6.3963e-04 - val_loss: 9.6868e-04\n",
      "Epoch 535/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 7.3317e-04 - val_loss: 6.9826e-04\n",
      "Epoch 536/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 6.8324e-04 - val_loss: 8.5588e-04\n",
      "Epoch 537/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 7.4395e-04 - val_loss: 0.0014\n",
      "Epoch 538/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 6.6472e-04 - val_loss: 3.8377e-04\n",
      "Epoch 539/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8978e-04 - val_loss: 4.0650e-04\n",
      "Epoch 540/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.7857e-04 - val_loss: 3.9242e-04\n",
      "Epoch 541/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.7750e-04 - val_loss: 0.0011\n",
      "Epoch 542/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.2142e-04 - val_loss: 2.6927e-04\n",
      "Epoch 543/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5747e-04 - val_loss: 5.3645e-04\n",
      "Epoch 544/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5921e-04 - val_loss: 6.4453e-04\n",
      "Epoch 545/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.1614e-04 - val_loss: 7.0901e-04\n",
      "Epoch 546/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.5707e-04 - val_loss: 5.9673e-04\n",
      "Epoch 547/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 7.5195e-04 - val_loss: 9.1374e-04\n",
      "Epoch 548/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 6.6630e-04 - val_loss: 8.0551e-04\n",
      "Epoch 549/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 7.0237e-04 - val_loss: 0.0013\n",
      "Epoch 550/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 6.6246e-04 - val_loss: 8.1301e-04\n",
      "Epoch 551/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 6.2978e-04 - val_loss: 7.8957e-04\n",
      "Epoch 552/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.9635e-04 - val_loss: 5.7677e-04\n",
      "Epoch 553/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.3623e-04 - val_loss: 4.7755e-04\n",
      "Epoch 554/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5788e-04 - val_loss: 3.8387e-04\n",
      "Epoch 555/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5528e-04 - val_loss: 3.4273e-04\n",
      "Epoch 556/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4406e-04 - val_loss: 4.0925e-04\n",
      "Epoch 557/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4544e-04 - val_loss: 3.3659e-04\n",
      "Epoch 558/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.3901e-04 - val_loss: 3.1371e-04\n",
      "Epoch 559/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5456e-04 - val_loss: 6.1482e-04\n",
      "Epoch 560/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5446e-04 - val_loss: 6.7618e-04\n",
      "Epoch 561/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5005e-04 - val_loss: 3.7947e-04\n",
      "Epoch 562/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.7249e-04 - val_loss: 3.2100e-04\n",
      "Epoch 563/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.3128e-04 - val_loss: 6.1799e-04\n",
      "Epoch 564/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.1997e-04 - val_loss: 4.3808e-04\n",
      "Epoch 565/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.1794e-04 - val_loss: 6.1867e-04\n",
      "Epoch 566/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4845e-04 - val_loss: 6.8444e-04\n",
      "Epoch 567/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.7147e-04 - val_loss: 3.6759e-04\n",
      "Epoch 568/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.7328e-04 - val_loss: 4.9221e-04\n",
      "Epoch 569/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.6151e-04 - val_loss: 3.0353e-04\n",
      "Epoch 570/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5893e-04 - val_loss: 7.4323e-04\n",
      "Epoch 571/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5243e-04 - val_loss: 4.8942e-04\n",
      "Epoch 572/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.7503e-04 - val_loss: 5.3787e-04\n",
      "Epoch 573/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.6736e-04 - val_loss: 7.5192e-04\n",
      "Epoch 574/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.6885e-04 - val_loss: 9.3257e-04\n",
      "Epoch 575/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.6899e-04 - val_loss: 7.2841e-04\n",
      "Epoch 576/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4406e-04 - val_loss: 5.5881e-04\n",
      "Epoch 577/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5173e-04 - val_loss: 3.3576e-04\n",
      "Epoch 578/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.7054e-04 - val_loss: 5.6446e-04\n",
      "Epoch 579/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.4702e-04 - val_loss: 5.9155e-04\n",
      "Epoch 580/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 6.2408e-04 - val_loss: 3.5766e-04\n",
      "Epoch 581/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.5211e-04 - val_loss: 6.7104e-04\n",
      "Epoch 582/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.2328e-04 - val_loss: 7.9537e-04\n",
      "Epoch 583/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.8399e-04 - val_loss: 4.3223e-04\n",
      "Epoch 584/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.3070e-04 - val_loss: 3.4116e-04\n",
      "Epoch 585/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5598e-04 - val_loss: 4.6802e-04\n",
      "Epoch 586/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4317e-04 - val_loss: 3.8103e-04\n",
      "Epoch 587/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.3591e-04 - val_loss: 8.2453e-04\n",
      "Epoch 588/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.7962e-04 - val_loss: 5.9743e-04\n",
      "Epoch 589/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.6397e-04 - val_loss: 4.9568e-04\n",
      "Epoch 590/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5520e-04 - val_loss: 3.9786e-04\n",
      "Epoch 591/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.7688e-04 - val_loss: 7.9051e-04\n",
      "Epoch 592/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.7524e-04 - val_loss: 3.6768e-04\n",
      "Epoch 593/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.2040e-04 - val_loss: 3.7559e-04\n",
      "Epoch 594/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5089e-04 - val_loss: 2.7362e-04\n",
      "Epoch 595/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.2609e-04 - val_loss: 5.4882e-04\n",
      "Epoch 596/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.6414e-04 - val_loss: 3.1886e-04\n",
      "Epoch 597/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.8309e-04 - val_loss: 3.0909e-04\n",
      "Epoch 598/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.8501e-04 - val_loss: 6.2788e-04\n",
      "Epoch 599/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.4926e-04 - val_loss: 6.6356e-04\n",
      "Epoch 600/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.8132e-04 - val_loss: 8.2429e-04\n",
      "Epoch 601/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.7413e-04 - val_loss: 3.3169e-04\n",
      "Epoch 602/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5869e-04 - val_loss: 6.9308e-04\n",
      "Epoch 603/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.6428e-04 - val_loss: 7.7051e-04\n",
      "Epoch 604/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4013e-04 - val_loss: 2.8111e-04\n",
      "Epoch 605/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5782e-04 - val_loss: 6.8523e-04\n",
      "Epoch 606/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.2546e-04 - val_loss: 5.8216e-04\n",
      "Epoch 607/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5364e-04 - val_loss: 0.0010\n",
      "Epoch 608/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.4227e-04 - val_loss: 2.4042e-04\n",
      "Epoch 609/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.2838e-04 - val_loss: 5.1497e-04\n",
      "Epoch 610/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.2939e-04 - val_loss: 3.3072e-04\n",
      "Epoch 611/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.6825e-04 - val_loss: 4.4363e-04\n",
      "Epoch 612/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.3660e-04 - val_loss: 5.5633e-04\n",
      "Epoch 613/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.6056e-04 - val_loss: 4.5610e-04\n",
      "Epoch 614/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4093e-04 - val_loss: 3.2061e-04\n",
      "Epoch 615/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5330e-04 - val_loss: 4.6105e-04\n",
      "Epoch 616/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4819e-04 - val_loss: 4.5797e-04\n",
      "Epoch 617/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.8893e-04 - val_loss: 5.4403e-04\n",
      "Epoch 618/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.1776e-04 - val_loss: 4.7274e-04\n",
      "Epoch 619/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.7994e-04 - val_loss: 4.6205e-04\n",
      "Epoch 620/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.9283e-04 - val_loss: 9.3658e-04\n",
      "Epoch 621/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.5523e-04 - val_loss: 6.2968e-04\n",
      "Epoch 622/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.9070e-04 - val_loss: 3.2309e-04\n",
      "Epoch 623/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5289e-04 - val_loss: 3.9908e-04\n",
      "Epoch 624/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4125e-04 - val_loss: 3.7008e-04\n",
      "Epoch 625/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5354e-04 - val_loss: 6.9246e-04\n",
      "Epoch 626/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.7320e-04 - val_loss: 3.8540e-04\n",
      "Epoch 627/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.3182e-04 - val_loss: 4.3132e-04\n",
      "Epoch 628/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.7048e-04 - val_loss: 3.4954e-04\n",
      "Epoch 629/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.3904e-04 - val_loss: 3.7898e-04\n",
      "Epoch 630/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5727e-04 - val_loss: 2.9078e-04\n",
      "Epoch 631/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5633e-04 - val_loss: 7.6976e-04\n",
      "Epoch 632/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.1203e-04 - val_loss: 3.5260e-04\n",
      "Epoch 633/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4740e-04 - val_loss: 3.1621e-04\n",
      "Epoch 634/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.6232e-04 - val_loss: 4.9555e-04\n",
      "Epoch 635/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4888e-04 - val_loss: 3.8141e-04\n",
      "Epoch 636/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4078e-04 - val_loss: 4.9496e-04\n",
      "Epoch 637/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.3754e-04 - val_loss: 9.3151e-04\n",
      "Epoch 638/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.4917e-04 - val_loss: 3.8374e-04\n",
      "Epoch 639/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.8147e-04 - val_loss: 5.2310e-04\n",
      "Epoch 640/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.2157e-04 - val_loss: 5.7016e-04\n",
      "Epoch 641/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.2487e-04 - val_loss: 8.2377e-04\n",
      "Epoch 642/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.0892e-04 - val_loss: 5.4431e-04\n",
      "Epoch 643/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.1147e-04 - val_loss: 3.6738e-04\n",
      "Epoch 644/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5136e-04 - val_loss: 5.2103e-04\n",
      "Epoch 645/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.3915e-04 - val_loss: 2.5180e-04\n",
      "Epoch 646/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5170e-04 - val_loss: 3.3809e-04\n",
      "Epoch 647/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.3080e-04 - val_loss: 3.7573e-04\n",
      "Epoch 648/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5013e-04 - val_loss: 7.5242e-04\n",
      "Epoch 649/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.6340e-04 - val_loss: 2.5801e-04\n",
      "Epoch 650/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5677e-04 - val_loss: 5.5742e-04\n",
      "Epoch 651/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4706e-04 - val_loss: 3.0703e-04\n",
      "Epoch 652/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.1355e-04 - val_loss: 7.4226e-04\n",
      "Epoch 653/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5227e-04 - val_loss: 3.1111e-04\n",
      "Epoch 654/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.3155e-04 - val_loss: 4.1550e-04\n",
      "Epoch 655/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4275e-04 - val_loss: 4.9087e-04\n",
      "Epoch 656/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.7085e-04 - val_loss: 5.2913e-04\n",
      "Epoch 657/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.3952e-04 - val_loss: 6.3926e-04\n",
      "Epoch 658/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.2910e-04 - val_loss: 7.0598e-04\n",
      "Epoch 659/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5304e-04 - val_loss: 5.2318e-04\n",
      "Epoch 660/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.3055e-04 - val_loss: 3.7265e-04\n",
      "Epoch 661/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5458e-04 - val_loss: 6.4541e-04\n",
      "Epoch 662/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.8341e-04 - val_loss: 7.3715e-04\n",
      "Epoch 663/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.7846e-04 - val_loss: 5.8564e-04\n",
      "Epoch 664/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4283e-04 - val_loss: 6.4489e-04\n",
      "Epoch 665/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.8155e-04 - val_loss: 7.6129e-04\n",
      "Epoch 666/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4609e-04 - val_loss: 3.5916e-04\n",
      "Epoch 667/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.8586e-04 - val_loss: 6.8839e-04\n",
      "Epoch 668/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 6.6098e-04 - val_loss: 7.8797e-04\n",
      "Epoch 669/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.7191e-04 - val_loss: 4.3113e-04\n",
      "Epoch 670/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4134e-04 - val_loss: 3.3599e-04\n",
      "Epoch 671/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5461e-04 - val_loss: 6.4074e-04\n",
      "Epoch 672/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.3334e-04 - val_loss: 8.8325e-04\n",
      "Epoch 673/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 6.1121e-04 - val_loss: 4.6236e-04\n",
      "Epoch 674/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.2991e-04 - val_loss: 4.9390e-04\n",
      "Epoch 675/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.3862e-04 - val_loss: 5.5181e-04\n",
      "Epoch 676/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.6228e-04 - val_loss: 4.6972e-04\n",
      "Epoch 677/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.9623e-04 - val_loss: 6.9550e-04\n",
      "Epoch 678/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.6918e-04 - val_loss: 5.9014e-04\n",
      "Epoch 679/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.7145e-04 - val_loss: 7.2005e-04\n",
      "Epoch 680/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4731e-04 - val_loss: 4.3641e-04\n",
      "Epoch 681/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.8101e-04 - val_loss: 6.5586e-04\n",
      "Epoch 682/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.3702e-04 - val_loss: 9.0667e-04\n",
      "Epoch 683/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.0718e-04 - val_loss: 5.2422e-04\n",
      "Epoch 684/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.2210e-04 - val_loss: 3.5309e-04\n",
      "Epoch 685/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.4555e-04 - val_loss: 8.8066e-04\n",
      "Epoch 686/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.0474e-04 - val_loss: 7.7157e-04\n",
      "Epoch 687/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.9365e-04 - val_loss: 8.0286e-04\n",
      "Epoch 688/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.9001e-04 - val_loss: 6.0451e-04\n",
      "Epoch 689/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.7420e-04 - val_loss: 7.5634e-04\n",
      "Epoch 690/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5145e-04 - val_loss: 5.3849e-04\n",
      "Epoch 691/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5208e-04 - val_loss: 2.6584e-04\n",
      "Epoch 692/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4749e-04 - val_loss: 3.7999e-04\n",
      "Epoch 693/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4673e-04 - val_loss: 5.1880e-04\n",
      "Epoch 694/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4681e-04 - val_loss: 3.6019e-04\n",
      "Epoch 695/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.3653e-04 - val_loss: 4.5628e-04\n",
      "Epoch 696/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.1923e-04 - val_loss: 0.0012\n",
      "Epoch 697/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4883e-04 - val_loss: 4.4263e-04\n",
      "Epoch 698/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.9789e-04 - val_loss: 6.6943e-04\n",
      "Epoch 699/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4050e-04 - val_loss: 6.2867e-04\n",
      "Epoch 700/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6693e-04 - val_loss: 2.7374e-04\n",
      "Epoch 701/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.6492e-04 - val_loss: 9.8261e-04\n",
      "Epoch 702/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8426e-04 - val_loss: 5.6368e-04\n",
      "Epoch 703/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.6372e-04 - val_loss: 5.8568e-04\n",
      "Epoch 704/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.6953e-04 - val_loss: 6.0282e-04\n",
      "Epoch 705/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.7972e-04 - val_loss: 4.0268e-04\n",
      "Epoch 706/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.8999e-04 - val_loss: 4.9416e-04\n",
      "Epoch 707/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.8888e-04 - val_loss: 4.6678e-04\n",
      "Epoch 708/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.6882e-04 - val_loss: 4.1315e-04\n",
      "Epoch 709/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.6709e-04 - val_loss: 6.3757e-04\n",
      "Epoch 710/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.7098e-04 - val_loss: 4.4911e-04\n",
      "Epoch 711/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.7255e-04 - val_loss: 4.4936e-04\n",
      "Epoch 712/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.6716e-04 - val_loss: 8.0047e-04\n",
      "Epoch 713/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.8311e-04 - val_loss: 5.3499e-04\n",
      "Epoch 714/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.6999e-04 - val_loss: 6.2384e-04\n",
      "Epoch 715/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.7667e-04 - val_loss: 3.2894e-04\n",
      "Epoch 716/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5028e-04 - val_loss: 4.4712e-04\n",
      "Epoch 717/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.6649e-04 - val_loss: 5.1754e-04\n",
      "Epoch 718/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.9360e-04 - val_loss: 5.0513e-04\n",
      "Epoch 719/5000\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 4.3913e-04 - val_loss: 6.5843e-04\n",
      "Epoch 720/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.7696e-04 - val_loss: 7.9887e-04\n",
      "Epoch 721/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.8521e-04 - val_loss: 7.2265e-04\n",
      "Epoch 722/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.0884e-04 - val_loss: 4.7540e-04\n",
      "Epoch 723/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.6546e-04 - val_loss: 7.4288e-04\n",
      "Epoch 724/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4871e-04 - val_loss: 7.2291e-04\n",
      "Epoch 725/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.2918e-04 - val_loss: 3.6967e-04\n",
      "Epoch 726/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.8292e-04 - val_loss: 6.6564e-04\n",
      "Epoch 727/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.7309e-04 - val_loss: 2.6864e-04\n",
      "Epoch 728/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4609e-04 - val_loss: 3.7753e-04\n",
      "Epoch 729/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5588e-04 - val_loss: 3.8800e-04\n",
      "Epoch 730/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.1460e-04 - val_loss: 0.0012\n",
      "Epoch 731/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4852e-04 - val_loss: 2.7047e-04\n",
      "Epoch 732/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.7484e-04 - val_loss: 8.6150e-04\n",
      "Epoch 733/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5797e-04 - val_loss: 7.8729e-04\n",
      "Epoch 734/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.1156e-04 - val_loss: 3.4558e-04\n",
      "Epoch 735/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.9834e-04 - val_loss: 7.9237e-04\n",
      "Epoch 736/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.5632e-04 - val_loss: 4.9626e-04\n",
      "Epoch 737/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.8726e-04 - val_loss: 6.4496e-04\n",
      "Epoch 738/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.8238e-04 - val_loss: 7.7425e-04\n",
      "Epoch 739/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.2698e-04 - val_loss: 3.7456e-04\n",
      "Epoch 740/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5422e-04 - val_loss: 6.2419e-04\n",
      "Epoch 741/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5914e-04 - val_loss: 3.1664e-04\n",
      "Epoch 742/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.6255e-04 - val_loss: 6.5642e-04\n",
      "Epoch 743/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4236e-04 - val_loss: 5.0190e-04\n",
      "Epoch 744/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.1379e-04 - val_loss: 7.9093e-04\n",
      "Epoch 745/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.0827e-04 - val_loss: 6.7585e-04\n",
      "Epoch 746/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4805e-04 - val_loss: 5.5604e-04\n",
      "Epoch 747/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.3931e-04 - val_loss: 2.5628e-04\n",
      "Epoch 748/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.9276e-04 - val_loss: 6.4089e-04\n",
      "Epoch 749/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.3971e-04 - val_loss: 3.4051e-04\n",
      "Epoch 750/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.6169e-04 - val_loss: 4.5680e-04\n",
      "Epoch 751/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.7549e-04 - val_loss: 5.1644e-04\n",
      "Epoch 752/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5112e-04 - val_loss: 4.9161e-04\n",
      "Epoch 753/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.6610e-04 - val_loss: 3.6700e-04\n",
      "Epoch 754/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.9208e-04 - val_loss: 5.3294e-04\n",
      "Epoch 755/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.1709e-04 - val_loss: 4.1685e-04\n",
      "Epoch 756/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.6707e-04 - val_loss: 4.0347e-04\n",
      "Epoch 757/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.8218e-04 - val_loss: 5.8231e-04\n",
      "Epoch 758/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.6286e-04 - val_loss: 3.5460e-04\n",
      "Epoch 759/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 5.3192e-04 - val_loss: 4.6715e-04\n",
      "Epoch 760/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.8306e-04 - val_loss: 4.2933e-04\n",
      "Epoch 761/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.9158e-04 - val_loss: 6.5001e-04\n",
      "Epoch 762/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.5452e-04 - val_loss: 3.2955e-04\n",
      "Epoch 763/5000\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 4.6379e-04 - val_loss: 4.4978e-04\n",
      "Epoch 764/5000\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 4.7028e-04 - val_loss: 6.6096e-04\n",
      "Epoch 765/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.7373e-04 - val_loss: 6.4379e-04\n",
      "Epoch 766/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.4579e-04 - val_loss: 8.2615e-04\n",
      "Epoch 767/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.1927e-04 - val_loss: 8.3895e-04\n",
      "Epoch 768/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 6.6606e-04 - val_loss: 6.7059e-04\n",
      "Epoch 769/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 7.2225e-04 - val_loss: 8.6774e-04\n",
      "Epoch 770/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 7.1501e-04 - val_loss: 9.2854e-04\n",
      "Epoch 771/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 6.6606e-04 - val_loss: 6.0502e-04\n",
      "Epoch 772/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 6.9840e-04 - val_loss: 9.7533e-04\n",
      "Epoch 773/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.1773e-04 - val_loss: 6.2035e-04\n",
      "Epoch 774/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.0400e-04 - val_loss: 6.5407e-04\n",
      "Epoch 775/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.4530e-04 - val_loss: 2.5733e-04\n",
      "Epoch 776/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5420e-04 - val_loss: 7.2355e-04\n",
      "Epoch 777/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.3997e-04 - val_loss: 3.5755e-04\n",
      "Epoch 778/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6594e-04 - val_loss: 3.0796e-04\n",
      "Epoch 779/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.4663e-04 - val_loss: 7.0544e-04\n",
      "Epoch 780/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.2356e-04 - val_loss: 5.4513e-04\n",
      "Epoch 781/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4554e-04 - val_loss: 5.1047e-04\n",
      "Epoch 782/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4674e-04 - val_loss: 7.4037e-04\n",
      "Epoch 783/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.6543e-04 - val_loss: 2.9612e-04\n",
      "Epoch 784/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.3247e-04 - val_loss: 5.0439e-04\n",
      "Epoch 785/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.3316e-04 - val_loss: 2.5825e-04\n",
      "Epoch 786/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9193e-04 - val_loss: 3.5660e-04\n",
      "Epoch 787/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.1589e-04 - val_loss: 5.7841e-04\n",
      "Epoch 788/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9538e-04 - val_loss: 3.2838e-04\n",
      "Epoch 789/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.9105e-04 - val_loss: 5.0446e-04\n",
      "Epoch 790/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.8184e-04 - val_loss: 4.8454e-04\n",
      "Epoch 791/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.0093e-04 - val_loss: 3.7963e-04\n",
      "Epoch 792/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.8053e-04 - val_loss: 7.8762e-04\n",
      "Epoch 793/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.7659e-04 - val_loss: 4.1527e-04\n",
      "Epoch 794/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.4391e-04 - val_loss: 6.6445e-04\n",
      "Epoch 795/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.2811e-04 - val_loss: 3.3896e-04\n",
      "Epoch 796/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8701e-04 - val_loss: 8.3073e-04\n",
      "Epoch 797/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.7016e-04 - val_loss: 2.9865e-04\n",
      "Epoch 798/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.2112e-04 - val_loss: 6.8886e-04\n",
      "Epoch 799/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8805e-04 - val_loss: 8.2083e-04\n",
      "Epoch 800/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.3614e-04 - val_loss: 3.7377e-04\n",
      "Epoch 801/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.3848e-04 - val_loss: 4.5615e-04\n",
      "Epoch 802/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.7367e-04 - val_loss: 3.5245e-04\n",
      "Epoch 803/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.3523e-04 - val_loss: 3.1215e-04\n",
      "Epoch 804/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5378e-04 - val_loss: 8.4486e-04\n",
      "Epoch 805/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6174e-04 - val_loss: 5.5327e-04\n",
      "Epoch 806/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6226e-04 - val_loss: 5.3047e-04\n",
      "Epoch 807/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5753e-04 - val_loss: 5.4502e-04\n",
      "Epoch 808/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5638e-04 - val_loss: 7.0106e-04\n",
      "Epoch 809/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8153e-04 - val_loss: 3.6437e-04\n",
      "Epoch 810/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8456e-04 - val_loss: 6.3359e-04\n",
      "Epoch 811/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.7300e-04 - val_loss: 8.0196e-04\n",
      "Epoch 812/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.9260e-04 - val_loss: 5.5121e-04\n",
      "Epoch 813/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.3566e-04 - val_loss: 4.3216e-04\n",
      "Epoch 814/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.9138e-04 - val_loss: 6.9155e-04\n",
      "Epoch 815/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.0561e-04 - val_loss: 7.6175e-04\n",
      "Epoch 816/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5757e-04 - val_loss: 5.0383e-04\n",
      "Epoch 817/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.7505e-04 - val_loss: 5.6558e-04\n",
      "Epoch 818/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6225e-04 - val_loss: 4.0363e-04\n",
      "Epoch 819/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.3356e-04 - val_loss: 3.8763e-04\n",
      "Epoch 820/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6172e-04 - val_loss: 6.5199e-04\n",
      "Epoch 821/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.3722e-04 - val_loss: 5.5921e-04\n",
      "Epoch 822/5000\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 4.4342e-04 - val_loss: 3.3909e-04\n",
      "Epoch 823/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6470e-04 - val_loss: 5.4445e-04\n",
      "Epoch 824/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.1436e-04 - val_loss: 6.2415e-04\n",
      "Epoch 825/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.0879e-04 - val_loss: 8.0261e-04\n",
      "Epoch 826/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.7326e-04 - val_loss: 8.5071e-04\n",
      "Epoch 827/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.8512e-04 - val_loss: 6.6457e-04\n",
      "Epoch 828/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8534e-04 - val_loss: 4.1190e-04\n",
      "Epoch 829/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.8256e-04 - val_loss: 7.8002e-04\n",
      "Epoch 830/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.8155e-04 - val_loss: 6.9717e-04\n",
      "Epoch 831/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6586e-04 - val_loss: 4.3296e-04\n",
      "Epoch 832/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6851e-04 - val_loss: 6.3334e-04\n",
      "Epoch 833/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.3935e-04 - val_loss: 7.8883e-04\n",
      "Epoch 834/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8523e-04 - val_loss: 5.2906e-04\n",
      "Epoch 835/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6510e-04 - val_loss: 3.9897e-04\n",
      "Epoch 836/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8471e-04 - val_loss: 3.9458e-04\n",
      "Epoch 837/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.8253e-04 - val_loss: 7.6874e-04\n",
      "Epoch 838/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.3375e-04 - val_loss: 3.2966e-04\n",
      "Epoch 839/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6436e-04 - val_loss: 6.5430e-04\n",
      "Epoch 840/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.4577e-04 - val_loss: 5.0937e-04\n",
      "Epoch 841/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.9676e-04 - val_loss: 5.9482e-04\n",
      "Epoch 842/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5422e-04 - val_loss: 4.1115e-04\n",
      "Epoch 843/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4976e-04 - val_loss: 6.3426e-04\n",
      "Epoch 844/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6130e-04 - val_loss: 4.0050e-04\n",
      "Epoch 845/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.4079e-04 - val_loss: 4.2012e-04\n",
      "Epoch 846/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.7924e-04 - val_loss: 5.0628e-04\n",
      "Epoch 847/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5575e-04 - val_loss: 6.7706e-04\n",
      "Epoch 848/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6175e-04 - val_loss: 3.5309e-04\n",
      "Epoch 849/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.2425e-04 - val_loss: 4.5574e-04\n",
      "Epoch 850/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8084e-04 - val_loss: 6.3486e-04\n",
      "Epoch 851/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8402e-04 - val_loss: 7.1330e-04\n",
      "Epoch 852/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8783e-04 - val_loss: 4.4119e-04\n",
      "Epoch 853/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6666e-04 - val_loss: 4.1953e-04\n",
      "Epoch 854/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5078e-04 - val_loss: 6.7894e-04\n",
      "Epoch 855/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.4000e-04 - val_loss: 4.6468e-04\n",
      "Epoch 856/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.4254e-04 - val_loss: 4.3103e-04\n",
      "Epoch 857/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6551e-04 - val_loss: 4.5085e-04\n",
      "Epoch 858/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.9367e-04 - val_loss: 3.8246e-04\n",
      "Epoch 859/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.3118e-04 - val_loss: 2.4959e-04\n",
      "Epoch 860/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6877e-04 - val_loss: 5.4909e-04\n",
      "Epoch 861/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.7828e-04 - val_loss: 7.9546e-04\n",
      "Epoch 862/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.0987e-04 - val_loss: 4.1736e-04\n",
      "Epoch 863/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6363e-04 - val_loss: 3.0241e-04\n",
      "Epoch 864/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.7487e-04 - val_loss: 5.2190e-04\n",
      "Epoch 865/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.3926e-04 - val_loss: 6.6387e-04\n",
      "Epoch 866/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8496e-04 - val_loss: 3.2446e-04\n",
      "Epoch 867/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8413e-04 - val_loss: 5.0014e-04\n",
      "Epoch 868/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.4307e-04 - val_loss: 3.7291e-04\n",
      "Epoch 869/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.3247e-04 - val_loss: 8.5224e-04\n",
      "Epoch 870/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.6380e-04 - val_loss: 5.0977e-04\n",
      "Epoch 871/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6884e-04 - val_loss: 4.2511e-04\n",
      "Epoch 872/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.2817e-04 - val_loss: 8.4569e-04\n",
      "Epoch 873/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8066e-04 - val_loss: 4.0086e-04\n",
      "Epoch 874/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5763e-04 - val_loss: 2.9602e-04\n",
      "Epoch 875/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.4284e-04 - val_loss: 3.8083e-04\n",
      "Epoch 876/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6418e-04 - val_loss: 4.3664e-04\n",
      "Epoch 877/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.3877e-04 - val_loss: 3.0261e-04\n",
      "Epoch 878/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.3354e-04 - val_loss: 4.2998e-04\n",
      "Epoch 879/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.4325e-04 - val_loss: 4.3916e-04\n",
      "Epoch 880/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5375e-04 - val_loss: 3.2742e-04\n",
      "Epoch 881/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.2448e-04 - val_loss: 2.9789e-04\n",
      "Epoch 882/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.7376e-04 - val_loss: 3.0480e-04\n",
      "Epoch 883/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4433e-04 - val_loss: 4.2944e-04\n",
      "Epoch 884/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.4622e-04 - val_loss: 5.4415e-04\n",
      "Epoch 885/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5402e-04 - val_loss: 3.0583e-04\n",
      "Epoch 886/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.7550e-04 - val_loss: 5.2026e-04\n",
      "Epoch 887/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.4635e-04 - val_loss: 6.3229e-04\n",
      "Epoch 888/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.7557e-04 - val_loss: 3.6081e-04\n",
      "Epoch 889/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.4926e-04 - val_loss: 4.0509e-04\n",
      "Epoch 890/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.9212e-04 - val_loss: 4.6314e-04\n",
      "Epoch 891/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5745e-04 - val_loss: 2.8250e-04\n",
      "Epoch 892/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8200e-04 - val_loss: 5.5204e-04\n",
      "Epoch 893/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.4392e-04 - val_loss: 6.9900e-04\n",
      "Epoch 894/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.0172e-04 - val_loss: 4.3505e-04\n",
      "Epoch 895/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.3226e-04 - val_loss: 2.7095e-04\n",
      "Epoch 896/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5392e-04 - val_loss: 5.9933e-04\n",
      "Epoch 897/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.6968e-04 - val_loss: 7.9379e-04\n",
      "Epoch 898/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 6.2930e-04 - val_loss: 4.9370e-04\n",
      "Epoch 899/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.3914e-04 - val_loss: 2.8239e-04\n",
      "Epoch 900/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5374e-04 - val_loss: 7.7718e-04\n",
      "Epoch 901/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6923e-04 - val_loss: 7.7662e-04\n",
      "Epoch 902/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.1959e-04 - val_loss: 4.7078e-04\n",
      "Epoch 903/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 6.1279e-04 - val_loss: 8.4103e-04\n",
      "Epoch 904/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9739e-04 - val_loss: 4.0251e-04\n",
      "Epoch 905/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 6.5105e-04 - val_loss: 8.0852e-04\n",
      "Epoch 906/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.3577e-04 - val_loss: 3.3336e-04\n",
      "Epoch 907/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.4497e-04 - val_loss: 3.6127e-04\n",
      "Epoch 908/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.9373e-04 - val_loss: 5.1541e-04\n",
      "Epoch 909/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.7712e-04 - val_loss: 5.7922e-04\n",
      "Epoch 910/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9272e-04 - val_loss: 2.5314e-04\n",
      "Epoch 911/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5902e-04 - val_loss: 9.5213e-04\n",
      "Epoch 912/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9598e-04 - val_loss: 7.2320e-04\n",
      "Epoch 913/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.2104e-04 - val_loss: 6.6357e-04\n",
      "Epoch 914/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.0043e-04 - val_loss: 4.5554e-04\n",
      "Epoch 915/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.1114e-04 - val_loss: 7.6018e-04\n",
      "Epoch 916/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 6.3000e-04 - val_loss: 8.1188e-04\n",
      "Epoch 917/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 6.0593e-04 - val_loss: 7.9301e-04\n",
      "Epoch 918/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.1516e-04 - val_loss: 3.8480e-04\n",
      "Epoch 919/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.0671e-04 - val_loss: 4.2865e-04\n",
      "Epoch 920/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.2282e-04 - val_loss: 4.5810e-04\n",
      "Epoch 921/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 6.0714e-04 - val_loss: 3.7818e-04\n",
      "Epoch 922/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.0189e-04 - val_loss: 6.8854e-04\n",
      "Epoch 923/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.0295e-04 - val_loss: 3.2798e-04\n",
      "Epoch 924/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.5309e-04 - val_loss: 5.7877e-04\n",
      "Epoch 925/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 6.4026e-04 - val_loss: 5.3537e-04\n",
      "Epoch 926/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 6.4202e-04 - val_loss: 7.8410e-04\n",
      "Epoch 927/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 6.6767e-04 - val_loss: 7.9615e-04\n",
      "Epoch 928/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.3381e-04 - val_loss: 6.1630e-04\n",
      "Epoch 929/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 6.5288e-04 - val_loss: 8.3124e-04\n",
      "Epoch 930/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 6.7472e-04 - val_loss: 8.4376e-04\n",
      "Epoch 931/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 6.2349e-04 - val_loss: 3.6586e-04\n",
      "Epoch 932/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.6738e-04 - val_loss: 6.0600e-04\n",
      "Epoch 933/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.2678e-04 - val_loss: 5.7583e-04\n",
      "Epoch 934/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.0350e-04 - val_loss: 3.7890e-04\n",
      "Epoch 935/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.3729e-04 - val_loss: 8.0321e-04\n",
      "Epoch 936/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8493e-04 - val_loss: 6.7676e-04\n",
      "Epoch 937/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8289e-04 - val_loss: 5.2139e-04\n",
      "Epoch 938/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6161e-04 - val_loss: 4.9588e-04\n",
      "Epoch 939/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.7760e-04 - val_loss: 4.1037e-04\n",
      "Epoch 940/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.3291e-04 - val_loss: 6.2378e-04\n",
      "Epoch 941/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.2253e-04 - val_loss: 3.8390e-04\n",
      "Epoch 942/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.3381e-04 - val_loss: 5.4073e-04\n",
      "Epoch 943/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8353e-04 - val_loss: 6.2304e-04\n",
      "Epoch 944/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5871e-04 - val_loss: 4.2578e-04\n",
      "Epoch 945/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.7267e-04 - val_loss: 4.4913e-04\n",
      "Epoch 946/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.1638e-04 - val_loss: 8.0376e-04\n",
      "Epoch 947/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.7543e-04 - val_loss: 7.1234e-04\n",
      "Epoch 948/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.5654e-04 - val_loss: 7.9590e-04\n",
      "Epoch 949/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.0372e-04 - val_loss: 5.6679e-04\n",
      "Epoch 950/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.3842e-04 - val_loss: 4.8375e-04\n",
      "Epoch 951/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.2665e-04 - val_loss: 6.0943e-04\n",
      "Epoch 952/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.3668e-04 - val_loss: 3.6525e-04\n",
      "Epoch 953/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5118e-04 - val_loss: 5.6668e-04\n",
      "Epoch 954/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5455e-04 - val_loss: 3.3464e-04\n",
      "Epoch 955/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.0425e-04 - val_loss: 0.0010\n",
      "Epoch 956/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6119e-04 - val_loss: 5.8030e-04\n",
      "Epoch 957/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8407e-04 - val_loss: 3.6811e-04\n",
      "Epoch 958/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.4728e-04 - val_loss: 8.2410e-04\n",
      "Epoch 959/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5019e-04 - val_loss: 5.3225e-04\n",
      "Epoch 960/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6962e-04 - val_loss: 3.0980e-04\n",
      "Epoch 961/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.4648e-04 - val_loss: 3.1382e-04\n",
      "Epoch 962/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8482e-04 - val_loss: 0.0011\n",
      "Epoch 963/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.2061e-04 - val_loss: 3.8217e-04\n",
      "Epoch 964/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 6.1108e-04 - val_loss: 0.0011\n",
      "Epoch 965/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.6239e-04 - val_loss: 7.0623e-04\n",
      "Epoch 966/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8785e-04 - val_loss: 2.9745e-04\n",
      "Epoch 967/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6224e-04 - val_loss: 2.9647e-04\n",
      "Epoch 968/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.9546e-04 - val_loss: 3.7373e-04\n",
      "Epoch 969/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.3761e-04 - val_loss: 8.6465e-04\n",
      "Epoch 970/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.5831e-04 - val_loss: 8.0457e-04\n",
      "Epoch 971/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9693e-04 - val_loss: 3.5149e-04\n",
      "Epoch 972/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6956e-04 - val_loss: 3.3920e-04\n",
      "Epoch 973/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.5773e-04 - val_loss: 5.7759e-04\n",
      "Epoch 974/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9811e-04 - val_loss: 6.0456e-04\n",
      "Epoch 975/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7444e-04 - val_loss: 5.9517e-04\n",
      "Epoch 976/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6917e-04 - val_loss: 4.5257e-04\n",
      "Epoch 977/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9702e-04 - val_loss: 5.9269e-04\n",
      "Epoch 978/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.3890e-04 - val_loss: 8.9695e-04\n",
      "Epoch 979/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7906e-04 - val_loss: 8.5604e-04\n",
      "Epoch 980/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5857e-04 - val_loss: 4.8985e-04\n",
      "Epoch 981/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6468e-04 - val_loss: 3.5746e-04\n",
      "Epoch 982/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.4455e-04 - val_loss: 4.0085e-04\n",
      "Epoch 983/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.0898e-04 - val_loss: 2.6511e-04\n",
      "Epoch 984/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.2273e-04 - val_loss: 4.9135e-04\n",
      "Epoch 985/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5309e-04 - val_loss: 7.5758e-04\n",
      "Epoch 986/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6892e-04 - val_loss: 5.8705e-04\n",
      "Epoch 987/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.5119e-04 - val_loss: 3.0278e-04\n",
      "Epoch 988/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5212e-04 - val_loss: 3.1873e-04\n",
      "Epoch 989/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6324e-04 - val_loss: 4.4444e-04\n",
      "Epoch 990/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6378e-04 - val_loss: 3.1994e-04\n",
      "Epoch 991/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.4245e-04 - val_loss: 3.2038e-04\n",
      "Epoch 992/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.4233e-04 - val_loss: 3.9335e-04\n",
      "Epoch 993/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6751e-04 - val_loss: 7.6503e-04\n",
      "Epoch 994/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.0499e-04 - val_loss: 6.8537e-04\n",
      "Epoch 995/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5904e-04 - val_loss: 4.0640e-04\n",
      "Epoch 996/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.4097e-04 - val_loss: 4.7411e-04\n",
      "Epoch 997/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5258e-04 - val_loss: 5.3916e-04\n",
      "Epoch 998/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6871e-04 - val_loss: 4.5098e-04\n",
      "Epoch 999/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.0201e-04 - val_loss: 7.2331e-04\n",
      "Epoch 1000/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5878e-04 - val_loss: 4.3732e-04\n",
      "Epoch 1001/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5332e-04 - val_loss: 3.5438e-04\n",
      "Epoch 1002/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.9565e-04 - val_loss: 3.3407e-04\n",
      "Epoch 1003/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5057e-04 - val_loss: 6.4625e-04\n",
      "Epoch 1004/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.1315e-04 - val_loss: 8.3128e-04\n",
      "Epoch 1005/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.0515e-04 - val_loss: 7.7217e-04\n",
      "Epoch 1006/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.7671e-04 - val_loss: 5.9395e-04\n",
      "Epoch 1007/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5525e-04 - val_loss: 5.5305e-04\n",
      "Epoch 1008/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5012e-04 - val_loss: 2.5158e-04\n",
      "Epoch 1009/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6430e-04 - val_loss: 5.0749e-04\n",
      "Epoch 1010/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6781e-04 - val_loss: 2.6553e-04\n",
      "Epoch 1011/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6543e-04 - val_loss: 4.9323e-04\n",
      "Epoch 1012/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6991e-04 - val_loss: 2.6588e-04\n",
      "Epoch 1013/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.1551e-04 - val_loss: 8.8475e-04\n",
      "Epoch 1014/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.7754e-04 - val_loss: 4.2947e-04\n",
      "Epoch 1015/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.7777e-04 - val_loss: 4.6535e-04\n",
      "Epoch 1016/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6718e-04 - val_loss: 8.0119e-04\n",
      "Epoch 1017/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.3324e-04 - val_loss: 4.0005e-04\n",
      "Epoch 1018/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.1438e-04 - val_loss: 5.7644e-04\n",
      "Epoch 1019/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.6485e-04 - val_loss: 2.8669e-04\n",
      "Epoch 1020/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6261e-04 - val_loss: 4.8820e-04\n",
      "Epoch 1021/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5507e-04 - val_loss: 4.4315e-04\n",
      "Epoch 1022/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.8742e-04 - val_loss: 4.9625e-04\n",
      "Epoch 1023/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.1142e-04 - val_loss: 6.6447e-04\n",
      "Epoch 1024/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.7459e-04 - val_loss: 4.5152e-04\n",
      "Epoch 1025/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.8750e-04 - val_loss: 5.9334e-04\n",
      "Epoch 1026/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.6347e-04 - val_loss: 7.9689e-04\n",
      "Epoch 1027/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4531e-04 - val_loss: 2.9247e-04\n",
      "Epoch 1028/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.2742e-04 - val_loss: 5.0254e-04\n",
      "Epoch 1029/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.3458e-04 - val_loss: 6.5598e-04\n",
      "Epoch 1030/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.3828e-04 - val_loss: 4.2901e-04\n",
      "Epoch 1031/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.2077e-04 - val_loss: 2.5865e-04\n",
      "Epoch 1032/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.4360e-04 - val_loss: 7.5603e-04\n",
      "Epoch 1033/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.4955e-04 - val_loss: 3.3800e-04\n",
      "Epoch 1034/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.4979e-04 - val_loss: 4.3137e-04\n",
      "Epoch 1035/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.2903e-04 - val_loss: 5.1084e-04\n",
      "Epoch 1036/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5810e-04 - val_loss: 4.9629e-04\n",
      "Epoch 1037/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9112e-04 - val_loss: 8.3257e-04\n",
      "Epoch 1038/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.7177e-04 - val_loss: 2.8549e-04\n",
      "Epoch 1039/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.3231e-04 - val_loss: 5.6349e-04\n",
      "Epoch 1040/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5377e-04 - val_loss: 4.7406e-04\n",
      "Epoch 1041/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.7255e-04 - val_loss: 7.7116e-04\n",
      "Epoch 1042/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.0057e-04 - val_loss: 5.9303e-04\n",
      "Epoch 1043/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5185e-04 - val_loss: 5.2974e-04\n",
      "Epoch 1044/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.4497e-04 - val_loss: 7.1014e-04\n",
      "Epoch 1045/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.3633e-04 - val_loss: 8.2902e-04\n",
      "Epoch 1046/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.8453e-04 - val_loss: 4.0592e-04\n",
      "Epoch 1047/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.3005e-04 - val_loss: 2.5956e-04\n",
      "Epoch 1048/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.8930e-04 - val_loss: 8.0452e-04\n",
      "Epoch 1049/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.8137e-04 - val_loss: 5.2084e-04\n",
      "Epoch 1050/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5077e-04 - val_loss: 3.5248e-04\n",
      "Epoch 1051/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5006e-04 - val_loss: 3.3654e-04\n",
      "Epoch 1052/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.3425e-04 - val_loss: 8.6084e-04\n",
      "Epoch 1053/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.5532e-04 - val_loss: 9.0839e-04\n",
      "Epoch 1054/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 5.0177e-04 - val_loss: 5.0390e-04\n",
      "Epoch 1055/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6625e-04 - val_loss: 6.4988e-04\n",
      "Epoch 1056/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.5839e-04 - val_loss: 5.4847e-04\n",
      "Epoch 1057/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.7303e-04 - val_loss: 2.8719e-04\n",
      "Epoch 1058/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.6937e-04 - val_loss: 8.3088e-04\n",
      "Epoch 1059/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7061e-04 - val_loss: 8.7656e-04\n",
      "Epoch 1060/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 5.2479e-04 - val_loss: 4.2742e-04\n",
      "Epoch 1061/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9610e-04 - val_loss: 8.5235e-04\n",
      "Epoch 1062/5000\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 4.9077e-04 - val_loss: 4.6660e-04\n",
      "Epoch 1063/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9110e-04 - val_loss: 5.3566e-04\n",
      "Epoch 1064/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.9236e-04 - val_loss: 7.4174e-04\n",
      "Epoch 1065/5000\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 4.7769e-04 - val_loss: 5.3037e-04\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X, Y, epochs = 5000, batch_size=16, validation_data=(X_test, Y_test), verbose=1, callbacks=early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2ccd0e080>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'], 'r')\n",
    "plt.plot(hist.history['val_loss'], 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-12 21:14:48.586384: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2ccddf310>,\n",
       " <matplotlib.lines.Line2D at 0x2ccddf2e0>,\n",
       " <matplotlib.lines.Line2D at 0x2ccddf2b0>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABDf0lEQVR4nO3deXxU9aH///dMJpnskwUSCGRjX6ICQZHFtYqitdpNrBZwqd9SRUFat9qfWxds7e1Vq1B365Uql1Z7aUvVtCouUBUkyqbsBEgCJCSZhJBlZs7vj8PMZJJJyITAIeT1fDw+j3PmzDmfnDlnMvOez/mcc2yGYRgCAACwiN3qFQAAAL0bYQQAAFiKMAIAACxFGAEAAJYijAAAAEsRRgAAgKUIIwAAwFKEEQAAYCmH1SvQGT6fT6WlpUpKSpLNZrN6dQAAQCcYhqHa2lplZWXJbm+//aNHhJHS0lJlZ2dbvRoAAKALdu/erYEDB7b7fI8II0lJSZLMF5OcnGzx2gAAgM5wu93Kzs4OfI+3p0eEEf+hmeTkZMIIAAA9zNG6WNCBFQAAWIowAgAALEUYAQAAliKMAAAASxFGAACApQgjAADAUoQRAABgKcIIAACwFGEEAABYijACAAAsRRgBAACWIowAAABL9Ygb5QFAr+f1SocPd1waGqTmZsnjaTsMN625WTKM9v9muJub2e2SwyFFR5ulM+NOpxQX136JjQ3/t9BrEEYAoLt5vZLbLdXWdlw6mqe+PjRoNDdb/aqOr9hYs/gDSny8lJQkJSaaQ39p+bij55KTpZgYq18VOokwAgDhGIYZFqqqzHLwYOfHa2qO77q119IQG9u2ZcLhCB1vPbS3c7S+vRYTrzfYqtKyhaWj8YaG0NYb/7jHE6y3ocEs1dXdt51iY6WUFLO4XKHDo01LSZESEmixOUEIIwBOXYZhtjC0Dg2dCRTV1eYX77GIjg7+Sm/5C751Cfd8QkL7gaO9ANHTeDztH3Kqr5fq6oItRS3HWz9uPV5fb9bf0CCVl5ulK6KizHDSMqCkpgaHRxt3OrtjK/UKhBEAJ7/GxshbJ/zjx3p4w+mU0tKCXzKdGefLqHMcjmD46k7+w2Q1NWaprg4O2xtvOa262gxKXq/5Hjp4sGvrERfXfmA5WqBJTOxVrTKEEQAnRlNTaMtEeyVcC8bhw8f2tx2OyMOE/3FcXPe8fpw4UVHB/dgVhmG+58KFFH+rmf/9GW68piZYx+HDUmlp115DZ1tgwgWdqKiuvXaLEEYAdF5nA0W4UOFvOu8qm838kI00TPTCX5k4Rjab2YE2Pl7Kyop8eZ8v2N/oaMGl9XhVldma5/VKlZVm6YqkpMhbY3JyzP8VCxBGgN6iddN1TU3bx+2V7gwULldoaGivtA4aycmnTl8JnNrs9mAfk0j5W1QiDTL+YV2dWY+/D01JSef/9ksvSbNmRb7O3YAwApzsPJ6OO+11JkzU1AQ/pI5VJIGidbhITu5xzcfACdWyVWbAgMiXb24O/QERSaBJT+/e1xIBwgjQnXy+4JkAhw5FfhZAuPHGxu5dx9jY4BkC4UpycttpLZt0XS4CBXCyio6W+vQxSw8ScRh5//339eijj2rNmjUqKyvTG2+8oauuuqrDZVasWKH58+drw4YNysrK0l133aXZs2d3dZ2BrjEMs89DfX0wLLQchpvW0XPhph1rR8uOOJ3hL/gULjx0VLgQFICTTMRh5NChQzrjjDN0ww036Nvf/vZR59+xY4cuu+wy3XzzzXrllVf00Ucf6ZZbblHfvn07tTxOIYZhNiE2NpqhwF/8FzvyXxAp3OPOjHdmvhMpLu7oV43s7HhiIiECwCkr4jAybdo0TZs2rdPz/+EPf1BOTo4ee+wxSdLIkSO1evVq/fa3vyWMdIX/Cz2Se090dh5/OGgdFlo+PpbnTqbLWTsc5kWl4uODw5bj7Q07O09cHJ0tAaCTjnufkVWrVmnq1Kkh0y655BI9//zzam5uVnR09PFehfYtW6amL9aaZxn4fObQ65V8huT1yhZ43PI5n6IMye4zJJ9PPk+zfJ5myeuTQ7bAfB5Pc7AOn0/yhdbdsj7/37F5PLJ5zEstG83NkqdZavbI1jIw+HzWba9uZtjt5q/92FgZsU4pNk46MrTHxwfuVeFxRkuxsYqKS5DtyFUoPTEO+ZwxMgLLxMqIO3JvC+eR+o5crdI4culsW1yc4pJSzbAQHa1DTYckSXHRcbLbzODQ5G1Sszey0GSz2RQfHR94XN9cL8NzWLGOWEXZzb4Vzd5mNXmbIt5GCTEJgfHDzYflM3xyOpxy2M1/XY/Po0ZP5H1KWtbb4GmQ1+dVTFSMoqPM/0evz6sGT+QtSfHR8bIdOYW20dMoj8+j6KhoxUSZrTo+w6fDzZEfygq3jxx2h5wO86JihmGovjnyM33C7aMoe5RiHbGBefzvk0iE20d2m11x0cFrlnSl3nD7KOz7r6Ob34XR3j4K9/6LRHv7KNz7LxLt7aNw779ItLePjstnhGGcVJ8RLeuxwnEPI+Xl5crMzAyZlpmZKY/Ho4qKCvXv37/NMo2NjWps0WnP7XYfn5V79VXFvPbaMVVhP1JaO9YNG+kVEZrsUnOU5LFLzfYjwyOPkxPSlJ6cKTkcOmzz6ovKjVK0QxNyJwfuT7Gy7BOVNR1UY5TUdKQ0OoLjTVFq89yUwRdoxpk3STExqjEaNH3ZTDVFSe/8vw/NkBETo3nv3KVlO98OW6fP7pPUcKQEfWfUd7T0u0sDj6MfMrfG/p/sV9+EvpKkuf+4VQtXL5QOyyydcF7ueXrv+vcCj/Mez1NFfYXW/2i9RmeMliT96oNf6aEVD0W07Uf1HaUNt2wIPD7z2TO18cBGvTvrXZ2fd74k6Zk1z2jOP+dEVG+f+D46cOeBwONpi6dpxa4V+t/v/K++O/q7kqQ3Nr2hq/98dUT1SpLxQPDLasYbM/TnjX/Wk9Oe1K1n3SpJ+qDkA13wxwsirrflPpr/1nwtXL1QD5z3gB48/0FJ0qYDm1SwqCDiesPto1vG36KnLn9KklRRX6GM32ZEXG+4fdT6/Ze4IPLrLoTbR+29/yIRbh+19/6LRLh91N77LxLt7aNw779ItLePwr3/ItGbPyNa7hMrnJCzaWytLjbkT+2tp/stWLBADz0U2c7ukgsu0PNfvSavTfLaJZ9NRx332qTpp12j07LGSFFRKt6/Ti+ue1mD0odo7qQ7zLMMoqJ0+9t3qKa5LqK6b5t0h751+tVSdLQ+3bdWs/5xs/L6DNby64sCN7ua/D/na8PBzSHBwzjK0YAHzrst8EGzff8Gnb2oQH3iU3TgzvcC8/z0pfMj/qBxjB+pGZdfJ0lqOnRAb2068sTkyYF59m5O1o7jlCUBAKcGmxFpe17LhW22o55Nc+6552rs2LF6/PHHA9PeeOMNXX311aqvrw97mCZcy0h2drZqamqUnJzc1dUNiyZYE02wp3YTbEf1cpiGwzR8Rph682fE8TpM43a75XK5jvr9fdxbRiZOnKi//e1vIdPefvttjR8/vt3+Ik6nU84TdIOpY90BDrtDjpi2m/FY642yR4Wto+UbuivsNnvYelv+A3aFzWYLW2/LD4yuClev0+GUU8f2HglXb0xUTOBDuavC7aPoqOjAl0hXhdtH7b3/IhFuH7X3/otEuH3U3vsvEuH2UXvvv0i0t4/4jDDxGWHqjZ8RJ0LE3f3r6upUXFys4uJiSeapu8XFxSo5csnZe++9VzNnzgzMP3v2bO3atUvz58/Xpk2b9MILL+j555/XT37yk+55BQAAoEeLOC6tXr1aF1wQ7Ng2f/58SdKsWbP00ksvqaysLBBMJCk/P1/Lly/XHXfcoaeeekpZWVl64oknOK0XAABIOsY+IydKZ485AQCAk0dnv7+5KhMAALAUYQQAAFiKMAIAACxFGAEAAJYijAAAAEsRRgAAgKUIIwAAwFKEEQAAYCnCCAAAsBRhBAAAWIowAgAALEUYAQAAliKMAAAASxFGAACApQgjAADAUoQRAABgKcIIAACwFGEEAABYijACAAAsRRgBAACWIowAAABLEUYAAIClCCMAAMBShBEAAGApwggAALAUYQQAAFiKMAIAACxFGAEAAJYijAAAAEsRRgAAgKUIIwAAwFKEEQAAYCnCCAAAsBRhBAAAWIowAgAALEUYAQAAliKMAAAASxFGAACApQgjAADAUoQRAABgKcIIAACwFGEEAABYijACAAAsRRgBAACWIowAAABLEUYAAIClCCMAAMBShBEAAGApwggAALAUYQQAAFiKMAIAACzVpTCycOFC5efnKzY2VoWFhfrggw86nH/x4sU644wzFB8fr/79++uGG25QZWVll1YYAACcWiIOI0uWLNG8efN03333ae3atTrnnHM0bdo0lZSUhJ3/ww8/1MyZM3XTTTdpw4YNWrp0qT799FP94Ac/OOaVBwAAPV/EYeR3v/udbrrpJv3gBz/QyJEj9dhjjyk7O1uLFi0KO/9//vMf5eXl6fbbb1d+fr6mTJmiH/7wh1q9evUxrzwAAOj5IgojTU1NWrNmjaZOnRoyferUqVq5cmXYZSZNmqQ9e/Zo+fLlMgxD+/bt05///GddfvnlXV9rAABwyogojFRUVMjr9SozMzNkemZmpsrLy8MuM2nSJC1evFjTp09XTEyM+vXrp5SUFP3+979v9+80NjbK7XaHFAAAcGrqUgdWm80W8tgwjDbT/DZu3Kjbb79d999/v9asWaM333xTO3bs0OzZs9utf8GCBXK5XIGSnZ3dldUEAAA9gM0wDKOzMzc1NSk+Pl5Lly7VN7/5zcD0uXPnqri4WCtWrGizzIwZM9TQ0KClS5cGpn344Yc655xzVFpaqv79+7dZprGxUY2NjYHHbrdb2dnZqqmpUXJycqdfHAAAsI7b7ZbL5Trq93dELSMxMTEqLCxUUVFRyPSioiJNmjQp7DL19fWy20P/TFRUlCSzRSUcp9Op5OTkkAIAAE5NER+mmT9/vp577jm98MIL2rRpk+644w6VlJQEDrvce++9mjlzZmD+K664Qq+//roWLVqk7du366OPPtLtt9+us846S1lZWd33SgAAQI/kiHSB6dOnq7KyUg8//LDKyspUUFCg5cuXKzc3V5JUVlYWcs2R66+/XrW1tXryySf14x//WCkpKbrwwgv161//uvteBQAA6LEi6jNilc4ecwIAACeP49JnBAAAoLsRRgAAgKUIIwAAwFKEEQAAYCnCCAAAsBRhBAAAWIowAgAALEUYAQAAliKMAAAASxFGAACApQgjAADAUoQRAABgKcIIAACwFGEEAABYijACAAAsRRgBAACWIowAAABLEUYAAIClCCMAAMBShBEAAGApwggAALAUYQQAAFiKMAIAACxFGAEAAJYijAAAAEsRRgAAgKUIIwAAwFKEEQAAYCnCCAAAsBRhBAAAWIowAgAALEUYAQAAliKMAAAASxFGAACApQgjAADAUoQRAABgKcIIAACwFGEEAABYijACAAAsRRgBAACWIowAAABLEUYAAIClCCMAAMBShBEAAGApwggAALAUYQQAAFiKMAIAACxFGAEAAJYijAAAAEsRRgAAgKUIIwAAwFJdCiMLFy5Ufn6+YmNjVVhYqA8++KDD+RsbG3XfffcpNzdXTqdTgwcP1gsvvNClFQYAAKcWR6QLLFmyRPPmzdPChQs1efJkPf3005o2bZo2btyonJycsMtcffXV2rdvn55//nkNGTJE+/fvl8fjOeaVBwAAPZ/NMAwjkgUmTJigcePGadGiRYFpI0eO1FVXXaUFCxa0mf/NN9/UNddco+3btystLa1LK+l2u+VyuVRTU6Pk5OQu1QEAAE6szn5/R3SYpqmpSWvWrNHUqVNDpk+dOlUrV64Mu8yyZcs0fvx4/eY3v9GAAQM0bNgw/eQnP9Hhw4cj+dMAAOAUFdFhmoqKCnm9XmVmZoZMz8zMVHl5edhltm/frg8//FCxsbF64403VFFRoVtuuUUHDx5st99IY2OjGhsbA4/dbnckqwkAAHqQLnVgtdlsIY8Nw2gzzc/n88lms2nx4sU666yzdNlll+l3v/udXnrppXZbRxYsWCCXyxUo2dnZXVlNAADQA0QURvr06aOoqKg2rSD79+9v01ri179/fw0YMEAulyswbeTIkTIMQ3v27Am7zL333quamppA2b17dySrCQAAepCIwkhMTIwKCwtVVFQUMr2oqEiTJk0Ku8zkyZNVWlqqurq6wLTNmzfLbrdr4MCBYZdxOp1KTk4OKQAA4NQU8WGa+fPn67nnntMLL7ygTZs26Y477lBJSYlmz54tyWzVmDlzZmD+a6+9Vunp6brhhhu0ceNGvf/++7rzzjt14403Ki4urvteCQAA6JEivs7I9OnTVVlZqYcfflhlZWUqKCjQ8uXLlZubK0kqKytTSUlJYP7ExEQVFRXptttu0/jx45Wenq6rr75av/jFL7rvVQAAgB4r4uuMWIHrjAAA0PMcl+uMAAAAdDfCCAAAsBRhBAAAWIowAgAALEUYAQAAliKMAAAASxFGAACApQgjAADAUoQRAABgKcIIAACwFGEEAABYijACAAAsRRgBAACWIowAAABLEUYAAIClCCMAAMBShBEAAGApwggAALAUYQQAAFiKMAIAACxFGAEAAJYijAAAAEsRRgAAgKUIIwAAwFKEEQAAYCnCCAAAsBRhBAAAWIowAgAALEUYAQAAliKMAAAASxFGAACApQgjAADAUoQRAABgKcIIAACwFGEEAABYijACAAAsRRgBAACWIowAAABLEUYAAIClCCMAAMBShBEAAGApwggAALAUYQQAAFiKMAIAACxFGAEAAJYijAAAAEsRRgAAgKUIIwAAwFKEEQAAYCnCCAAAsBRhBAAAWKpLYWThwoXKz89XbGysCgsL9cEHH3RquY8++kgOh0Njxozpyp8FAACnoIjDyJIlSzRv3jzdd999Wrt2rc455xxNmzZNJSUlHS5XU1OjmTNn6mtf+1qXVxYAAJx6bIZhGJEsMGHCBI0bN06LFi0KTBs5cqSuuuoqLViwoN3lrrnmGg0dOlRRUVH661//quLi4k7/TbfbLZfLpZqaGiUnJ0eyugAAwCKd/f6OqGWkqalJa9as0dSpU0OmT506VStXrmx3uRdffFHbtm3TAw880Km/09jYKLfbHVIAAMCpKaIwUlFRIa/Xq8zMzJDpmZmZKi8vD7vMli1bdM8992jx4sVyOByd+jsLFiyQy+UKlOzs7EhWEwAA9CBd6sBqs9lCHhuG0WaaJHm9Xl177bV66KGHNGzYsE7Xf++996qmpiZQdu/e3ZXVBAAAPUDnmiqO6NOnj6Kiotq0guzfv79Na4kk1dbWavXq1Vq7dq3mzJkjSfL5fDIMQw6HQ2+//bYuvPDCNss5nU45nc5IVg0AAPRQEbWMxMTEqLCwUEVFRSHTi4qKNGnSpDbzJycna926dSouLg6U2bNna/jw4SouLtaECROObe0BAECPF1HLiCTNnz9fM2bM0Pjx4zVx4kQ988wzKikp0ezZsyWZh1j27t2rl19+WXa7XQUFBSHLZ2RkKDY2ts10AADQO0UcRqZPn67Kyko9/PDDKisrU0FBgZYvX67c3FxJUllZ2VGvOQIAAOAX8XVGrMB1RgAA6HmOy3VGAAAAuhthBAAAWIowAgAALEUYAQAAliKMAAAASxFGAACApQgjAADAUoQRAABgKcIIAACwFGEEAABYijACAAAsRRgBAACWIowAAABLEUYAAIClCCMAAMBShBEAAGApwggAALAUYQQAAFiKMAIAACxFGAEAAJYijAAAAEsRRgAAgKUIIwAAwFKEEQAAYCnCCAAAsBRhBAAAWIowAgAALEUYAQAAliKMAAAASxFGAACApQgjAADAUoQRAABgKcIIAACwFGEEAABYijACAAAsRRgBAACWIowAAABLEUYAAIClCCMAAMBShBEAAGApwggAALAUYQQAAFiKMAIAACxFGAEAAJYijAAAAEsRRgAAgKUIIwAAwFKEEQAAYCnCCAAAsBRhBAAAWIowAgAALNWlMLJw4ULl5+crNjZWhYWF+uCDD9qd9/XXX9fFF1+svn37Kjk5WRMnTtRbb73V5RUGAACnlojDyJIlSzRv3jzdd999Wrt2rc455xxNmzZNJSUlYed///33dfHFF2v58uVas2aNLrjgAl1xxRVau3btMa88AADo+WyGYRiRLDBhwgSNGzdOixYtCkwbOXKkrrrqKi1YsKBTdYwePVrTp0/X/fff36n53W63XC6XampqlJycHMnqAgAAi3T2+zuilpGmpiatWbNGU6dODZk+depUrVy5slN1+Hw+1dbWKi0trd15Ghsb5Xa7QwoAADg1RRRGKioq5PV6lZmZGTI9MzNT5eXlnarjv/7rv3To0CFdffXV7c6zYMECuVyuQMnOzo5kNQEAQA/SpQ6sNpst5LFhGG2mhfPqq6/qwQcf1JIlS5SRkdHufPfee69qamoCZffu3V1ZTQAA0AM4Ipm5T58+ioqKatMKsn///jatJa0tWbJEN910k5YuXaqLLrqow3mdTqecTmckqwYAAHqoiFpGYmJiVFhYqKKiopDpRUVFmjRpUrvLvfrqq7r++uv1pz/9SZdffnnX1hQAAJySImoZkaT58+drxowZGj9+vCZOnKhnnnlGJSUlmj17tiTzEMvevXv18ssvSzKDyMyZM/X444/r7LPPDrSqxMXFyeVydeNLAQAAPVHEYWT69OmqrKzUww8/rLKyMhUUFGj58uXKzc2VJJWVlYVcc+Tpp5+Wx+PRrbfeqltvvTUwfdasWXrppZeO/RUAAIAeLeLrjFiB64wAANDzHJfrjAAAAHQ3wggAALAUYQQAAFiKMAIAACxFGAEAAJYijAAAAEsRRgAAgKUIIwAAwFKEEQAAYCnCCAAAsBRhBAAAWIowAgAALEUYAQAAliKMAAAASxFGAACApRxWrwAAIEI+n1RbKx06JNXXhy+HD4c+bm42i8fTdthy3OeTbLajF7tdiokxi9MZOmw9HhsrJSRIiYlm8Y/7h7GxZp3otQgjAGAFr1c6eFCqqJAOHDCHFRVSdbVUUxMchhuvrZUMw9r17052e2hYSUkJltTU0GG4aWlpkoOvs56MvQcA3aWuTiorCy3794cGDv/w4MFjDxRRUVJ8fMclLs4sTqf5he1wSNHRocOW43a7uV5HKz6f1NQULI2NocOW44cPm604hw6Z28g/PHzYfB3+lp7a2q5vi9RUqW/ftqVPn/DTY2OPbdujWxFGAOBoamul3bul0tK2YaNlqauLvO7U1OAXZnq6+djlMn/xu1xtx1s+djp79uENr9c8hNQyoNTVma0/1dVSVVXHQ3+LkWROq6qSNm/u3N9OTpb695f69TOH/tLycb9+ZqtLT97GPQRhBEDv1tQk7d2rg18Va9m/nlJceYXSD9QrvfLQkWG9Eg41dbo6X0K87FkDpP79VRLfrE32g6pNdqo2ySm3K1Z1SU65k52qTXbqUKJTXkfoeQQ5rgH61dd+FXj847d+rH2H9un+cfdrWPowSdKbW9/UKx+9EtHLTI1N1e8v+33g8UPvPaQtB7do3tnzND5rvCTpo5KPtGj1oojqjbJH6Y9X/THw+L9X/bfWlK3RTWNv0gX5F0iS1u1bp19/9OuI6lW89PTVTyshJkGS9Nxnz+m9ne/pu6Ou15UjrpQk7arepZ8V3avEuiYluRuV7G4whzXmMKm2xbi7QclHpjk8PsntNstXX3W4Gs0Ou2pSYlWdGqfqlDhzmBang+nxIaUhLlr3n9dqH33xiiZlT9ItZ94iSfL4PLr+r9dHth2ksPuoIKNA90y5JzDP//vb/1N9c31E9bbcR1YjjAA4tdXUSDt2mGXnTrOFo6QkONy3TzIMpUm6vqNqnNLeJKk0SSpLkpwDc3X1RXMDv6ILl31dm511+uzHxRqaPlSS9Id//1QLPlwQWpEhqeZICeOMzDNCwsiyzcu09eBW3XLmLYEvui8rvtTidYsj2gxZSVkhYeStbW9p1Z5V+s6o7wS+6HZW74y43mh7dEgYWbFrhf7vq//TebnnBb7oyurKIq5Xkp687MnA+Md7PtbidYs1qu+oQBipaqjSKxtfDV0o8UgZ0E6lhrR15moNbk6Uysr0WtF/6+M1y/T1pEJ9LXakVF6u5r275d75ldIPS9Een/pU1KtPRcdf9DVOKTrnIyl/mDRggFK0V/EH3lbd6Xsk5xRp4ED5khO6tB3C7aOLB10cEkb+d8P/qqaxnTdVO1ruI6sRRgD0bA0NZsjwB47Wparq6HU4nSpPjdbG2DopJ1vxg0aoLiNF7n4pqstIVW1mipoSQvsY5LhypFHfDjy+Kf7XavQ0qk98n8C0y4Zepr7xfSN6OX0TQuf/6ZSfqrqhWnkpeYFp5+aeq99N/V1E9fpbGPxun3C7vjvquyrIKAhMG9d/XMT12m2hLTs3jr1R5+Wep7MHnh2YNjx9eMT1SlKsI7jNrx59tUb1HaXJOZMD07KSsrpUb1r/QVJcqjR8uAYOcqhs7/mK6j9WyjtfktTYVKeX1zyrqCaP4g/WKqHSrfjKWiVWupVQ6VbCgRolHqhR0v5qJR6oUWztYbkaJW3ZaRZJZx8p+vsK6VdnSJKi4+J0sE9f1fZ1qS4jRXUZLtVmpsqdmaLafmlyZ6aoOaFtX5Zw+yjHlRMyz6++9is1ehoj2g4t95HVbIZx8nfJdrvdcrlcqqmpUXJystWrA+A4KC4v1r+3/1uS9PVhX9fwPsMlSV/t26gVHy3WkJooXaj8QMgoW7dSiXsrlFThPmrd9SkJqumfptghI5Q6/AwpJ0cVaXH6Z+MGReXl6doLbtdrG5bo/V3v69rTrtWUnCnH9bXiFFNXJ+3dK+3Z036pqOhcXSkpUm6ulJMTfpiZaXYy7iE6+/1NGAFgOcPj0cT7s+TafUBDK6Xb06dp2EGbtGWLfDu2y+7xdrh8bYy0I0XakRp+eMhpzvfktCd161m3SpJW7Fyh8/94vkb2GamNt248jq8OkNmC1zqw7N5tll27zEOGnWnFi4mRsrPNcBIusOTknFRnCnX2+5vDNABODK/X/ODdsiW0bN0qbd+u/zQ3t5j5n4ExuySPwy53ZqrSRo2T8vOl/Hw9X/WOdqVH6UBmouqS2p5V4pI05kjx8/e5kKSMhAx9//TvKysxq/tfK9BabKw0eLBZ2lNba4aSkpJgQGk53LvX7HC9bZtZ2pOR0Tao5OUFhykp3fzijh0tIwC6j88XDBxbt4aGju3bzQ/SdjRESSV9HBo24TJp6NBgGTJEGjDAvKYG0Jt5PObp5eGCin946NDR60lObhtQcnOls8+WBg7s1lWmZQTA8eHzmb/QWrdubNli/lpr7KATXUyMNGhQaNAYOlRveNbrO6vu0HmDztE7s/7vxL0WoCdxOIKHYsIxDPNQT+uA4i87d5oX3XO7pXXrzNLSiy9K119/vF9FWIQRAG35fOYvsNatG/7A0dDQ/rIOR2jgaBk8cnLCtnB8/t6H8tmlIWlDjuOLAk5xNpt5kba0NGnMmPDzHDoUDCk7d4YOhw8/gSsbijACnAKavc3yGb6IlrFJitlfGQgZns1fybZ1q+xbt8m2dWvwUt1hGA6HjPw8GUOGyBg82BwOHSJjyGDZcnMV44wPzNvkbZJhGIqOig6cBur1eeXxeQLzfFVpXniKMAIcZwkJ0siRZjmJEEaAU8DcN+eGv3KmIWXWSUMPSkMrg8MhB6Xh1XapMRhg2nwYREXpYGayPo6v0pY0aUu6tCVN2pom7UrxyBO1VdJWc95qSZ+aZVTfUdpwy4ZANWP+MEabKjbp3Vnv6vwj13H4w+o/aM4/57RZXcII0DsRRoCezjCUVFWvybvMkNEyeAw5KCW122fUZ16vIC9PGjpUz9eu0BfJDfrxzEXKGf81KS9PT3z0Sz204qET8jIyEjI0OXvy0WcEcMrhbBqgJzAM86JJLfpu+LZskW3rVtm2bOnwbqeG3S4je6B8QwYHy+BB0pChih8+2uxUKqm2sVaGDCVEJyjKbvbraPQ0qtEb2VUd7Ta7EmMSA4/rmurkM3yKj46Xw27+/mnyNqnBE9rvpOXzAE4NnE0D9DSGIVVWhu80unVr8O6kR4Rcg9FmMzuHtjhDxV9s+fmyOZ062jUbk5xJbaY5HU45Hc5jelktg4lfTFSMYqJijqleAKcOwghwoh082PaUWH+pru542ezsQMhYneDWz0tfVfrpE/TCHe+dVFddBIBIEEaATnri4yfU2aOazppDStlzQOc29VfWvnppyxY1fLle2rJVsbUd3/2zNiNF1dl9VT2wj6oH9lV1dh9VZ/dVTVa6vLFma8K5ueeqsN8YLax9VNUN1QQRAD0aYQTopPlvzZfXCN4jJa3+SIfRymDHUf/jtDCX4QiJC1lZgRaOX5Yu0ZrEWm1Jl7alSodjqmWenrLFnLfhyOiW4OKPXfKYxvYfqwHJAzQgub17pQNAz0AYATrSog/H/24YpcxSt/qV1yqzvE6Jde1f2lySKtPi5Bg2XK6C8dKQIdqaZtOz7neVPGqs7pv2q8B8e/8RrdiGap0m6bROrpb/jrYAcCrgbBqgsjK0/0bL4dHuojlgQGinUf9w0CDz4kIA0ItxNg16neqGatU31ysxJlHJTvNN3+xt1v66fbJXVStq23Y5tu+UY9uOkKG9uqbDevckSckF45Q8elxo4Bg8WIqP73BZAMDREUbQYzV7mxUdFW0+8Pn06JK5+vC9l3VH3yt0VfRp0rZt8ny1XnFfbgjbh6OlPUmhVxi99brHlFt4oV6u/Ujbm8p121m3SfHpx/9FAUAvxGEa9ByNjdKOHeaN2rZt07NL71F+pU/negYopmRvx3eLlbQ7WdqWZtPWdGnrkeG2dNuRTqO2kHnX/nCtCjIKjuerAYBTHodp0DNVVwfCRpuyZ495YbAjbg6MbTcHDoeUm2sePglTsuPjlS3p/BP6goCTjGGYV+ytqgot1dXmHV0PH25bGhvNOzn7//8MI1gkKTpacjrDl8REKTlZcrnaDl2uwBWA0bsRRnDMvD6vvveX76m4vPio8w5LHaK/X/BsIGC8+tdfKGnPfp3vzVHi7nLzgmAdOBRjU0mfaO1Kj9K6pMPan5moR3/0uhk4cnLMQAL0Rl6vtHeveXv4sjKzlJYGx8vKpPJyM3h4vUev70RJSpIyMqS+fcMPMzOlgQPNktj2ar44NfDJjWO2fv96Ld24NPA4tlnKq5byq6RBVdLgKmnwwSPD6q1S88DAvN8LjG0MjB1KS1JxQq22p5rX3diWFhzuTzAkW/CU2iuHf026+OLj+wKBk0V1tbR5s7R9u3nIsmUpKZGamztfV0yMlJoqpaWZw5QU88s+Li5YYmODQ7vdvO2A7cghzZbjzc1m60nr0tBgtrbU1Ehud+iwrs5ctrbWLNu2HX2dk5ODwWTAgOD4wIFmq2heHmex9VCEEUTO6zUPmRz5QIz99E39z0fS6ENxGl0br5gDlR0sbEhRUYHDKQf6J6tuYKYyTp+ohJGnS4MGyW3USlXbNUjSIEntRQ27za5x/cd1/+sDrOTzmf9fX34ZLJs2mcPy8o6XdTjMWwZkZUn9+weH/tKvn5SeboaPuLhgmLCC12uGkooK6cABaf/+tsP9+80Wnb17zRDjdksbN5qlPRkZUn6+WfLyguP5+WbrKYeFTkp0YO1ltlRu0a6aXR3PZBiKrqxW3J5yxe0uV+LeA8qvMgK/wHwlu2T3HKWZNynJ/OcfNKht/42cHPMYM9DbVVdLX3whff55sGzaZLYmtCcry/w/CveFO2CAGfZPRbW1ZijZs8csLcd375Z27mxzM8k27HZzG+Xnt7020ODBHAY6Djr7/U0Y6UXKass08qmRqmmsUUKjlF8dPJTiH/cPE4/S2tsYJTUN7K+k4adpb7pTnzor5Bo5RhdceKP5j56WZu2vLuBk4vOZhyFaho7PPzcPrYTjcJhfkCNGmGXkSHM4fLh5qALhVVWZoaT1ISz/tMOHO16+X79gQGkZVoYMMX9gIWKEkd7u8GHzg27nTmnXrmCrxrZtqvvqCyXXdnwpc59NOuByaG96jPZnJunSi2cHfn3N3/x7vdP4pR699He6eDD9NYAQxpFWxE8/DZbPPgv2kWgtJ0c644xgKSgwf6XTeti9DMM87LNjh3mIeevW0KstV1R0vHxmZtuA4h/ne6ldhJFTndtthoxdu6SdO/XFJ3/Xjs/f08CDHg12RynF3XHYkGS2XrQ8nuo/rJKfb/bpcDqP/+sAerqystDgsXq1eYuB1mJjzaDRMnicdprZfwPWq65uG1D8wwMHOl62b99gQBkyxAyT/vFe3kpMGOmhGj2NavY2Kbq6Vs49ZdKuXfLt3CnP9q2yl5TIVrJb9l0lslVXH72yxEQpL09ve77SV8nNuvryu5R52tnB4OFyHffXA5xSqqrMsNEyfOzd23a+6GhpzBjpzDODZfhwTj3vqWpqgkGldVjZv7/jZVNSwoeUIUPM1pZTPKhw0bOTmc8n7dsXaNVo2cJRvfFTJZRWyNmiz4ZdUrj+35Vx0i6XtDNF2pVijt/63d8oaWiB+hVMMH9x2WwafHCb8g2fUlNypSh6kgMdeXPrm7pp2U2KbfLptD3NGrO7WWfsbtaY3U3Kr2jbcdtnkzZnOjT0kmsVddYE6cwzNa/0BS3dtkwPnnezbi68XpK0unS1rnztyojXZ/XNq9U/qb8k6YF3H9Bza5/TbWfdpnum3CNJ2l61Xee8eE7E9b79/bc1OmO0JOmJj5/Qrz/6ta4tuFaPTn1UklTTUKNRC0dFXO+fvvUnnZd3niRp8ReLdde/7tIlgy/RC1e+EJgn77E8NfsiOA1Z0u+n/V7fGvktScF9VNi/UMu+tywwT+EzhSqvO8oZR608eN6DurnQvISifx/lp+Trwxs/DMxz8bLvaOOBI2fwxEk6/UiRQ4kN/ZRb4VVepUd5FR7lHRkvcMcq+YDbbHFZvdosrTTEOrQtTdqZ7tDOPg7t7BNlDtMdKnPZZdjbBpX29tGOuTsUc+Tzfc7yOXrjyzci2g4vXfmSpYfduxRGFi5cqEcffVRlZWUaPXq0HnvsMZ1zTvv/DCtWrND8+fO1YcMGZWVl6a677tLs2bO7vNIn0qGmQ9rj3hN4nBqXqoyEDEnmvVG2V5lX/2x5S/fSvV+pcedWOfaWKXpv+ZFhWXC4p1T2pvD/iJkhDzKlvDwdHpCh35f/LSR07EqR6lodRbly+JUaes2dbeocnDa4C68c6EWam6V166RPPtHIoj/rnx+WavQBKSpMu/G2VOnTLOnTAebws/7SIadHjT97VlFHvgwOvP6YSmtLdag5eFZMk7dJpbWlEa+az/AFxmsaa1RaWyp3ozswzevzdqnelmGgrqlOpbWlqm6oDkwzZHSp3kZv8LYM9c31Kq0t1cHDoRczLK0tjTiMHG4Odj5t8DSotLZUua7ckHnK68ojXudw+yg+OvQGmAcOHeiw3s0uSS6Z1yI44t4pP9avJv5M2r5dpWs/0H+9cotG1cTopqRzzVaVkhLFNng0ulQaXeppU2dDlMxrLaWZPzj9JVlbpcJKKS2t3X1U1VAV8XZo8BzlBl7HWcSHaZYsWaIZM2Zo4cKFmjx5sp5++mk999xz2rhxo3JyctrMv2PHDhUUFOjmm2/WD3/4Q3300Ue65ZZb9Oqrr+rb3/52p/6mVYdpDjUd0tDfD1VZXVlg2vxxc/Rfo++Qdu/Wwc2f67evzVVurV0/zJhmdhjdvdtMwkfhtUl7k8xQsTNFyjv9XJ121hVKHDpanuwBsuflKSbBfK2GYXTqjRIXHdfFVwr0Ij6f9NVXoYdaiovD3tuoOaOP6s8YpUNjRqt+7GjVnz5K3rSUsNWe0e8M2W12SdLO6p2qOlylAckDAj9e6prqtKVyS8SrOzpjdOAX7x73Hh04dECZiZnKSsqSZH6JbDqwKeJ6h/cZHvjSLa8rV1ltmdLj05XjMj/HPT6P1u1bF3G9g9MGB+6aXVFfod01u+WKdWlQavCburi8WJH2EMhNyVVaXJok8w7dO6p2KCEmQcPShwXmWb9/vZq9kYWccPvI6XBqVN9gq9CXFV+GhKHOCLePouxROj3zdHOGxkbtKl4hz5av5NyxWzE7d8vpLyWlsnnaBpQQiYky8nLl7pempuwspY8aL/uR/n4lqXZVximiQ0CDUgfJFdv9h+6PW5+RCRMmaNy4cVq0aFFg2siRI3XVVVdpwYIFbea/++67tWzZMm3aFPxnmT17tj7//HOtWrWqU3/zeIaRlz9/WR+WmM1xNp+h5OrDSjtQp9SKOkWXlsu9ZYNy3FJebZQGVPuUUWfI3oktVhNr016XTXtS7NqbYtdel90cd9m1O9WuUpddnqjgGyXHlaN3Zr0T+GcD0A0MwzwE2jJ4rFljXrOilfpEp7YPSlXf8y5T5gVfN/t5DBhwyh/Tx0nI4zF/2G7dap4S7j+k7y9Hu/idZF6JNi/PvAjewIHhhyfguirHpc9IU1OT1qxZo3vuuSdk+tSpU7Vy5cqwy6xatUpTp04NmXbJJZfo+eefV3Nzs6LDnL7W2Nioxha/Utxud5t5usUzz6jfi/+frivbr5waaYBbivG1N3PwWLEnxiFHTp55Sl52tv7ZsF770p268qI5Sh12upSdraI9b+sfW/7RppY+R8rYMH/h8f88rocueCjw+O6iu7W/fr9+OuWnGpo+VJL01ta39NqG1yJ6mSnOFP33pf8dePzL93+prVVbddtZtwWuYLpq9yo989kzEdUbZYvSc994LmT915av1fVjrtf5eedLMn+p/HblbyOqV5IWXr4w8KvtxbUvasWuFfr2yG/riuFXSJJ2Ve/SA+89EHG9j1z0iPol9pMk/Xnjn/X3zX/X1MFTde1p10qSqg5Xad5b8yKu92fn/Cywj97c+qZeXf+qJg6cqNnjzcORHp9HNy27KeJ6506YG9hHK3ev1NNrnlZB3wLdOTl4OG7232frsCeyX203jLkhsI/W7Vun3676rXKSc/TzC38emOfOt+/UvkP7Iqr3O6O+o28M/4Yks3Xg/nfvV2psqh6f9nhgnodXPKytB7e2Wba2qVZ73WE6g0qaecZMzTlrjiRpr3uvvrnkm4qPjtd7178XmOf2f96uzRs/0KidhzRy5yGNOlJS68I0gUfb9WVOvBrGnqazvzlHOvNM/Wj9z/XyF/+j92bNVOaRPg+AJRyO4IkG4W530fLSDf7SMrCUlZkXztuwwSztcblCA8qNN0oTJx6Xl3Q0EYWRiooKeb1eZWaG9GxQZmamyttJauXl5WHn93g8qqioUP/+/dsss2DBAj300ENtpne7d9/V1P+E9oT22qTSJPN28yUuabcrdLzEJY0tOF9vzywKLHPNIy65G93afO5LSj3yhfRZ2Wd6qfiliFZnTL8xIWHk9S9f19aDW3XzuJsDX3QbD2yMuN6spKyQMPKPLf/Qqj2rdNXwqwJfdNurtkdcb7Q9OiSMvLPzHS37apkmZ08OfNGV1pbqj5//MaJ6JenxSx+XjuTUlbtX6o+f/1HD0ocFwsjBwwe7VO9959wXCCOflX2mP37+R6XGpgbCSH1zvV7+/OWI6/1h4Q8D+2jTgU16+fOX5fF5AmHEZ/i6VO+3RnwrsI92VO3Qy5+/rIsHXRwSRl5d/2pIH4LOmJI9JbCPyurK9PLnL2tMvzEhYeSvX/01bGjoyIg+IwJhpOpwlf7ni/9RVlJWSBh5c+ubWrWnc62ifhfmXxgYb/Q26tPSTzW4IV5avlxau1b67DP9bMVyZVS2PZzZbJe+yAzt57Gxr0/eqDrdMGaEzr7S3PePZz+h1zYs0ci+IyNaN+CEi4szz84aPjz88w0NwbDiv0Jty6vV7tljniHkL/7ActFFPSOM+NlaNVsahtFm2tHmDzfd795779X8+fMDj91ut7Kzs7uyqh275hp9mR2nzfENcme45M5MUV16kgxH6OWUBxwp/l2Ul5IX8vyD5z2oJm+T0uPTA9MuGXyJXM7Ijr/5j1v63TnpTlU3VId00pqSM0W/vujXEdWbGBPaFHfLmbfoqhFXhRwTHdt/bMT1+o+P+806Y5YmZ0/WWQPOCkwbmjZUv7noNxHVK0mxjtjA+HdGfUfD0ofpnNxgJ+mspKwu1dsnvk9g/NIhlyo1NlWFWYWBacnO5C7V23IfnZN7jn5z0W9CvtSibFF69OJHI67Xf7aDZO6jRy9+tE2nvZ9f8HM1eTtxXZkWJgycEBgflj5Mj178aJv3392T7w7p0NgZU3KmBMazkrL06MWPtnn/zTlrjr49sm1/MbvNrvzUfEXbW7WW+nwa4Y6Rli6V1q5V7merdXhNimIrqqVHLg/MliHJsNlUNzhb1acNVc1pQ1VTMFTuEfnyOWOULSlb0rdaVD0wOXjTxsSYRK370bo22wHocWJjpWHDzNKe2tq2AWWcdff6iqjPSFNTk+Lj47V06VJ985vfDEyfO3euiouLtWLFijbLnHvuuRo7dqwefzz4y+iNN97Q1Vdfrfr6+rCHaVrrTdcZAXq1hgbzhnDFxWaLx9q15niYPh6y281LpI8da5bx480PUy7bDZw0jkufkZiYGBUWFqqoqCgkjBQVFenKK8OfPz9x4kT97W9/C5n29ttva/z48Z0KIgBOQV6v2TFv/XrzlNr1682yZYv5XGtOp3m1Un/wGDtWOv10KT6+7bwAepyID9PMnz9fM2bM0Pjx4zVx4kQ988wzKikpCVw35N5779XevXv18svm8fHZs2frySef1Pz583XzzTdr1apVev755/Xqq6927ysBcPLxes1j119+GQwc69ebt4BvaOd09dRUM2i0DB4jRnCvFuAUFnEYmT59uiorK/Xwww+rrKxMBQUFWr58uXJzzePYZWVlKmlxJ8r8/HwtX75cd9xxh5566illZWXpiSee6PQ1RgD0ADU15rU7/OXLL83hli1hr98hyeyEN3q0eb+WggKz5aOgQOrfn9NpgV6md9+bxuPhXhFAZ1VXB+946r/rqT947OvgFOCYGPPupqNHBwNHQYF52mJUVPvLAejxuDdNZ8yeLS1eLPXpEyzp6eHHWz7mODVORY2N5k3f/GGjdamq6nj5/v2DpxuOGBEcz80ldADoUO8OIxUV5nFr/+lNnRUX135Q8Y+np5vHvlNSgkNaYWCV5maptNQ8hc9/Gl/r8Y5aN/wyMqRBg4LFHziGD5c40w1AF/Xub8dXXpEOHDBDSWWlOexovKLC/FA/fDjyACOZpxympnZc/OGldaHzHlozDMntNi8NvW+fOWw57h+WlprDzhyRjY01D5+0DBz+kpd3Qi4fDaD36d1hJDHRLPn5nZvfMKS6us6FloMHzWbtqipzGcm8VkJtrXl2QaQSEsxfnv6SlBT6uLPPOZ10DjwZGYbZCbSy0nzvVFaGjreetn+/GTbaOyMlnOjo4KWf/aX14/R03h8ATrjeHUYiZbOZX/RJSZ0PMJLZmlJdbQYT/7Cj0nKemhqzjkOHzFJW1sEf6gSHwwxgCQlm35eWw86M+4dxcWaw8ZfY2NDHTuep20/A5zNbx+rrzWHL4p926JDZalFbaw79peXj1s+Fu75GZyQnS5mZUr9+ZvGPt5yWnS317WteKAwATjKEkRMhOtr8IujbN/JlvV4zkFRVBb+8Wn+JtVdaf/FJ5hlE1dVmOd4cjvYDS0yM+XxUVPhhR8/5Q47/sINhhI63NzQMc3t6PGZA9JeWj8M919QUGjiaIrv0ekQSEqS0NLOFwj8MN963bzBwxMUdv/UBgBOAMHKyi4oyv3zS0o6tHp/PPFxUU2P+aq+vDx22N97e8w0N5tkX/qG/tOyX4PGY5dChY1v3k1lMjNlaFBcXWjpzWK31tLQ0M7ABQC9DGOkt7Pbgl97xYhhma4I/mLQOKi0fNzUFWyk6Grb3nL9fQ8thuGmth/7WlejoYOnosX/cHzJaBo/Y2FP3UBQAnECEEXQfm81sKYiJ4WZlAIBOozcbAACwFGEEAABYijACAAAsRRgBAACWIowAAABLEUYAAIClCCMAAMBShBEAAGApwggAALAUYQQAAFiKMAIAACxFGAEAAJYijAAAAEv1iLv2GoYhSXK73RavCQAA6Cz/97b/e7w9PSKM1NbWSpKys7MtXhMAABCp2tpauVyudp+3GUeLKycBn8+n0tJSJSUlyWazdVu9brdb2dnZ2r17t5KTk7utXrTFtj4x2M4nBtv5xGA7nxjHczsbhqHa2lplZWXJbm+/Z0iPaBmx2+0aOHDgcas/OTmZN/oJwrY+MdjOJwbb+cRgO58Yx2s7d9Qi4kcHVgAAYCnCCAAAsFSvDiNOp1MPPPCAnE6n1atyymNbnxhs5xOD7XxisJ1PjJNhO/eIDqwAAODU1atbRgAAgPUIIwAAwFKEEQAAYCnCCAAAsFSvDiMLFy5Ufn6+YmNjVVhYqA8++MDqVeoxFixYoDPPPFNJSUnKyMjQVVddpa+++ipkHsMw9OCDDyorK0txcXE6//zztWHDhpB5Ghsbddttt6lPnz5KSEjQN77xDe3Zs+dEvpQeZcGCBbLZbJo3b15gGtu5++zdu1ff//73lZ6ervj4eI0ZM0Zr1qwJPM+2PnYej0c/+9nPlJ+fr7i4OA0aNEgPP/ywfD5fYB62c+Tef/99XXHFFcrKypLNZtNf//rXkOe7a5tWVVVpxowZcrlccrlcmjFjhqqrq4/9BRi91GuvvWZER0cbzz77rLFx40Zj7ty5RkJCgrFr1y6rV61HuOSSS4wXX3zRWL9+vVFcXGxcfvnlRk5OjlFXVxeY55FHHjGSkpKMv/zlL8a6deuM6dOnG/379zfcbndgntmzZxsDBgwwioqKjM8++8y44IILjDPOOMPweDxWvKyT2ieffGLk5eUZp59+ujF37tzAdLZz9zh48KCRm5trXH/99cbHH39s7Nixw/jXv/5lbN26NTAP2/rY/eIXvzDS09ONv//978aOHTuMpUuXGomJicZjjz0WmIftHLnly5cb9913n/GXv/zFkGS88cYbIc931za99NJLjYKCAmPlypXGypUrjYKCAuPrX//6Ma9/rw0jZ511ljF79uyQaSNGjDDuuecei9aoZ9u/f78hyVixYoVhGIbh8/mMfv36GY888khgnoaGBsPlchl/+MMfDMMwjOrqaiM6Otp47bXXAvPs3bvXsNvtxptvvnliX8BJrra21hg6dKhRVFRknHfeeYEwwnbuPnfffbcxZcqUdp9nW3ePyy+/3LjxxhtDpn3rW98yvv/97xuGwXbuDq3DSHdt040bNxqSjP/85z+BeVatWmVIMr788stjWudeeZimqalJa9as0dSpU0OmT506VStXrrRorXq2mpoaSVJaWpokaceOHSovLw/Zxk6nU+edd15gG69Zs0bNzc0h82RlZamgoID90Mqtt96qyy+/XBdddFHIdLZz91m2bJnGjx+v7373u8rIyNDYsWP17LPPBp5nW3ePKVOm6N///rc2b94sSfr888/14Ycf6rLLLpPEdj4eumubrlq1Si6XSxMmTAjMc/bZZ8vlch3zdu8RN8rrbhUVFfJ6vcrMzAyZnpmZqfLycovWqucyDEPz58/XlClTVFBQIEmB7RhuG+/atSswT0xMjFJTU9vMw34Ieu211/TZZ5/p008/bfMc27n7bN++XYsWLdL8+fP105/+VJ988oluv/12OZ1OzZw5k23dTe6++27V1NRoxIgRioqKktfr1S9/+Ut973vfk8R7+njorm1aXl6ujIyMNvVnZGQc83bvlWHEz2azhTw2DKPNNBzdnDlz9MUXX+jDDz9s81xXtjH7IWj37t2aO3eu3n77bcXGxrY7H9v52Pl8Po0fP16/+tWvJEljx47Vhg0btGjRIs2cOTMwH9v62CxZskSvvPKK/vSnP2n06NEqLi7WvHnzlJWVpVmzZgXmYzt3v+7YpuHm747t3isP0/Tp00dRUVFtktz+/fvbJEd07LbbbtOyZcv07rvvauDAgYHp/fr1k6QOt3G/fv3U1NSkqqqqdufp7dasWaP9+/ersLBQDodDDodDK1as0BNPPCGHwxHYTmznY9e/f3+NGjUqZNrIkSNVUlIiifd0d7nzzjt1zz336JprrtFpp52mGTNm6I477tCCBQsksZ2Ph+7apv369dO+ffva1H/gwIFj3u69MozExMSosLBQRUVFIdOLioo0adIki9aqZzEMQ3PmzNHrr7+ud955R/n5+SHP5+fnq1+/fiHbuKmpSStWrAhs48LCQkVHR4fMU1ZWpvXr17Mfjvja176mdevWqbi4OFDGjx+v6667TsXFxRo0aBDbuZtMnjy5zenpmzdvVm5uriTe092lvr5ednvoV09UVFTg1F62c/frrm06ceJE1dTU6JNPPgnM8/HHH6umpubYt/sxdX/twfyn9j7//PPGxo0bjXnz5hkJCQnGzp07rV61HuFHP/qR4XK5jPfee88oKysLlPr6+sA8jzzyiOFyuYzXX3/dWLdunfG9730v7KlkAwcONP71r38Zn332mXHhhRf26tPzOqPl2TSGwXbuLp988onhcDiMX/7yl8aWLVuMxYsXG/Hx8cYrr7wSmIdtfexmzZplDBgwIHBq7+uvv2706dPHuOuuuwLzsJ0jV1tba6xdu9ZYu3atIcn43e9+Z6xduzZwuYru2qaXXnqpcfrppxurVq0yVq1aZZx22mmc2nusnnrqKSM3N9eIiYkxxo0bFzgtFUcnKWx58cUXA/P4fD7jgQceMPr162c4nU7j3HPPNdatWxdSz+HDh405c+YYaWlpRlxcnPH1r3/dKCkpOcGvpmdpHUbYzt3nb3/7m1FQUGA4nU5jxIgRxjPPPBPyPNv62LndbmPu3LlGTk6OERsbawwaNMi47777jMbGxsA8bOfIvfvuu2E/k2fNmmUYRvdt08rKSuO6664zkpKSjKSkJOO6664zqqqqjnn9bYZhGMfWtgIAANB1vbLPCAAAOHkQRgAAgKUIIwAAwFKEEQAAYCnCCAAAsBRhBAAAWIowAgAALEUYAQAAliKMAAAASxFGAACApQgjAADAUoQRAABgqf8f3M7tdF1uNi0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot((model.predict(X[:1001,:])), 'g-.')\n",
    "plt.plot(Y[:1001,:], 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physics Informed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_X = tf.concat([tf.reshape(X[2002:3003,1], (-1,1)), Y[2002:3003,:]], axis=1)\n",
    "X_seq = transform_sequence(LSTM_X, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, None, 4)]         0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, None, 8)           40        \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, None, 64)          18688     \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, None, 1)           65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18793 (73.41 KB)\n",
      "Trainable params: 18793 (73.41 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None, 4))\n",
    "outs = keras.layers.Dense(8, activation='tanh')(inputs)\n",
    "outs = keras.layers.LSTM(64, activation='tanh', return_sequences=True, return_state=False)(outs)\n",
    "outs = keras.layers.Dense(1, activation='sigmoid')(outs)\n",
    "PINN = keras.Model(inputs=inputs, outputs = outs)\n",
    "PINN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(X, model, f_pred):\n",
    "    f_mean = tf.reduce_mean(f_pred)\n",
    "    eta = tf.reshape(X[:,1], (-1,1))\n",
    "    meanflow = X[:,2:]\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(X)\n",
    "        r = model(X)\n",
    "        dr_dn = tf.reshape(tape.gradient(r, X)[:,1], (-1,1))\n",
    "    \n",
    "    residual = dr_dn + RHS_ff_t(r, meanflow, f_mean)\n",
    "\n",
    "    del tape\n",
    "\n",
    "    return tf.reduce_mean(tf.square(residual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_theta(pinn, model, X_seq, X): \n",
    "     with tf.GradientTape(persistent=True) as tape:\n",
    "        # This tape is for derivatives with\n",
    "        # respect to trainable variables\n",
    "        tape.watch(pinn.trainable_variables)\n",
    "        f_pred = pinn(X_seq)\n",
    "        f_pred = transform_original(f_pred)\n",
    "        loss = loss_func(X, model, f_pred)\n",
    "\n",
    "     grad = tape.gradient(loss, pinn.trainable_variables)\n",
    "     del tape\n",
    "     return loss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define one training step as a TensorFlow function to increase speed of training\n",
    "@tf.function\n",
    "def train_step(X_seq, X, pinn, train = True):\n",
    "\n",
    "    # Compute current loss and gradient w.r.t. parameters\n",
    "    loss, grad_theta = loss_theta(pinn, model, X_seq, X)\n",
    "    \n",
    "    if train:\n",
    "        # Perform gradient descent step\n",
    "        optim.apply_gradients(zip(grad_theta, pinn.trainable_variables))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-12 21:34:02.772316: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-08-12 21:34:02.797808: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] function_optimizer failed: INVALID_ARGUMENT: Input 0 of node zeros_like was passed float from model_5/lstm_3/PartitionedCall:6 incompatible with expected variant.\n",
      "2023-08-12 21:34:02.865467: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] function_optimizer failed: INVALID_ARGUMENT: Input 0 of node zeros_like was passed float from model_5/lstm_3/PartitionedCall:6 incompatible with expected variant.\n",
      "2023-08-12 21:34:02.907009: W tensorflow/core/common_runtime/optimize_function_graph_utils.cc:573] Ignoring multi-device function optimization failure: INVALID_ARGUMENT: Input 0 of node zeros_like was passed float from model_5/lstm_3/PartitionedCall:6 incompatible with expected variant.\n",
      "2023-08-12 21:34:03.010280: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-08-12 21:34:03.542727: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000 - training_loss: 75961.96286963\n",
      "Epoch 2/2000 - training_loss: 39778.69769020\n",
      "Epoch 3/2000 - training_loss: 5268.43188164\n",
      "Epoch 4/2000 - training_loss: 2837.01616084\n",
      "Epoch 5/2000 - training_loss: 3415.10755773\n",
      "Epoch 6/2000 - training_loss: 9498.45949247\n",
      "Epoch 7/2000 - training_loss: 3599.14841594\n",
      "Epoch 8/2000 - training_loss: 20298.72043649\n",
      "Epoch 9/2000 - training_loss: 114657.02692043\n",
      "Epoch 10/2000 - training_loss: 927595.42825565\n",
      "Epoch 11/2000 - training_loss: 2957.77018195\n",
      "Epoch 12/2000 - training_loss: 3885.92414080\n",
      "Epoch 13/2000 - training_loss: 4352.48329551\n",
      "Epoch 14/2000 - training_loss: 6870.79784611\n",
      "Epoch 15/2000 - training_loss: 38910.79124634\n",
      "Epoch 16/2000 - training_loss: 8380.07465807\n",
      "Epoch 17/2000 - training_loss: 45814.78535619\n",
      "Epoch 18/2000 - training_loss: 5643.26784312\n",
      "Epoch 19/2000 - training_loss: 4974.00003721\n",
      "Epoch 20/2000 - training_loss: 13283.58474222\n",
      "Epoch 21/2000 - training_loss: 12759.03797674\n",
      "Epoch 22/2000 - training_loss: 2960.86988848\n",
      "Epoch 23/2000 - training_loss: 12953.97478876\n",
      "Epoch 24/2000 - training_loss: 10590.66018461\n",
      "Epoch 25/2000 - training_loss: 3507.79034814\n",
      "Epoch 26/2000 - training_loss: 4385.47211201\n",
      "Epoch 27/2000 - training_loss: 8145.00995920\n",
      "Epoch 28/2000 - training_loss: 10179.97669083\n",
      "Epoch 29/2000 - training_loss: 35694.77213139\n",
      "Epoch 30/2000 - training_loss: 4655.33385261\n",
      "Epoch 31/2000 - training_loss: 7240.02626487\n",
      "Epoch 32/2000 - training_loss: 11631.33272498\n",
      "Epoch 33/2000 - training_loss: 3027.16278793\n",
      "Epoch 34/2000 - training_loss: 3477.51860041\n",
      "Epoch 35/2000 - training_loss: 5863.76858557\n",
      "Epoch 36/2000 - training_loss: 10152.44477661\n",
      "Epoch 37/2000 - training_loss: 7570.75300606\n",
      "Epoch 38/2000 - training_loss: 7013.56520957\n",
      "Epoch 39/2000 - training_loss: 9074.64806219\n",
      "Epoch 40/2000 - training_loss: 3500.55178301\n",
      "Epoch 41/2000 - training_loss: 91411.95046941\n",
      "Epoch 42/2000 - training_loss: 10660.85154557\n",
      "Epoch 43/2000 - training_loss: 7314.59625563\n",
      "Epoch 44/2000 - training_loss: 12316.79508659\n",
      "Epoch 45/2000 - training_loss: 12066.43592080\n",
      "Epoch 46/2000 - training_loss: 13859.94973542\n",
      "Epoch 47/2000 - training_loss: 3346.73266165\n",
      "Epoch 48/2000 - training_loss: 110503.87775914\n",
      "Epoch 49/2000 - training_loss: 36433.13751948\n",
      "Epoch 50/2000 - training_loss: 18115.50823622\n",
      "Epoch 51/2000 - training_loss: 3172.88804004\n",
      "Epoch 52/2000 - training_loss: 4183.21927928\n",
      "Epoch 53/2000 - training_loss: 61456.25960751\n",
      "Epoch 54/2000 - training_loss: 55058.96230454\n",
      "Epoch 55/2000 - training_loss: 6144.66848917\n",
      "Epoch 56/2000 - training_loss: 5084.21510958\n",
      "Epoch 57/2000 - training_loss: 5101.73916244\n",
      "Epoch 58/2000 - training_loss: 65600.97239316\n",
      "Epoch 59/2000 - training_loss: 2385619675.65960026\n",
      "Epoch 60/2000 - training_loss: 9249149.39467919\n",
      "Epoch 61/2000 - training_loss: 5120.67370729\n",
      "Epoch 62/2000 - training_loss: 7016.99060873\n",
      "Epoch 63/2000 - training_loss: 3095.08083998\n",
      "Epoch 64/2000 - training_loss: 19237.81560727\n",
      "Epoch 65/2000 - training_loss: 3389.54978917\n",
      "Epoch 66/2000 - training_loss: 90272.16829708\n",
      "Epoch 67/2000 - training_loss: 6160.44697598\n",
      "Epoch 68/2000 - training_loss: 7567.85225565\n",
      "Epoch 69/2000 - training_loss: 8806.94825956\n",
      "Epoch 70/2000 - training_loss: 3953.25461791\n",
      "Epoch 71/2000 - training_loss: 3058.47071116\n",
      "Epoch 72/2000 - training_loss: 7795.60340886\n",
      "Epoch 73/2000 - training_loss: 36915.33966461\n",
      "Epoch 74/2000 - training_loss: 25824.76036930\n",
      "Epoch 75/2000 - training_loss: 10455.56832820\n",
      "Epoch 76/2000 - training_loss: 3383.57531099\n",
      "Epoch 77/2000 - training_loss: 8051.99203225\n",
      "Epoch 78/2000 - training_loss: 298627.39621297\n",
      "Epoch 79/2000 - training_loss: 75345.08505858\n",
      "Epoch 80/2000 - training_loss: 67465.13479350\n",
      "Epoch 81/2000 - training_loss: 1679026.57914642\n",
      "Epoch 82/2000 - training_loss: 25015.33633544\n",
      "Epoch 83/2000 - training_loss: 5690.84229031\n",
      "Epoch 84/2000 - training_loss: 3665.61187086\n",
      "Epoch 85/2000 - training_loss: 92178.01956288\n",
      "Epoch 86/2000 - training_loss: 5054.34337843\n",
      "Epoch 87/2000 - training_loss: 10201.01613657\n",
      "Epoch 88/2000 - training_loss: 16019.48845985\n",
      "Epoch 89/2000 - training_loss: 25575.72746824\n",
      "Epoch 90/2000 - training_loss: 15674.14572250\n",
      "Epoch 91/2000 - training_loss: 12136.79276662\n",
      "Epoch 92/2000 - training_loss: 6417.95541631\n",
      "Epoch 93/2000 - training_loss: 15047.91843585\n",
      "Epoch 94/2000 - training_loss: 20709.08400490\n",
      "Epoch 95/2000 - training_loss: 12669.62323329\n",
      "Epoch 96/2000 - training_loss: 10533.40444072\n",
      "Epoch 97/2000 - training_loss: 4734.42153630\n",
      "Epoch 98/2000 - training_loss: 407167.49912430\n",
      "Epoch 99/2000 - training_loss: 3125547.68357444\n",
      "Epoch 100/2000 - training_loss: 5438.43999640\n",
      "Epoch 101/2000 - training_loss: 3728.64468394\n",
      "Epoch 102/2000 - training_loss: 9792.98269410\n",
      "Epoch 103/2000 - training_loss: 90714.45023040\n",
      "Epoch 104/2000 - training_loss: 24113.08726430\n",
      "Epoch 105/2000 - training_loss: 4965.53804915\n",
      "Epoch 106/2000 - training_loss: 3402.14669147\n",
      "Epoch 107/2000 - training_loss: 4047.51755183\n",
      "Epoch 108/2000 - training_loss: 8214.29972222\n",
      "Epoch 109/2000 - training_loss: 48346.94036203\n",
      "Epoch 110/2000 - training_loss: 184508.03733169\n",
      "Epoch 111/2000 - training_loss: 13887.03129977\n",
      "Epoch 112/2000 - training_loss: 5747.40377216\n",
      "Epoch 113/2000 - training_loss: 3862.41776625\n",
      "Epoch 114/2000 - training_loss: 3404.48058899\n",
      "Epoch 115/2000 - training_loss: 3674.99340473\n",
      "Epoch 116/2000 - training_loss: 4792.49888321\n",
      "Epoch 117/2000 - training_loss: 7818.57810196\n",
      "Epoch 118/2000 - training_loss: 18356.88417216\n",
      "Epoch 119/2000 - training_loss: 122484.72978640\n",
      "Epoch 120/2000 - training_loss: 259887.42689378\n",
      "Epoch 121/2000 - training_loss: 126880.39998167\n",
      "Epoch 122/2000 - training_loss: 169896.59302271\n",
      "Epoch 123/2000 - training_loss: 22052.33886208\n",
      "Epoch 124/2000 - training_loss: 10141.98264015\n",
      "Epoch 125/2000 - training_loss: 6558.06784400\n",
      "Epoch 126/2000 - training_loss: 5004.48270775\n",
      "Epoch 127/2000 - training_loss: 4228.66063780\n",
      "Epoch 128/2000 - training_loss: 3839.88151704\n",
      "Epoch 129/2000 - training_loss: 3684.43206665\n",
      "Epoch 130/2000 - training_loss: 3697.84170434\n",
      "Epoch 131/2000 - training_loss: 3857.08405482\n",
      "Epoch 132/2000 - training_loss: 4164.91791350\n",
      "Epoch 133/2000 - training_loss: 4645.95647805\n",
      "Epoch 134/2000 - training_loss: 5350.55553074\n",
      "Epoch 135/2000 - training_loss: 6367.68982174\n",
      "Epoch 136/2000 - training_loss: 7850.14667332\n",
      "Epoch 137/2000 - training_loss: 10072.90688635\n",
      "Epoch 138/2000 - training_loss: 13561.90792393\n",
      "Epoch 139/2000 - training_loss: 19388.23891396\n",
      "Epoch 140/2000 - training_loss: 30132.09612903\n",
      "Epoch 141/2000 - training_loss: 52902.08993620\n",
      "Epoch 142/2000 - training_loss: 114645.52080787\n",
      "Epoch 143/2000 - training_loss: 391699.62062607\n",
      "Epoch 144/2000 - training_loss: 11347760.70490158\n",
      "Epoch 145/2000 - training_loss: 14075920.52171286\n",
      "Epoch 146/2000 - training_loss: 267839.22981950\n",
      "Epoch 147/2000 - training_loss: 82518.90029329\n",
      "Epoch 148/2000 - training_loss: 41656.73755321\n",
      "Epoch 149/2000 - training_loss: 26120.08074023\n",
      "Epoch 150/2000 - training_loss: 18487.03836184\n",
      "Epoch 151/2000 - training_loss: 14150.80497562\n",
      "Epoch 152/2000 - training_loss: 11434.58826254\n",
      "Epoch 153/2000 - training_loss: 9610.91432901\n",
      "Epoch 154/2000 - training_loss: 8325.79387776\n",
      "Epoch 155/2000 - training_loss: 7383.80953727\n",
      "Epoch 156/2000 - training_loss: 6672.97397138\n",
      "Epoch 157/2000 - training_loss: 6122.81155801\n",
      "Epoch 158/2000 - training_loss: 5689.56410389\n",
      "Epoch 159/2000 - training_loss: 5342.15817036\n",
      "Epoch 160/2000 - training_loss: 5060.65327455\n",
      "Epoch 161/2000 - training_loss: 4829.78452536\n",
      "Epoch 162/2000 - training_loss: 4639.38083174\n",
      "Epoch 163/2000 - training_loss: 4480.21827649\n",
      "Epoch 164/2000 - training_loss: 4347.73127912\n",
      "Epoch 165/2000 - training_loss: 4235.59603583\n",
      "Epoch 166/2000 - training_loss: 4141.54226533\n",
      "Epoch 167/2000 - training_loss: 4062.17187727\n",
      "Epoch 168/2000 - training_loss: 3994.51948268\n",
      "Epoch 169/2000 - training_loss: 3937.50699555\n",
      "Epoch 170/2000 - training_loss: 3889.26107704\n",
      "Epoch 171/2000 - training_loss: 3848.19273510\n",
      "Epoch 172/2000 - training_loss: 3813.87063306\n",
      "Epoch 173/2000 - training_loss: 3784.96383127\n",
      "Epoch 174/2000 - training_loss: 3760.43411038\n",
      "Epoch 175/2000 - training_loss: 3740.34401936\n",
      "Epoch 176/2000 - training_loss: 3723.65533352\n",
      "Epoch 177/2000 - training_loss: 3710.04560624\n",
      "Epoch 178/2000 - training_loss: 3699.08120969\n",
      "Epoch 179/2000 - training_loss: 3690.36866886\n",
      "Epoch 180/2000 - training_loss: 3683.63347515\n",
      "Epoch 181/2000 - training_loss: 3678.63026342\n",
      "Epoch 182/2000 - training_loss: 3675.22852196\n",
      "Epoch 183/2000 - training_loss: 3672.93318048\n",
      "Epoch 184/2000 - training_loss: 3671.75472770\n",
      "Epoch 185/2000 - training_loss: 3671.56537282\n",
      "Epoch 186/2000 - training_loss: 3672.11380711\n",
      "Epoch 187/2000 - training_loss: 3673.31315096\n",
      "Epoch 188/2000 - training_loss: 3675.25249197\n",
      "Epoch 189/2000 - training_loss: 3677.57236067\n",
      "Epoch 190/2000 - training_loss: 3680.22156962\n",
      "Epoch 191/2000 - training_loss: 3683.34017681\n",
      "Epoch 192/2000 - training_loss: 3686.53926627\n",
      "Epoch 193/2000 - training_loss: 3690.19610602\n",
      "Epoch 194/2000 - training_loss: 3693.77998281\n",
      "Epoch 195/2000 - training_loss: 3697.57987923\n",
      "Epoch 196/2000 - training_loss: 3701.50517700\n",
      "Epoch 197/2000 - training_loss: 3705.41578484\n",
      "Epoch 198/2000 - training_loss: 3709.54698377\n",
      "Epoch 199/2000 - training_loss: 3713.36934263\n",
      "Epoch 200/2000 - training_loss: 3717.37640111\n",
      "Epoch 201/2000 - training_loss: 3721.24359672\n",
      "Epoch 202/2000 - training_loss: 3725.11610647\n",
      "Epoch 203/2000 - training_loss: 3728.94631181\n",
      "Epoch 204/2000 - training_loss: 3732.68949121\n",
      "Epoch 205/2000 - training_loss: 3736.41719341\n",
      "Epoch 206/2000 - training_loss: 3739.98112626\n",
      "Epoch 207/2000 - training_loss: 3743.58626581\n",
      "Epoch 208/2000 - training_loss: 3746.90870091\n",
      "Epoch 209/2000 - training_loss: 3750.10999514\n",
      "Epoch 210/2000 - training_loss: 3753.41325655\n",
      "Epoch 211/2000 - training_loss: 3756.37645415\n",
      "Epoch 212/2000 - training_loss: 3759.26346208\n",
      "Epoch 213/2000 - training_loss: 3762.32818771\n",
      "Epoch 214/2000 - training_loss: 3764.96022982\n",
      "Epoch 215/2000 - training_loss: 3767.73386240\n",
      "Epoch 216/2000 - training_loss: 3770.20297999\n",
      "Epoch 217/2000 - training_loss: 3772.73529459\n",
      "Epoch 218/2000 - training_loss: 3775.04286829\n",
      "Epoch 219/2000 - training_loss: 3777.25364993\n",
      "Epoch 220/2000 - training_loss: 3779.44879532\n",
      "Epoch 221/2000 - training_loss: 3781.46172836\n",
      "Epoch 222/2000 - training_loss: 3783.44308586\n",
      "Epoch 223/2000 - training_loss: 3785.27216199\n",
      "Epoch 224/2000 - training_loss: 3787.17109484\n",
      "Epoch 225/2000 - training_loss: 3788.89539418\n",
      "Epoch 226/2000 - training_loss: 3790.57849762\n",
      "Epoch 227/2000 - training_loss: 3792.01441730\n",
      "Epoch 228/2000 - training_loss: 3793.54989386\n",
      "Epoch 229/2000 - training_loss: 3795.10753037\n",
      "Epoch 230/2000 - training_loss: 3796.28181777\n",
      "Epoch 231/2000 - training_loss: 3797.56422857\n",
      "Epoch 232/2000 - training_loss: 3798.83774728\n",
      "Epoch 233/2000 - training_loss: 3799.93248020\n",
      "Epoch 234/2000 - training_loss: 3801.06772614\n",
      "Epoch 235/2000 - training_loss: 3801.87765394\n",
      "Epoch 236/2000 - training_loss: 3802.99897737\n",
      "Epoch 237/2000 - training_loss: 3803.89917732\n",
      "Epoch 238/2000 - training_loss: 3804.70443455\n",
      "Epoch 239/2000 - training_loss: 3805.59780480\n",
      "Epoch 240/2000 - training_loss: 3806.23272856\n",
      "Epoch 241/2000 - training_loss: 3806.83314584\n",
      "Epoch 242/2000 - training_loss: 3807.57009322\n",
      "Epoch 243/2000 - training_loss: 3808.21211901\n",
      "Epoch 244/2000 - training_loss: 3808.54996849\n",
      "Epoch 245/2000 - training_loss: 3809.07144450\n",
      "Epoch 246/2000 - training_loss: 3809.63705732\n",
      "Epoch 247/2000 - training_loss: 3810.06699552\n",
      "Epoch 248/2000 - training_loss: 3810.48131212\n",
      "Epoch 249/2000 - training_loss: 3810.63017133\n",
      "Epoch 250/2000 - training_loss: 3810.99276517\n",
      "Epoch 251/2000 - training_loss: 3811.26810895\n",
      "Epoch 252/2000 - training_loss: 3811.45452965\n",
      "Epoch 253/2000 - training_loss: 3811.70636889\n",
      "Epoch 254/2000 - training_loss: 3811.83434040\n",
      "Epoch 255/2000 - training_loss: 3811.93719736\n",
      "Epoch 256/2000 - training_loss: 3811.94943031\n",
      "Epoch 257/2000 - training_loss: 3812.02826070\n",
      "Epoch 258/2000 - training_loss: 3812.02826070\n",
      "Epoch 259/2000 - training_loss: 3811.93719736\n",
      "Epoch 260/2000 - training_loss: 3811.83201173\n",
      "Epoch 261/2000 - training_loss: 3811.72190515\n",
      "Epoch 262/2000 - training_loss: 3811.54854382\n",
      "Epoch 263/2000 - training_loss: 3811.28476060\n",
      "Epoch 264/2000 - training_loss: 3811.00798644\n",
      "Epoch 265/2000 - training_loss: 3810.80169061\n",
      "Epoch 266/2000 - training_loss: 3810.64252479\n",
      "Epoch 267/2000 - training_loss: 3810.28691464\n",
      "Epoch 268/2000 - training_loss: 3809.71803345\n",
      "Epoch 269/2000 - training_loss: 3809.26315715\n",
      "Epoch 270/2000 - training_loss: 3808.88418400\n",
      "Epoch 271/2000 - training_loss: 3808.38804616\n",
      "Epoch 272/2000 - training_loss: 3807.83540043\n",
      "Epoch 273/2000 - training_loss: 3807.11248135\n",
      "Epoch 274/2000 - training_loss: 3806.38071034\n",
      "Epoch 275/2000 - training_loss: 3805.84618607\n",
      "Epoch 276/2000 - training_loss: 3805.05169682\n",
      "Epoch 277/2000 - training_loss: 3804.06689862\n",
      "Epoch 278/2000 - training_loss: 3803.43647497\n",
      "Epoch 279/2000 - training_loss: 3802.37441508\n",
      "Epoch 280/2000 - training_loss: 3801.33814894\n",
      "Epoch 281/2000 - training_loss: 3800.43692964\n",
      "Epoch 282/2000 - training_loss: 3799.25123172\n",
      "Epoch 283/2000 - training_loss: 3798.14046044\n",
      "Epoch 284/2000 - training_loss: 3797.07892909\n",
      "Epoch 285/2000 - training_loss: 3795.66471010\n",
      "Epoch 286/2000 - training_loss: 3794.38624455\n",
      "Epoch 287/2000 - training_loss: 3793.04838179\n",
      "Epoch 288/2000 - training_loss: 3791.40737388\n",
      "Epoch 289/2000 - training_loss: 3790.00628329\n",
      "Epoch 290/2000 - training_loss: 3788.31497948\n",
      "Epoch 291/2000 - training_loss: 3786.65305968\n",
      "Epoch 292/2000 - training_loss: 3784.86269068\n",
      "Epoch 293/2000 - training_loss: 3783.06446194\n",
      "Epoch 294/2000 - training_loss: 3781.15438511\n",
      "Epoch 295/2000 - training_loss: 3779.14807938\n",
      "Epoch 296/2000 - training_loss: 3777.18933618\n",
      "Epoch 297/2000 - training_loss: 3774.88172701\n",
      "Epoch 298/2000 - training_loss: 3772.73529459\n",
      "Epoch 299/2000 - training_loss: 3770.36640725\n",
      "Epoch 300/2000 - training_loss: 3767.91174956\n",
      "Epoch 301/2000 - training_loss: 3765.55666756\n",
      "Epoch 302/2000 - training_loss: 3762.89618121\n",
      "Epoch 303/2000 - training_loss: 3760.42561572\n",
      "Epoch 304/2000 - training_loss: 3757.72286722\n",
      "Epoch 305/2000 - training_loss: 3755.00319291\n",
      "Epoch 306/2000 - training_loss: 3752.07616990\n",
      "Epoch 307/2000 - training_loss: 3749.24851069\n",
      "Epoch 308/2000 - training_loss: 3746.31273568\n",
      "Epoch 309/2000 - training_loss: 3743.27802415\n",
      "Epoch 310/2000 - training_loss: 3740.21668447\n",
      "Epoch 311/2000 - training_loss: 3737.08999992\n",
      "Epoch 312/2000 - training_loss: 3733.93887877\n",
      "Epoch 313/2000 - training_loss: 3730.75449181\n",
      "Epoch 314/2000 - training_loss: 3727.54249437\n",
      "Epoch 315/2000 - training_loss: 3724.30449104\n",
      "Epoch 316/2000 - training_loss: 3721.02065026\n",
      "Epoch 317/2000 - training_loss: 3717.86118128\n",
      "Epoch 318/2000 - training_loss: 3714.56147896\n",
      "Epoch 319/2000 - training_loss: 3711.42038320\n",
      "Epoch 320/2000 - training_loss: 3708.33220736\n",
      "Epoch 321/2000 - training_loss: 3705.20338375\n",
      "Epoch 322/2000 - training_loss: 3702.17058466\n",
      "Epoch 323/2000 - training_loss: 3699.13306605\n",
      "Epoch 324/2000 - training_loss: 3696.30771204\n",
      "Epoch 325/2000 - training_loss: 3693.58397099\n",
      "Epoch 326/2000 - training_loss: 3690.96120494\n",
      "Epoch 327/2000 - training_loss: 3688.36593601\n",
      "Epoch 328/2000 - training_loss: 3685.98387403\n",
      "Epoch 329/2000 - training_loss: 3683.79071301\n",
      "Epoch 330/2000 - training_loss: 3681.73646772\n",
      "Epoch 331/2000 - training_loss: 3679.93275874\n",
      "Epoch 332/2000 - training_loss: 3678.16856733\n",
      "Epoch 333/2000 - training_loss: 3676.68392046\n",
      "Epoch 334/2000 - training_loss: 3675.36771401\n",
      "Epoch 335/2000 - training_loss: 3674.28047978\n",
      "Epoch 336/2000 - training_loss: 3673.31315096\n",
      "Epoch 337/2000 - training_loss: 3672.69992441\n",
      "Epoch 338/2000 - training_loss: 3672.11881550\n",
      "Epoch 339/2000 - training_loss: 3671.79460361\n",
      "Epoch 340/2000 - training_loss: 3671.53275565\n",
      "Epoch 341/2000 - training_loss: 3671.43936175\n",
      "Epoch 342/2000 - training_loss: 3671.66399260\n",
      "Epoch 343/2000 - training_loss: 3671.75723023\n",
      "Epoch 344/2000 - training_loss: 3672.05626288\n",
      "Epoch 345/2000 - training_loss: 3672.36997212\n",
      "Epoch 346/2000 - training_loss: 3672.70340276\n",
      "Epoch 347/2000 - training_loss: 3672.99687181\n",
      "Epoch 348/2000 - training_loss: 3673.36019924\n",
      "Epoch 349/2000 - training_loss: 3673.65701947\n",
      "Epoch 350/2000 - training_loss: 3673.79804510\n",
      "Epoch 351/2000 - training_loss: 3673.96226261\n",
      "Epoch 352/2000 - training_loss: 3674.03285198\n",
      "Epoch 353/2000 - training_loss: 3674.00358698\n",
      "Epoch 354/2000 - training_loss: 3673.91599491\n",
      "Epoch 355/2000 - training_loss: 3673.65356116\n",
      "Epoch 356/2000 - training_loss: 3673.40992378\n",
      "Epoch 357/2000 - training_loss: 3673.07691936\n",
      "Epoch 358/2000 - training_loss: 3672.65639795\n",
      "Epoch 359/2000 - training_loss: 3672.38781721\n",
      "Epoch 360/2000 - training_loss: 3672.15251030\n",
      "Epoch 361/2000 - training_loss: 3671.84487627\n",
      "Epoch 362/2000 - training_loss: 3671.64482751\n",
      "Epoch 363/2000 - training_loss: 3671.60024833\n",
      "Epoch 364/2000 - training_loss: 3671.53254094\n",
      "Epoch 365/2000 - training_loss: 3671.52982321\n",
      "Epoch 366/2000 - training_loss: 3671.59507814\n",
      "Epoch 367/2000 - training_loss: 3671.66197997\n",
      "Epoch 368/2000 - training_loss: 3671.88655756\n",
      "Epoch 369/2000 - training_loss: 3672.00427238\n",
      "Epoch 370/2000 - training_loss: 3672.10946885\n",
      "Epoch 371/2000 - training_loss: 3672.03886765\n",
      "Epoch 372/2000 - training_loss: 3672.13095380\n",
      "Epoch 373/2000 - training_loss: 3672.01418782\n",
      "Epoch 374/2000 - training_loss: 3671.89379761\n",
      "Epoch 375/2000 - training_loss: 3671.75407613\n",
      "Epoch 376/2000 - training_loss: 3671.58558374\n",
      "Epoch 377/2000 - training_loss: 3671.56377739\n",
      "Epoch 378/2000 - training_loss: 3671.39536544\n",
      "Epoch 379/2000 - training_loss: 3671.53569550\n",
      "Epoch 380/2000 - training_loss: 3671.55338942\n",
      "Epoch 381/2000 - training_loss: 3671.65301906\n",
      "Epoch 382/2000 - training_loss: 3671.73421003\n",
      "Epoch 383/2000 - training_loss: 3671.72582695\n",
      "Epoch 384/2000 - training_loss: 3671.73764405\n",
      "Epoch 385/2000 - training_loss: 3671.67795231\n",
      "Epoch 386/2000 - training_loss: 3671.61210925\n",
      "Epoch 387/2000 - training_loss: 3671.60024833\n",
      "Epoch 388/2000 - training_loss: 3671.44687744\n",
      "Epoch 389/2000 - training_loss: 3671.47659389\n",
      "Epoch 390/2000 - training_loss: 3671.46746383\n",
      "Epoch 391/2000 - training_loss: 3671.63317883\n",
      "Epoch 392/2000 - training_loss: 3671.57422208\n",
      "Epoch 393/2000 - training_loss: 3671.58617435\n",
      "Epoch 394/2000 - training_loss: 3671.54413122\n",
      "Epoch 395/2000 - training_loss: 3671.52982321\n",
      "Epoch 396/2000 - training_loss: 3671.45273091\n",
      "Epoch 397/2000 - training_loss: 3671.42624142\n",
      "Epoch 398/2000 - training_loss: 3671.61410566\n",
      "Epoch 399/2000 - training_loss: 3671.51064805\n",
      "Epoch 400/2000 - training_loss: 3671.52467796\n",
      "Epoch 401/2000 - training_loss: 3671.51684904\n",
      "Epoch 402/2000 - training_loss: 3671.53671034\n",
      "Epoch 403/2000 - training_loss: 3671.54315492\n",
      "Epoch 404/2000 - training_loss: 3671.46115715\n",
      "Epoch 405/2000 - training_loss: 3671.45864969\n",
      "Epoch 406/2000 - training_loss: 3671.46746383\n",
      "Epoch 407/2000 - training_loss: 3671.49140062\n",
      "Epoch 408/2000 - training_loss: 3671.43461788\n",
      "Epoch 409/2000 - training_loss: 3671.52522624\n",
      "Epoch 410/2000 - training_loss: 3671.53671034\n",
      "Epoch 411/2000 - training_loss: 3671.52120251\n",
      "Epoch 412/2000 - training_loss: 3671.47221159\n",
      "Epoch 413/2000 - training_loss: 3671.51051658\n",
      "Epoch 414/2000 - training_loss: 3671.49980272\n",
      "Epoch 415/2000 - training_loss: 3671.48785911\n",
      "Epoch 416/2000 - training_loss: 3671.49592123\n",
      "Epoch 417/2000 - training_loss: 3671.50708954\n",
      "Epoch 418/2000 - training_loss: 3671.46556067\n",
      "Epoch 419/2000 - training_loss: 3671.54569855\n",
      "Epoch 420/2000 - training_loss: 3671.53671034\n",
      "Epoch 421/2000 - training_loss: 3671.42624142\n",
      "Epoch 422/2000 - training_loss: 3671.42475624\n",
      "Epoch 423/2000 - training_loss: 3671.49725208\n",
      "Epoch 424/2000 - training_loss: 3671.55704132\n",
      "Epoch 425/2000 - training_loss: 3671.45743258\n",
      "Epoch 426/2000 - training_loss: 3671.56982691\n",
      "Epoch 427/2000 - training_loss: 3671.48001631\n",
      "Epoch 428/2000 - training_loss: 3671.55416838\n",
      "Epoch 429/2000 - training_loss: 3671.50945719\n",
      "Epoch 430/2000 - training_loss: 3671.51538731\n",
      "Epoch 431/2000 - training_loss: 3671.57893500\n",
      "Epoch 432/2000 - training_loss: 3671.59187368\n",
      "Epoch 433/2000 - training_loss: 3671.59187368\n",
      "Epoch 434/2000 - training_loss: 3671.56808704\n",
      "Epoch 435/2000 - training_loss: 3671.47659389\n",
      "Epoch 436/2000 - training_loss: 3671.37049757\n",
      "Epoch 437/2000 - training_loss: 3671.59187368\n",
      "Epoch 438/2000 - training_loss: 3671.52309879\n",
      "Epoch 439/2000 - training_loss: 3671.38998224\n",
      "Epoch 440/2000 - training_loss: 3671.41220295\n",
      "Epoch 441/2000 - training_loss: 3671.45173986\n",
      "Epoch 442/2000 - training_loss: 3671.48924093\n",
      "Epoch 443/2000 - training_loss: 3671.50465490\n",
      "Epoch 444/2000 - training_loss: 3671.50465490\n",
      "Epoch 445/2000 - training_loss: 3671.52309879\n",
      "Epoch 446/2000 - training_loss: 3671.44670926\n",
      "Epoch 447/2000 - training_loss: 3671.53233290\n",
      "Epoch 448/2000 - training_loss: 3671.56052705\n",
      "Epoch 449/2000 - training_loss: 3671.44340929\n",
      "Epoch 450/2000 - training_loss: 3671.50465490\n",
      "Epoch 451/2000 - training_loss: 3671.57647872\n",
      "Epoch 452/2000 - training_loss: 3671.44340929\n",
      "Epoch 453/2000 - training_loss: 3671.49041021\n",
      "Epoch 454/2000 - training_loss: 3671.51195724\n",
      "Epoch 455/2000 - training_loss: 3671.49980272\n",
      "Epoch 456/2000 - training_loss: 3671.43936175\n",
      "Epoch 457/2000 - training_loss: 3671.53254094\n",
      "Epoch 458/2000 - training_loss: 3671.42475624\n",
      "Epoch 459/2000 - training_loss: 3671.49610743\n",
      "Epoch 460/2000 - training_loss: 3671.50928957\n",
      "Epoch 461/2000 - training_loss: 3671.91219824\n",
      "Epoch 462/2000 - training_loss: 3675.53160535\n",
      "Epoch 463/2000 - training_loss: 3714.55715769\n",
      "Epoch 464/2000 - training_loss: 4221.17227808\n",
      "Epoch 465/2000 - training_loss: 1623070.00823836\n",
      "Epoch 466/2000 - training_loss: 8382.26286255\n",
      "Epoch 467/2000 - training_loss: 3698.60379479\n",
      "Epoch 468/2000 - training_loss: 19819.19015440\n",
      "Epoch 469/2000 - training_loss: 5297.96328325\n",
      "Epoch 470/2000 - training_loss: 3449.90966547\n",
      "Epoch 471/2000 - training_loss: 4002.89170048\n",
      "Epoch 472/2000 - training_loss: 31639.36958674\n",
      "Epoch 473/2000 - training_loss: 7285.40159557\n",
      "Epoch 474/2000 - training_loss: 3514.01165607\n",
      "Epoch 475/2000 - training_loss: 3513.22525349\n",
      "Epoch 476/2000 - training_loss: 12834.93233273\n",
      "Epoch 477/2000 - training_loss: 5311.53042451\n",
      "Epoch 478/2000 - training_loss: 3332.79764069\n",
      "Epoch 479/2000 - training_loss: 14469.88392493\n",
      "Epoch 480/2000 - training_loss: 31415.63055317\n",
      "Epoch 481/2000 - training_loss: 35479.57239169\n",
      "Epoch 482/2000 - training_loss: 4958.78377655\n",
      "Epoch 483/2000 - training_loss: 6215.85587717\n",
      "Epoch 484/2000 - training_loss: 11853.75345272\n",
      "Epoch 485/2000 - training_loss: 15274.95247393\n",
      "Epoch 486/2000 - training_loss: 10984.99261128\n",
      "Epoch 487/2000 - training_loss: 9116.31226070\n",
      "Epoch 488/2000 - training_loss: 2979.31970456\n",
      "Epoch 489/2000 - training_loss: 5474.71675484\n",
      "Epoch 490/2000 - training_loss: 553670.43600608\n",
      "Epoch 491/2000 - training_loss: 3512.74004168\n",
      "Epoch 492/2000 - training_loss: 86859.15733782\n",
      "Epoch 493/2000 - training_loss: 5001.03021932\n",
      "Epoch 494/2000 - training_loss: 4247.03833050\n",
      "Epoch 495/2000 - training_loss: 8354.13438164\n",
      "Epoch 496/2000 - training_loss: 3255.39516976\n",
      "Epoch 497/2000 - training_loss: 5405.28385536\n",
      "Epoch 498/2000 - training_loss: 6392.76165152\n",
      "Epoch 499/2000 - training_loss: 9720.27854597\n",
      "Epoch 500/2000 - training_loss: 5273.03110869\n",
      "Epoch 501/2000 - training_loss: 8639.07362667\n",
      "Epoch 502/2000 - training_loss: 18217.27598579\n",
      "Epoch 503/2000 - training_loss: 203778.84215223\n",
      "Epoch 504/2000 - training_loss: 4103.00423712\n",
      "Epoch 505/2000 - training_loss: 14426.87635824\n",
      "Epoch 506/2000 - training_loss: 5004.29570545\n",
      "Epoch 507/2000 - training_loss: 17668.89617822\n",
      "Epoch 508/2000 - training_loss: 535231.75186479\n",
      "Epoch 509/2000 - training_loss: 24099.44642824\n",
      "Epoch 510/2000 - training_loss: 5967.98943347\n",
      "Epoch 511/2000 - training_loss: 3905.95887422\n",
      "Epoch 512/2000 - training_loss: 3381.77615392\n",
      "Epoch 513/2000 - training_loss: 3818.51043957\n",
      "Epoch 514/2000 - training_loss: 13632.62924752\n",
      "Epoch 515/2000 - training_loss: 11740.38542478\n",
      "Epoch 516/2000 - training_loss: 26719.78834230\n",
      "Epoch 517/2000 - training_loss: 83331.36420404\n",
      "Epoch 518/2000 - training_loss: 11743.06438618\n",
      "Epoch 519/2000 - training_loss: 17031.12905224\n",
      "Epoch 520/2000 - training_loss: 11837.70265872\n",
      "Epoch 521/2000 - training_loss: 6913.16850561\n",
      "Epoch 522/2000 - training_loss: 7054.61066930\n",
      "Epoch 523/2000 - training_loss: 8297.84264372\n",
      "Epoch 524/2000 - training_loss: 4516.19896330\n",
      "Epoch 525/2000 - training_loss: 234441.02966072\n",
      "Epoch 526/2000 - training_loss: 13430.12351034\n",
      "Epoch 527/2000 - training_loss: 4155.63080351\n",
      "Epoch 528/2000 - training_loss: 3208.09602749\n",
      "Epoch 529/2000 - training_loss: 4488.28362007\n",
      "Epoch 530/2000 - training_loss: 22759.69816024\n",
      "Epoch 531/2000 - training_loss: 11385.91622333\n",
      "Epoch 532/2000 - training_loss: 140814.14243987\n",
      "Epoch 533/2000 - training_loss: 5321.89624020\n",
      "Epoch 534/2000 - training_loss: 18345.97315607\n",
      "Epoch 535/2000 - training_loss: 14033.94805207\n",
      "Epoch 536/2000 - training_loss: 4179.38396169\n",
      "Epoch 537/2000 - training_loss: 8117.08341699\n",
      "Epoch 538/2000 - training_loss: 3188.19285748\n",
      "Epoch 539/2000 - training_loss: 14387.52554052\n",
      "Epoch 540/2000 - training_loss: 426017137.58734804\n",
      "Epoch 541/2000 - training_loss: 12662.17524338\n",
      "Epoch 542/2000 - training_loss: 3398.82573568\n",
      "Epoch 543/2000 - training_loss: 246919.21144534\n",
      "Epoch 544/2000 - training_loss: 28790.36385559\n",
      "Epoch 545/2000 - training_loss: 3810.53133397\n",
      "Epoch 546/2000 - training_loss: 57618.97542432\n",
      "Epoch 547/2000 - training_loss: 23570.69738074\n",
      "Epoch 548/2000 - training_loss: 3467.32626786\n",
      "Epoch 549/2000 - training_loss: 75043.74142490\n",
      "Epoch 550/2000 - training_loss: 26909.09281887\n",
      "Epoch 551/2000 - training_loss: 2747.64744202\n",
      "Epoch 552/2000 - training_loss: 15153.14572639\n",
      "Epoch 553/2000 - training_loss: 9508.55819072\n",
      "Epoch 554/2000 - training_loss: 5642.27448889\n",
      "Epoch 555/2000 - training_loss: 8931.74582338\n",
      "Epoch 556/2000 - training_loss: 6423.11885493\n",
      "Epoch 557/2000 - training_loss: 342777.98477522\n",
      "Epoch 558/2000 - training_loss: 3239.51698538\n",
      "Epoch 559/2000 - training_loss: 5602.29034029\n",
      "Epoch 560/2000 - training_loss: 19908.43580966\n",
      "Epoch 561/2000 - training_loss: 28067.66643255\n",
      "Epoch 562/2000 - training_loss: 4272.53705282\n",
      "Epoch 563/2000 - training_loss: 3914.13979236\n",
      "Epoch 564/2000 - training_loss: 7292.24311689\n",
      "Epoch 565/2000 - training_loss: 24063.93880364\n",
      "Epoch 566/2000 - training_loss: 84668.81577795\n",
      "Epoch 567/2000 - training_loss: 17743.98108855\n",
      "Epoch 568/2000 - training_loss: 5871.20780640\n",
      "Epoch 569/2000 - training_loss: 3802.79783695\n",
      "Epoch 570/2000 - training_loss: 14138.10465318\n",
      "Epoch 571/2000 - training_loss: 3999.76157709\n",
      "Epoch 572/2000 - training_loss: 190475.66891062\n",
      "Epoch 573/2000 - training_loss: 3314.78784133\n",
      "Epoch 574/2000 - training_loss: 4104.53449010\n",
      "Epoch 575/2000 - training_loss: 17127.88202518\n",
      "Epoch 576/2000 - training_loss: 3491.24968072\n",
      "Epoch 577/2000 - training_loss: 2809200.31259447\n",
      "Epoch 578/2000 - training_loss: 7244.89810039\n",
      "Epoch 579/2000 - training_loss: 31642.03617863\n",
      "Epoch 580/2000 - training_loss: 44342.38040144\n",
      "Epoch 581/2000 - training_loss: 228178.00587959\n",
      "Epoch 582/2000 - training_loss: 2831.54285705\n",
      "Epoch 583/2000 - training_loss: 45976.38351248\n",
      "Epoch 584/2000 - training_loss: 3565.51252427\n",
      "Epoch 585/2000 - training_loss: 3133.75258806\n",
      "Epoch 586/2000 - training_loss: 1925212.17191641\n",
      "Epoch 587/2000 - training_loss: 3081.55976423\n",
      "Epoch 588/2000 - training_loss: 3725.06305557\n",
      "Epoch 589/2000 - training_loss: 32291.93251645\n",
      "Epoch 590/2000 - training_loss: 7176.22031967\n",
      "Epoch 591/2000 - training_loss: 3348.14504862\n",
      "Epoch 592/2000 - training_loss: 212027694.07760000\n",
      "Epoch 593/2000 - training_loss: 2731.94244345\n",
      "Epoch 594/2000 - training_loss: 317204.23058008\n",
      "Epoch 595/2000 - training_loss: 13408.01362726\n",
      "Epoch 596/2000 - training_loss: 2416.72823076\n",
      "Epoch 597/2000 - training_loss: 47956.83409527\n",
      "Epoch 598/2000 - training_loss: 2853.34013716\n",
      "Epoch 599/2000 - training_loss: 20933.76897029\n",
      "Epoch 600/2000 - training_loss: 24249.01767786\n",
      "Epoch 601/2000 - training_loss: 92578.43666445\n",
      "Epoch 602/2000 - training_loss: 113707.17855711\n",
      "Epoch 603/2000 - training_loss: 25829.23746675\n",
      "Epoch 604/2000 - training_loss: 6230774.89956772\n",
      "Epoch 605/2000 - training_loss: 1108910.16566348\n",
      "Epoch 606/2000 - training_loss: 343481.63459093\n",
      "Epoch 607/2000 - training_loss: 3321.63993252\n",
      "Epoch 608/2000 - training_loss: 93080.62966827\n",
      "Epoch 609/2000 - training_loss: 282900.95085993\n",
      "Epoch 610/2000 - training_loss: 2861.05807141\n",
      "Epoch 611/2000 - training_loss: 67336.84112111\n",
      "Epoch 612/2000 - training_loss: 270336.90714663\n",
      "Epoch 613/2000 - training_loss: 8357.75963201\n",
      "Epoch 614/2000 - training_loss: 11675.81545116\n",
      "Epoch 615/2000 - training_loss: 6404832.67930811\n",
      "Epoch 616/2000 - training_loss: 7899.65077662\n",
      "Epoch 617/2000 - training_loss: 6240.67116039\n",
      "Epoch 618/2000 - training_loss: 16308.95963802\n",
      "Epoch 619/2000 - training_loss: 42268.02436319\n",
      "Epoch 620/2000 - training_loss: 4123.78554671\n",
      "Epoch 621/2000 - training_loss: 3299.72160359\n",
      "Epoch 622/2000 - training_loss: 6002.76616458\n",
      "Epoch 623/2000 - training_loss: 438396.96842517\n",
      "Epoch 624/2000 - training_loss: 8280.78503034\n",
      "Epoch 625/2000 - training_loss: 3212.92116060\n",
      "Epoch 626/2000 - training_loss: 18898.09244726\n",
      "Epoch 627/2000 - training_loss: 51705.26472747\n",
      "Epoch 628/2000 - training_loss: 75355.81353290\n",
      "Epoch 629/2000 - training_loss: 8447.47273430\n",
      "Epoch 630/2000 - training_loss: 88689.04737000\n",
      "Epoch 631/2000 - training_loss: 4242.71649199\n",
      "Epoch 632/2000 - training_loss: 22233.06193099\n",
      "Epoch 633/2000 - training_loss: 4708.87529037\n",
      "Epoch 634/2000 - training_loss: 4745.19318325\n",
      "Epoch 635/2000 - training_loss: 17885.42716869\n",
      "Epoch 636/2000 - training_loss: 2924.60782870\n",
      "Epoch 637/2000 - training_loss: 71308.72750783\n",
      "Epoch 638/2000 - training_loss: 12560.57097110\n",
      "Epoch 639/2000 - training_loss: 3615.13132787\n",
      "Epoch 640/2000 - training_loss: 3036496.25660691\n",
      "Epoch 641/2000 - training_loss: 11598.95976414\n",
      "Epoch 642/2000 - training_loss: 404680.30773795\n",
      "Epoch 643/2000 - training_loss: 10220.35400366\n",
      "Epoch 644/2000 - training_loss: 4517.02227871\n",
      "Epoch 645/2000 - training_loss: 5611.32541890\n",
      "Epoch 646/2000 - training_loss: 20035.37850128\n",
      "Epoch 647/2000 - training_loss: 543077.06556573\n",
      "Epoch 648/2000 - training_loss: 10767.09441966\n",
      "Epoch 649/2000 - training_loss: 5918.74125067\n",
      "Epoch 650/2000 - training_loss: 10349.95049444\n",
      "Epoch 651/2000 - training_loss: 283826.11135319\n",
      "Epoch 652/2000 - training_loss: 27618.40083206\n",
      "Epoch 653/2000 - training_loss: 8195.96796794\n",
      "Epoch 654/2000 - training_loss: 7098.29166590\n",
      "Epoch 655/2000 - training_loss: 11837.90532546\n",
      "Epoch 656/2000 - training_loss: 48349.70602433\n",
      "Epoch 657/2000 - training_loss: 2420766.78778393\n",
      "Epoch 658/2000 - training_loss: 4185.89193083\n",
      "Epoch 659/2000 - training_loss: 19061.87584262\n",
      "Epoch 660/2000 - training_loss: 15736.72875428\n",
      "Epoch 661/2000 - training_loss: 15618.48602255\n",
      "Epoch 662/2000 - training_loss: 3396.10737948\n",
      "Epoch 663/2000 - training_loss: 4573.36900233\n",
      "Epoch 664/2000 - training_loss: 112105.89652325\n",
      "Epoch 665/2000 - training_loss: 946996.04540075\n",
      "Epoch 666/2000 - training_loss: 4167.72945908\n",
      "Epoch 667/2000 - training_loss: 3132.08730120\n",
      "Epoch 668/2000 - training_loss: 9979.82750839\n",
      "Epoch 669/2000 - training_loss: 166530.67172766\n",
      "Epoch 670/2000 - training_loss: 5452.88515689\n",
      "Epoch 671/2000 - training_loss: 2998.43383102\n",
      "Epoch 672/2000 - training_loss: 4254.28679028\n",
      "Epoch 673/2000 - training_loss: 27850.59806386\n",
      "Epoch 674/2000 - training_loss: 48470.91285724\n",
      "Epoch 675/2000 - training_loss: 15744.63975571\n",
      "Epoch 676/2000 - training_loss: 4195.09726719\n",
      "Epoch 677/2000 - training_loss: 3361.78910893\n",
      "Epoch 678/2000 - training_loss: 5140.74798949\n",
      "Epoch 679/2000 - training_loss: 23398.76921071\n",
      "Epoch 680/2000 - training_loss: 115036.35457930\n",
      "Epoch 681/2000 - training_loss: 13077.70749897\n",
      "Epoch 682/2000 - training_loss: 316443.68232989\n",
      "Epoch 683/2000 - training_loss: 50638.52560404\n",
      "Epoch 684/2000 - training_loss: 10842.91619443\n",
      "Epoch 685/2000 - training_loss: 5863.83095898\n",
      "Epoch 686/2000 - training_loss: 4491.64199504\n",
      "Epoch 687/2000 - training_loss: 4221.96049410\n",
      "Epoch 688/2000 - training_loss: 4634.93399314\n",
      "Epoch 689/2000 - training_loss: 5880.10912154\n",
      "Epoch 690/2000 - training_loss: 8766.74112666\n",
      "Epoch 691/2000 - training_loss: 16304.97205810\n",
      "Epoch 692/2000 - training_loss: 44462.79532387\n",
      "Epoch 693/2000 - training_loss: 302600.83591696\n",
      "Epoch 694/2000 - training_loss: 66192.04273721\n",
      "Epoch 695/2000 - training_loss: 25658.23719817\n",
      "Epoch 696/2000 - training_loss: 14124.52389033\n",
      "Epoch 697/2000 - training_loss: 9435.25697458\n",
      "Epoch 698/2000 - training_loss: 7111.98091815\n",
      "Epoch 699/2000 - training_loss: 5821.72669578\n",
      "Epoch 700/2000 - training_loss: 5061.36142356\n",
      "Epoch 701/2000 - training_loss: 4607.26904888\n",
      "Epoch 702/2000 - training_loss: 4350.96987496\n",
      "Epoch 703/2000 - training_loss: 4233.77175961\n",
      "Epoch 704/2000 - training_loss: 4224.75456888\n",
      "Epoch 705/2000 - training_loss: 4308.39208450\n",
      "Epoch 706/2000 - training_loss: 4478.25375153\n",
      "Epoch 707/2000 - training_loss: 4734.05462549\n",
      "Epoch 708/2000 - training_loss: 5081.90927924\n",
      "Epoch 709/2000 - training_loss: 5531.30947273\n",
      "Epoch 710/2000 - training_loss: 6096.71628035\n",
      "Epoch 711/2000 - training_loss: 6796.41239642\n",
      "Epoch 712/2000 - training_loss: 7644.90747913\n",
      "Epoch 713/2000 - training_loss: 8660.73491798\n",
      "Epoch 714/2000 - training_loss: 9836.01047277\n",
      "Epoch 715/2000 - training_loss: 11141.30043149\n",
      "Epoch 716/2000 - training_loss: 12485.69661053\n",
      "Epoch 717/2000 - training_loss: 13711.92830403\n",
      "Epoch 718/2000 - training_loss: 14581.95626040\n",
      "Epoch 719/2000 - training_loss: 14876.67591437\n",
      "Epoch 720/2000 - training_loss: 14522.23928011\n",
      "Epoch 721/2000 - training_loss: 13631.81299109\n",
      "Epoch 722/2000 - training_loss: 12403.09740964\n",
      "Epoch 723/2000 - training_loss: 11086.37886314\n",
      "Epoch 724/2000 - training_loss: 9836.01047277\n",
      "Epoch 725/2000 - training_loss: 8730.51914490\n",
      "Epoch 726/2000 - training_loss: 7801.67623606\n",
      "Epoch 727/2000 - training_loss: 7033.19187432\n",
      "Epoch 728/2000 - training_loss: 6406.28855104\n",
      "Epoch 729/2000 - training_loss: 5902.87638353\n",
      "Epoch 730/2000 - training_loss: 5495.66034526\n",
      "Epoch 731/2000 - training_loss: 5171.25085554\n",
      "Epoch 732/2000 - training_loss: 4912.53979213\n",
      "Epoch 733/2000 - training_loss: 4707.71978477\n",
      "Epoch 734/2000 - training_loss: 4548.99978518\n",
      "Epoch 735/2000 - training_loss: 4427.78788016\n",
      "Epoch 736/2000 - training_loss: 4338.68287520\n",
      "Epoch 737/2000 - training_loss: 4276.68008077\n",
      "Epoch 738/2000 - training_loss: 4237.95045449\n",
      "Epoch 739/2000 - training_loss: 4219.31146323\n",
      "Epoch 740/2000 - training_loss: 4217.72998153\n",
      "Epoch 741/2000 - training_loss: 4230.90931508\n",
      "Epoch 742/2000 - training_loss: 4256.07742560\n",
      "Epoch 743/2000 - training_loss: 4291.04643051\n",
      "Epoch 744/2000 - training_loss: 4333.86381023\n",
      "Epoch 745/2000 - training_loss: 4381.27408883\n",
      "Epoch 746/2000 - training_loss: 4431.52985494\n",
      "Epoch 747/2000 - training_loss: 4481.60226489\n",
      "Epoch 748/2000 - training_loss: 4529.69567697\n",
      "Epoch 749/2000 - training_loss: 4572.67201406\n",
      "Epoch 750/2000 - training_loss: 4607.79056196\n",
      "Epoch 751/2000 - training_loss: 4632.85133294\n",
      "Epoch 752/2000 - training_loss: 4648.04815251\n",
      "Epoch 753/2000 - training_loss: 4650.54383846\n",
      "Epoch 754/2000 - training_loss: 4641.36317867\n",
      "Epoch 755/2000 - training_loss: 4620.22485055\n",
      "Epoch 756/2000 - training_loss: 4588.37803024\n",
      "Epoch 757/2000 - training_loss: 4549.17765508\n",
      "Epoch 758/2000 - training_loss: 4503.96654757\n",
      "Epoch 759/2000 - training_loss: 4455.56811924\n",
      "Epoch 760/2000 - training_loss: 4406.56747600\n",
      "Epoch 761/2000 - training_loss: 4360.05532771\n",
      "Epoch 762/2000 - training_loss: 4317.52897531\n",
      "Epoch 763/2000 - training_loss: 4281.51162512\n",
      "Epoch 764/2000 - training_loss: 4252.71032692\n",
      "Epoch 765/2000 - training_loss: 4232.07896566\n",
      "Epoch 766/2000 - training_loss: 4220.32707884\n",
      "Epoch 767/2000 - training_loss: 4216.65805106\n",
      "Epoch 768/2000 - training_loss: 4220.47073260\n",
      "Epoch 769/2000 - training_loss: 4229.91385002\n",
      "Epoch 770/2000 - training_loss: 4243.69063640\n",
      "Epoch 771/2000 - training_loss: 4259.01665852\n",
      "Epoch 772/2000 - training_loss: 4274.12583786\n",
      "Epoch 773/2000 - training_loss: 4286.33753858\n",
      "Epoch 774/2000 - training_loss: 4293.95128211\n",
      "Epoch 775/2000 - training_loss: 4295.44024282\n",
      "Epoch 776/2000 - training_loss: 4291.13964058\n",
      "Epoch 777/2000 - training_loss: 4281.40217149\n",
      "Epoch 778/2000 - training_loss: 4267.66980767\n",
      "Epoch 779/2000 - training_loss: 4252.61020273\n",
      "Epoch 780/2000 - training_loss: 4237.97296045\n",
      "Epoch 781/2000 - training_loss: 4226.14467548\n",
      "Epoch 782/2000 - training_loss: 4218.99060713\n",
      "Epoch 783/2000 - training_loss: 4216.51786966\n",
      "Epoch 784/2000 - training_loss: 4219.14013608\n",
      "Epoch 785/2000 - training_loss: 4225.13826143\n",
      "Epoch 786/2000 - training_loss: 4232.52652749\n",
      "Epoch 787/2000 - training_loss: 4239.46601475\n",
      "Epoch 788/2000 - training_loss: 4243.21897912\n",
      "Epoch 789/2000 - training_loss: 4243.21897912\n",
      "Epoch 790/2000 - training_loss: 4239.24215930\n",
      "Epoch 791/2000 - training_loss: 4232.56887865\n",
      "Epoch 792/2000 - training_loss: 4225.13826143\n",
      "Epoch 793/2000 - training_loss: 4219.16223964\n",
      "Epoch 794/2000 - training_loss: 4216.79677264\n",
      "Epoch 795/2000 - training_loss: 4217.68308360\n",
      "Epoch 796/2000 - training_loss: 4221.58460317\n",
      "Epoch 797/2000 - training_loss: 4225.73598243\n",
      "Epoch 798/2000 - training_loss: 4228.76505084\n",
      "Epoch 799/2000 - training_loss: 4228.64019795\n",
      "Epoch 800/2000 - training_loss: 4225.53986006\n",
      "Epoch 801/2000 - training_loss: 4221.33847925\n",
      "Epoch 802/2000 - training_loss: 4217.47617979\n",
      "Epoch 803/2000 - training_loss: 4216.58990917\n",
      "Epoch 804/2000 - training_loss: 4218.31452427\n",
      "Epoch 805/2000 - training_loss: 4221.31462975\n",
      "Epoch 806/2000 - training_loss: 4223.07924115\n",
      "Epoch 807/2000 - training_loss: 4222.87745502\n",
      "Epoch 808/2000 - training_loss: 4220.16182350\n",
      "Epoch 809/2000 - training_loss: 4217.60396415\n",
      "Epoch 810/2000 - training_loss: 4216.65136709\n",
      "Epoch 811/2000 - training_loss: 4218.08217308\n",
      "Epoch 812/2000 - training_loss: 4219.91182729\n",
      "Epoch 813/2000 - training_loss: 4220.63334559\n",
      "Epoch 814/2000 - training_loss: 4219.27749287\n",
      "Epoch 815/2000 - training_loss: 4217.37074503\n",
      "Epoch 816/2000 - training_loss: 4216.57132395\n",
      "Epoch 817/2000 - training_loss: 4217.82960777\n",
      "Epoch 818/2000 - training_loss: 4219.19498896\n",
      "Epoch 819/2000 - training_loss: 4218.84022741\n",
      "Epoch 820/2000 - training_loss: 4217.13280164\n",
      "Epoch 821/2000 - training_loss: 4216.57577763\n",
      "Epoch 822/2000 - training_loss: 4217.53890037\n",
      "Epoch 823/2000 - training_loss: 4218.37863446\n",
      "Epoch 824/2000 - training_loss: 4217.88843407\n",
      "Epoch 825/2000 - training_loss: 4216.74786049\n",
      "Epoch 826/2000 - training_loss: 4216.86227334\n",
      "Epoch 827/2000 - training_loss: 4217.82960777\n",
      "Epoch 828/2000 - training_loss: 4217.66314976\n",
      "Epoch 829/2000 - training_loss: 4216.49313693\n",
      "Epoch 830/2000 - training_loss: 4216.89872955\n",
      "Epoch 831/2000 - training_loss: 4217.57616547\n",
      "Epoch 832/2000 - training_loss: 4217.01357917\n",
      "Epoch 833/2000 - training_loss: 4216.63855454\n",
      "Epoch 834/2000 - training_loss: 4217.04148819\n",
      "Epoch 835/2000 - training_loss: 4217.28311861\n",
      "Epoch 836/2000 - training_loss: 4216.56575327\n",
      "Epoch 837/2000 - training_loss: 4216.94277870\n",
      "Epoch 838/2000 - training_loss: 4217.27073609\n",
      "Epoch 839/2000 - training_loss: 4216.61657983\n",
      "Epoch 840/2000 - training_loss: 4216.95958779\n",
      "Epoch 841/2000 - training_loss: 4216.86227334\n",
      "Epoch 842/2000 - training_loss: 4216.59015775\n",
      "Epoch 843/2000 - training_loss: 4216.84527817\n",
      "Epoch 844/2000 - training_loss: 4216.60250785\n",
      "Epoch 845/2000 - training_loss: 4216.78381497\n",
      "Epoch 846/2000 - training_loss: 4216.69526338\n",
      "Epoch 847/2000 - training_loss: 4216.69309548\n",
      "Epoch 848/2000 - training_loss: 4216.85867595\n",
      "Epoch 849/2000 - training_loss: 4216.63855454\n",
      "Epoch 850/2000 - training_loss: 4216.89493344\n",
      "Epoch 851/2000 - training_loss: 4216.54405951\n",
      "Epoch 852/2000 - training_loss: 4216.74343072\n",
      "Epoch 853/2000 - training_loss: 4216.53157931\n",
      "Epoch 854/2000 - training_loss: 4216.75345120\n",
      "Epoch 855/2000 - training_loss: 4216.62717785\n",
      "Epoch 856/2000 - training_loss: 4216.53556146\n",
      "Epoch 857/2000 - training_loss: 4216.91429875\n",
      "Epoch 858/2000 - training_loss: 4216.62403362\n",
      "Epoch 859/2000 - training_loss: 4216.62717785\n",
      "Epoch 860/2000 - training_loss: 4216.84551228\n",
      "Epoch 861/2000 - training_loss: 4216.64336162\n",
      "Epoch 862/2000 - training_loss: 4216.47844223\n",
      "Epoch 863/2000 - training_loss: 4216.68293362\n",
      "Epoch 864/2000 - training_loss: 4216.89657535\n",
      "Epoch 865/2000 - training_loss: 4217.32305889\n",
      "Epoch 866/2000 - training_loss: 4218.65956839\n",
      "Epoch 867/2000 - training_loss: 4224.36244349\n",
      "Epoch 868/2000 - training_loss: 4254.74165343\n",
      "Epoch 869/2000 - training_loss: 4486.56940486\n",
      "Epoch 870/2000 - training_loss: 9252.47383226\n",
      "Epoch 871/2000 - training_loss: 7875.14620148\n",
      "Epoch 872/2000 - training_loss: 27653.54983500\n",
      "Epoch 873/2000 - training_loss: 3421.93481650\n",
      "Epoch 874/2000 - training_loss: 169998.11959870\n",
      "Epoch 875/2000 - training_loss: 243584.12195884\n",
      "Epoch 876/2000 - training_loss: 108326.84900308\n",
      "Epoch 877/2000 - training_loss: 5226.02749525\n",
      "Epoch 878/2000 - training_loss: 993197.21690154\n",
      "Epoch 879/2000 - training_loss: 20629.23863951\n",
      "Epoch 880/2000 - training_loss: 2695222.29247546\n",
      "Epoch 881/2000 - training_loss: 48564.94161486\n",
      "Epoch 882/2000 - training_loss: 19641.32187906\n",
      "Epoch 883/2000 - training_loss: 22992.91962218\n",
      "Epoch 884/2000 - training_loss: 155167.69727360\n",
      "Epoch 885/2000 - training_loss: 1126370.87990873\n",
      "Epoch 886/2000 - training_loss: 33116.34034116\n",
      "Epoch 887/2000 - training_loss: 4178.74240865\n",
      "Epoch 888/2000 - training_loss: 251761.24509718\n",
      "Epoch 889/2000 - training_loss: 22213.49942535\n",
      "Epoch 890/2000 - training_loss: 330959.38055003\n",
      "Epoch 891/2000 - training_loss: 13989.91159460\n",
      "Epoch 892/2000 - training_loss: 5384.81388867\n",
      "Epoch 893/2000 - training_loss: 7832.16994124\n",
      "Epoch 894/2000 - training_loss: 133161.32376874\n",
      "Epoch 895/2000 - training_loss: 687807.47408274\n",
      "Epoch 896/2000 - training_loss: 30257.15494423\n",
      "Epoch 897/2000 - training_loss: 45098.21195651\n",
      "Epoch 898/2000 - training_loss: 1428644.76653134\n",
      "Epoch 899/2000 - training_loss: 1450503.78084604\n",
      "Epoch 900/2000 - training_loss: 3301.90314534\n",
      "Epoch 901/2000 - training_loss: 13469.03823162\n",
      "Epoch 902/2000 - training_loss: 7468.30021994\n",
      "Epoch 903/2000 - training_loss: 3932.83671891\n",
      "Epoch 904/2000 - training_loss: 108072.28672876\n",
      "Epoch 905/2000 - training_loss: 3012.35043374\n",
      "Epoch 906/2000 - training_loss: 20166.61628188\n",
      "Epoch 907/2000 - training_loss: 58095.63998158\n",
      "Epoch 908/2000 - training_loss: 3014.37537853\n",
      "Epoch 909/2000 - training_loss: 17622.41961204\n",
      "Epoch 910/2000 - training_loss: 7364.82407929\n",
      "Epoch 911/2000 - training_loss: 3417.83096630\n",
      "Epoch 912/2000 - training_loss: 8503.83855014\n",
      "Epoch 913/2000 - training_loss: 4755.87607964\n",
      "Epoch 914/2000 - training_loss: 4115.66971251\n",
      "Epoch 915/2000 - training_loss: 83532.32262572\n",
      "Epoch 916/2000 - training_loss: 6355.05354029\n",
      "Epoch 917/2000 - training_loss: 5048.37455930\n",
      "Epoch 918/2000 - training_loss: 743223.38533378\n",
      "Epoch 919/2000 - training_loss: 4078.93735441\n",
      "Epoch 920/2000 - training_loss: 42814.50677094\n",
      "Epoch 921/2000 - training_loss: 33239.92386211\n",
      "Epoch 922/2000 - training_loss: 2811.75964972\n",
      "Epoch 923/2000 - training_loss: 1050307.86340431\n",
      "Epoch 924/2000 - training_loss: 345336.55353124\n",
      "Epoch 925/2000 - training_loss: 14767.00042846\n",
      "Epoch 926/2000 - training_loss: 22001.93475249\n",
      "Epoch 927/2000 - training_loss: 1175994.48599987\n",
      "Epoch 928/2000 - training_loss: 5857.52297416\n",
      "Epoch 929/2000 - training_loss: 2595.56722594\n",
      "Epoch 930/2000 - training_loss: 18319.22421473\n",
      "Epoch 931/2000 - training_loss: 10153.79810215\n",
      "Epoch 932/2000 - training_loss: 2721.17171259\n",
      "Epoch 933/2000 - training_loss: 4773.86504409\n",
      "Epoch 934/2000 - training_loss: 367519.14032133\n",
      "Epoch 935/2000 - training_loss: 5925.33434139\n",
      "Epoch 936/2000 - training_loss: 9923.11907586\n",
      "Epoch 937/2000 - training_loss: 5357.48766613\n",
      "Epoch 938/2000 - training_loss: 18136.34334287\n",
      "Epoch 939/2000 - training_loss: 3658.59127277\n",
      "Epoch 940/2000 - training_loss: 18216.30772084\n",
      "Epoch 941/2000 - training_loss: 4063.77242635\n",
      "Epoch 942/2000 - training_loss: 7231.66530897\n",
      "Epoch 943/2000 - training_loss: 4170.24395756\n",
      "Epoch 944/2000 - training_loss: 86248.34946814\n",
      "Epoch 945/2000 - training_loss: 6341.26137222\n",
      "Epoch 946/2000 - training_loss: 65763605.36959603\n",
      "Epoch 947/2000 - training_loss: 1728235.94786625\n",
      "Epoch 948/2000 - training_loss: 3379.24329837\n",
      "Epoch 949/2000 - training_loss: 3011.57058846\n",
      "Epoch 950/2000 - training_loss: 3307.12981624\n",
      "Epoch 951/2000 - training_loss: 14293.10803783\n",
      "Epoch 952/2000 - training_loss: 4358.00313867\n",
      "Epoch 953/2000 - training_loss: 2428.83167357\n",
      "Epoch 954/2000 - training_loss: 3168.14313355\n",
      "Epoch 955/2000 - training_loss: 4197.21862261\n",
      "Epoch 956/2000 - training_loss: 15416.48723407\n",
      "Epoch 957/2000 - training_loss: 3353.87212333\n",
      "Epoch 958/2000 - training_loss: 6617.26039783\n",
      "Epoch 959/2000 - training_loss: 13064.18734653\n",
      "Epoch 960/2000 - training_loss: 66042.50363027\n",
      "Epoch 961/2000 - training_loss: 266903.68662824\n",
      "Epoch 962/2000 - training_loss: 13056.33143617\n",
      "Epoch 963/2000 - training_loss: 7795.75805008\n",
      "Epoch 964/2000 - training_loss: 12748.42517015\n",
      "Epoch 965/2000 - training_loss: 2432333.40919812\n",
      "Epoch 966/2000 - training_loss: 413105.88892240\n",
      "Epoch 967/2000 - training_loss: 9238.16997855\n",
      "Epoch 968/2000 - training_loss: 3130.75574263\n",
      "Epoch 969/2000 - training_loss: 55340.78491932\n",
      "Epoch 970/2000 - training_loss: 4815.59523304\n",
      "Epoch 971/2000 - training_loss: 24086.79104542\n",
      "Epoch 972/2000 - training_loss: 3173.50706388\n",
      "Epoch 973/2000 - training_loss: 6499.63809777\n",
      "Epoch 974/2000 - training_loss: 5045.10676279\n",
      "Epoch 975/2000 - training_loss: 5650.08406145\n",
      "Epoch 976/2000 - training_loss: 27926.53592553\n",
      "Epoch 977/2000 - training_loss: 2481366.50427626\n",
      "Epoch 978/2000 - training_loss: 8750.43388064\n",
      "Epoch 979/2000 - training_loss: 462514.40182149\n",
      "Epoch 980/2000 - training_loss: 14452.34419918\n",
      "Epoch 981/2000 - training_loss: 118154.87588807\n",
      "Epoch 982/2000 - training_loss: 26595.28697028\n",
      "Epoch 983/2000 - training_loss: 7163.62109347\n",
      "Epoch 984/2000 - training_loss: 5565.56454231\n",
      "Epoch 985/2000 - training_loss: 7298.16766731\n",
      "Epoch 986/2000 - training_loss: 16881.87590861\n",
      "Epoch 987/2000 - training_loss: 266384.40627228\n",
      "Epoch 988/2000 - training_loss: 137795.72976159\n",
      "Epoch 989/2000 - training_loss: 525616.30313688\n",
      "Epoch 990/2000 - training_loss: 16068.28572798\n",
      "Epoch 991/2000 - training_loss: 12491.03543349\n",
      "Epoch 992/2000 - training_loss: 42552.11099827\n",
      "Epoch 993/2000 - training_loss: 635347.54950271\n",
      "Epoch 994/2000 - training_loss: 9315.91637208\n",
      "Epoch 995/2000 - training_loss: 4967.74627982\n",
      "Epoch 996/2000 - training_loss: 5952.08706660\n",
      "Epoch 997/2000 - training_loss: 19222.94271702\n",
      "Epoch 998/2000 - training_loss: 212774.89632780\n",
      "Epoch 999/2000 - training_loss: 35945.62319603\n",
      "Epoch 1000/2000 - training_loss: 67657.00387860\n",
      "Epoch 1001/2000 - training_loss: 79932.80536585\n",
      "Epoch 1002/2000 - training_loss: 60531.13696822\n",
      "Epoch 1003/2000 - training_loss: 10166.08333389\n",
      "Epoch 1004/2000 - training_loss: 5922.11597173\n",
      "Epoch 1005/2000 - training_loss: 5706.46550164\n",
      "Epoch 1006/2000 - training_loss: 8961.14543413\n",
      "Epoch 1007/2000 - training_loss: 36255.51102643\n",
      "Epoch 1008/2000 - training_loss: 616958.76911354\n",
      "Epoch 1009/2000 - training_loss: 7667.06328962\n",
      "Epoch 1010/2000 - training_loss: 76231.31782649\n",
      "Epoch 1011/2000 - training_loss: 25344.57849303\n",
      "Epoch 1012/2000 - training_loss: 7966.63390804\n",
      "Epoch 1013/2000 - training_loss: 213255.54998585\n",
      "Epoch 1014/2000 - training_loss: 15326.72033315\n",
      "Epoch 1015/2000 - training_loss: 5981.59538128\n",
      "Epoch 1016/2000 - training_loss: 6513.01400083\n",
      "Epoch 1017/2000 - training_loss: 15366.36183409\n",
      "Epoch 1018/2000 - training_loss: 518732.36813650\n",
      "Epoch 1019/2000 - training_loss: 31376.61551796\n",
      "Epoch 1020/2000 - training_loss: 12135.07512241\n",
      "Epoch 1021/2000 - training_loss: 7601.39423037\n",
      "Epoch 1022/2000 - training_loss: 6059.72687637\n",
      "Epoch 1023/2000 - training_loss: 5681.64468624\n",
      "Epoch 1024/2000 - training_loss: 6102.53499371\n",
      "Epoch 1025/2000 - training_loss: 7525.89257115\n",
      "Epoch 1026/2000 - training_loss: 10955.56198038\n",
      "Epoch 1027/2000 - training_loss: 20157.41326794\n",
      "Epoch 1028/2000 - training_loss: 54099.73736421\n",
      "Epoch 1029/2000 - training_loss: 200846.31127931\n",
      "Epoch 1030/2000 - training_loss: 24474.56559987\n",
      "Epoch 1031/2000 - training_loss: 10638.49888870\n",
      "Epoch 1032/2000 - training_loss: 7106.44702552\n",
      "Epoch 1033/2000 - training_loss: 5927.56095002\n",
      "Epoch 1034/2000 - training_loss: 5686.31294324\n",
      "Epoch 1035/2000 - training_loss: 6049.50531478\n",
      "Epoch 1036/2000 - training_loss: 7030.16904284\n",
      "Epoch 1037/2000 - training_loss: 8899.14834430\n",
      "Epoch 1038/2000 - training_loss: 12375.27782808\n",
      "Epoch 1039/2000 - training_loss: 19169.05910347\n",
      "Epoch 1040/2000 - training_loss: 33343.69788034\n",
      "Epoch 1041/2000 - training_loss: 58490.56999659\n",
      "Epoch 1042/2000 - training_loss: 62038.03135179\n",
      "Epoch 1043/2000 - training_loss: 36469.54847762\n",
      "Epoch 1044/2000 - training_loss: 21100.17836636\n",
      "Epoch 1045/2000 - training_loss: 13837.38428777\n",
      "Epoch 1046/2000 - training_loss: 10157.94329885\n",
      "Epoch 1047/2000 - training_loss: 8123.93292451\n",
      "Epoch 1048/2000 - training_loss: 6937.10893889\n",
      "Epoch 1049/2000 - training_loss: 6240.48143794\n",
      "Epoch 1050/2000 - training_loss: 5855.28567604\n",
      "Epoch 1051/2000 - training_loss: 5694.25462301\n",
      "Epoch 1052/2000 - training_loss: 5710.82410995\n",
      "Epoch 1053/2000 - training_loss: 5887.83788072\n",
      "Epoch 1054/2000 - training_loss: 6222.95255710\n",
      "Epoch 1055/2000 - training_loss: 6723.65073000\n",
      "Epoch 1056/2000 - training_loss: 7399.53941015\n",
      "Epoch 1057/2000 - training_loss: 8254.16251095\n",
      "Epoch 1058/2000 - training_loss: 9274.78554298\n",
      "Epoch 1059/2000 - training_loss: 10353.65731756\n",
      "Epoch 1060/2000 - training_loss: 11328.68922526\n",
      "Epoch 1061/2000 - training_loss: 11946.42439289\n",
      "Epoch 1062/2000 - training_loss: 11997.75980926\n",
      "Epoch 1063/2000 - training_loss: 11447.88415982\n",
      "Epoch 1064/2000 - training_loss: 10519.25196607\n",
      "Epoch 1065/2000 - training_loss: 9456.04115711\n",
      "Epoch 1066/2000 - training_loss: 8456.14928046\n",
      "Epoch 1067/2000 - training_loss: 7617.86231903\n",
      "Epoch 1068/2000 - training_loss: 6956.26203093\n",
      "Epoch 1069/2000 - training_loss: 6461.32345526\n",
      "Epoch 1070/2000 - training_loss: 6108.65516419\n",
      "Epoch 1071/2000 - training_loss: 5873.85565866\n",
      "Epoch 1072/2000 - training_loss: 5738.54137850\n",
      "Epoch 1073/2000 - training_loss: 5683.72272441\n",
      "Epoch 1074/2000 - training_loss: 5696.99682990\n",
      "Epoch 1075/2000 - training_loss: 5766.89700105\n",
      "Epoch 1076/2000 - training_loss: 5881.90741258\n",
      "Epoch 1077/2000 - training_loss: 6027.53622885\n",
      "Epoch 1078/2000 - training_loss: 6192.43230786\n",
      "Epoch 1079/2000 - training_loss: 6355.73246982\n",
      "Epoch 1080/2000 - training_loss: 6500.39354883\n",
      "Epoch 1081/2000 - training_loss: 6607.23525153\n",
      "Epoch 1082/2000 - training_loss: 6661.43454960\n",
      "Epoch 1083/2000 - training_loss: 6655.16271890\n",
      "Epoch 1084/2000 - training_loss: 6588.87296408\n",
      "Epoch 1085/2000 - training_loss: 6471.28090761\n",
      "Epoch 1086/2000 - training_loss: 6323.29363244\n",
      "Epoch 1087/2000 - training_loss: 6160.79704077\n",
      "Epoch 1088/2000 - training_loss: 6005.01489188\n",
      "Epoch 1089/2000 - training_loss: 5870.18462293\n",
      "Epoch 1090/2000 - training_loss: 5768.36279564\n",
      "Epoch 1091/2000 - training_loss: 5704.17555164\n",
      "Epoch 1092/2000 - training_loss: 5681.17721763\n",
      "Epoch 1093/2000 - training_loss: 5695.77277287\n",
      "Epoch 1094/2000 - training_loss: 5741.44769713\n",
      "Epoch 1095/2000 - training_loss: 5806.22418255\n",
      "Epoch 1096/2000 - training_loss: 5875.19907791\n",
      "Epoch 1097/2000 - training_loss: 5930.31609556\n",
      "Epoch 1098/2000 - training_loss: 5957.52310222\n",
      "Epoch 1099/2000 - training_loss: 5950.56734220\n",
      "Epoch 1100/2000 - training_loss: 5909.39165021\n",
      "Epoch 1101/2000 - training_loss: 5846.71552128\n",
      "Epoch 1102/2000 - training_loss: 5778.93533850\n",
      "Epoch 1103/2000 - training_loss: 5721.95294625\n",
      "Epoch 1104/2000 - training_loss: 5688.20789533\n",
      "Epoch 1105/2000 - training_loss: 5681.61456878\n",
      "Epoch 1106/2000 - training_loss: 5700.00328673\n",
      "Epoch 1107/2000 - training_loss: 5734.02989436\n",
      "Epoch 1108/2000 - training_loss: 5768.87117085\n",
      "Epoch 1109/2000 - training_loss: 5791.45980678\n",
      "Epoch 1110/2000 - training_loss: 5793.39058290\n",
      "Epoch 1111/2000 - training_loss: 5772.47256714\n",
      "Epoch 1112/2000 - training_loss: 5738.96715714\n",
      "Epoch 1113/2000 - training_loss: 5704.37014105\n",
      "Epoch 1114/2000 - training_loss: 5683.89778490\n",
      "Epoch 1115/2000 - training_loss: 5683.11374428\n",
      "Epoch 1116/2000 - training_loss: 5700.39220620\n",
      "Epoch 1117/2000 - training_loss: 5723.47197684\n",
      "Epoch 1118/2000 - training_loss: 5738.54137850\n",
      "Epoch 1119/2000 - training_loss: 5735.12381878\n",
      "Epoch 1120/2000 - training_loss: 5715.01591886\n",
      "Epoch 1121/2000 - training_loss: 5692.79088889\n",
      "Epoch 1122/2000 - training_loss: 5681.29897392\n",
      "Epoch 1123/2000 - training_loss: 5686.24396737\n",
      "Epoch 1124/2000 - training_loss: 5701.64965914\n",
      "Epoch 1125/2000 - training_loss: 5713.58098357\n",
      "Epoch 1126/2000 - training_loss: 5711.34104473\n",
      "Epoch 1127/2000 - training_loss: 5697.84957860\n",
      "Epoch 1128/2000 - training_loss: 5684.07468434\n",
      "Epoch 1129/2000 - training_loss: 5682.17091110\n",
      "Epoch 1130/2000 - training_loss: 5692.12330781\n",
      "Epoch 1131/2000 - training_loss: 5701.38046678\n",
      "Epoch 1132/2000 - training_loss: 5699.12072318\n",
      "Epoch 1133/2000 - training_loss: 5687.84760070\n",
      "Epoch 1134/2000 - training_loss: 5681.01459970\n",
      "Epoch 1135/2000 - training_loss: 5685.80996282\n",
      "Epoch 1136/2000 - training_loss: 5694.25462301\n",
      "Epoch 1137/2000 - training_loss: 5693.83485674\n",
      "Epoch 1138/2000 - training_loss: 5685.03454941\n",
      "Epoch 1139/2000 - training_loss: 5681.32980991\n",
      "Epoch 1140/2000 - training_loss: 5686.57170363\n",
      "Epoch 1141/2000 - training_loss: 5691.20943533\n",
      "Epoch 1142/2000 - training_loss: 5686.38194804\n",
      "Epoch 1143/2000 - training_loss: 5680.71304122\n",
      "Epoch 1144/2000 - training_loss: 5684.74653175\n",
      "Epoch 1145/2000 - training_loss: 5688.64527014\n",
      "Epoch 1146/2000 - training_loss: 5684.18024072\n",
      "Epoch 1147/2000 - training_loss: 5680.78641618\n",
      "Epoch 1148/2000 - training_loss: 5685.84310756\n",
      "Epoch 1149/2000 - training_loss: 5685.76525466\n",
      "Epoch 1150/2000 - training_loss: 5681.46391381\n",
      "Epoch 1151/2000 - training_loss: 5683.33090932\n",
      "Epoch 1152/2000 - training_loss: 5685.45320996\n",
      "Epoch 1153/2000 - training_loss: 5681.63658898\n",
      "Epoch 1154/2000 - training_loss: 5682.40724619\n",
      "Epoch 1155/2000 - training_loss: 5684.86744359\n",
      "Epoch 1156/2000 - training_loss: 5681.12984276\n",
      "Epoch 1157/2000 - training_loss: 5683.21169703\n",
      "Epoch 1158/2000 - training_loss: 5683.12974034\n",
      "Epoch 1159/2000 - training_loss: 5680.78641618\n",
      "Epoch 1160/2000 - training_loss: 5684.01257834\n",
      "Epoch 1161/2000 - training_loss: 5681.46391381\n",
      "Epoch 1162/2000 - training_loss: 5682.73007340\n",
      "Epoch 1163/2000 - training_loss: 5681.91555969\n",
      "Epoch 1164/2000 - training_loss: 5682.21825469\n",
      "Epoch 1165/2000 - training_loss: 5682.41885719\n",
      "Epoch 1166/2000 - training_loss: 5681.70459679\n",
      "Epoch 1167/2000 - training_loss: 5681.88831741\n",
      "Epoch 1168/2000 - training_loss: 5681.54074652\n",
      "Epoch 1169/2000 - training_loss: 5681.54074652\n",
      "Epoch 1170/2000 - training_loss: 5681.88831741\n",
      "Epoch 1171/2000 - training_loss: 5681.21417063\n",
      "Epoch 1172/2000 - training_loss: 5682.58956894\n",
      "Epoch 1173/2000 - training_loss: 5680.91482564\n",
      "Epoch 1174/2000 - training_loss: 5681.52861374\n",
      "Epoch 1175/2000 - training_loss: 5682.61370397\n",
      "Epoch 1176/2000 - training_loss: 5681.69577290\n",
      "Epoch 1177/2000 - training_loss: 5681.21564467\n",
      "Epoch 1178/2000 - training_loss: 5681.70106332\n",
      "Epoch 1179/2000 - training_loss: 5683.09092273\n",
      "Epoch 1180/2000 - training_loss: 5683.80780009\n",
      "Epoch 1181/2000 - training_loss: 5686.08946040\n",
      "Epoch 1182/2000 - training_loss: 5693.57185912\n",
      "Epoch 1183/2000 - training_loss: 5728.45706740\n",
      "Epoch 1184/2000 - training_loss: 5928.60867864\n",
      "Epoch 1185/2000 - training_loss: 7975.32682314\n",
      "Epoch 1186/2000 - training_loss: 10459.57951365\n",
      "Epoch 1187/2000 - training_loss: 7346.98097167\n",
      "Epoch 1188/2000 - training_loss: 21187.72903161\n",
      "Epoch 1189/2000 - training_loss: 111529.74157466\n",
      "Epoch 1190/2000 - training_loss: 4206.80300269\n",
      "Epoch 1191/2000 - training_loss: 35216.43045673\n",
      "Epoch 1192/2000 - training_loss: 24639.92505123\n",
      "Epoch 1193/2000 - training_loss: 3292.87031296\n",
      "Epoch 1194/2000 - training_loss: 44205.89782464\n",
      "Epoch 1195/2000 - training_loss: 3428.70134806\n",
      "Epoch 1196/2000 - training_loss: 7190.76318176\n",
      "Epoch 1197/2000 - training_loss: 10329.70982177\n",
      "Epoch 1198/2000 - training_loss: 2481.53165829\n",
      "Epoch 1199/2000 - training_loss: 21550.11295510\n",
      "Epoch 1200/2000 - training_loss: 36156211.98427463\n",
      "Epoch 1201/2000 - training_loss: 4896.71777352\n",
      "Epoch 1202/2000 - training_loss: 83343.47645918\n",
      "Epoch 1203/2000 - training_loss: 4350.27184292\n",
      "Epoch 1204/2000 - training_loss: 4023.77310823\n",
      "Epoch 1205/2000 - training_loss: 6016.08643963\n",
      "Epoch 1206/2000 - training_loss: 53061.46863200\n",
      "Epoch 1207/2000 - training_loss: 2479.02690468\n",
      "Epoch 1208/2000 - training_loss: 107019.82048806\n",
      "Epoch 1209/2000 - training_loss: 6223.96578388\n",
      "Epoch 1210/2000 - training_loss: 41665.13249136\n",
      "Epoch 1211/2000 - training_loss: 97023.42199311\n",
      "Epoch 1212/2000 - training_loss: 11393.13054383\n",
      "Epoch 1213/2000 - training_loss: 6573.04599678\n",
      "Epoch 1214/2000 - training_loss: 8762.54618088\n",
      "Epoch 1215/2000 - training_loss: 140812.89729381\n",
      "Epoch 1216/2000 - training_loss: 11994.88876890\n",
      "Epoch 1217/2000 - training_loss: 11565.46995312\n",
      "Epoch 1218/2000 - training_loss: 489615.31570380\n",
      "Epoch 1219/2000 - training_loss: 48546.82476053\n",
      "Epoch 1220/2000 - training_loss: 59025.14252454\n",
      "Epoch 1221/2000 - training_loss: 15453.47187431\n",
      "Epoch 1222/2000 - training_loss: 16298.34500265\n",
      "Epoch 1223/2000 - training_loss: 17846.43249474\n",
      "Epoch 1224/2000 - training_loss: 4210.47362047\n",
      "Epoch 1225/2000 - training_loss: 450614.83554461\n",
      "Epoch 1226/2000 - training_loss: 4839.24116881\n",
      "Epoch 1227/2000 - training_loss: 8379.19620516\n",
      "Epoch 1228/2000 - training_loss: 216109.32671331\n",
      "Epoch 1229/2000 - training_loss: 4304.19723733\n",
      "Epoch 1230/2000 - training_loss: 9940.07645915\n",
      "Epoch 1231/2000 - training_loss: 51677.51944865\n",
      "Epoch 1232/2000 - training_loss: 65041.39787904\n",
      "Epoch 1233/2000 - training_loss: 6234.58639868\n",
      "Epoch 1234/2000 - training_loss: 34918.77044906\n",
      "Epoch 1235/2000 - training_loss: 16950.26185575\n",
      "Epoch 1236/2000 - training_loss: 4631.12339744\n",
      "Epoch 1237/2000 - training_loss: 8892.98176972\n",
      "Epoch 1238/2000 - training_loss: 57345.13044988\n",
      "Epoch 1239/2000 - training_loss: 93766.47539902\n",
      "Epoch 1240/2000 - training_loss: 24486.33588591\n",
      "Epoch 1241/2000 - training_loss: 5544.29402486\n",
      "Epoch 1242/2000 - training_loss: 3639.76932743\n",
      "Epoch 1243/2000 - training_loss: 3834.61659363\n",
      "Epoch 1244/2000 - training_loss: 5973.06837589\n",
      "Epoch 1245/2000 - training_loss: 16112.28281599\n",
      "Epoch 1246/2000 - training_loss: 591095.35299596\n",
      "Epoch 1247/2000 - training_loss: 103683.64603373\n",
      "Epoch 1248/2000 - training_loss: 39952.93014901\n",
      "Epoch 1249/2000 - training_loss: 22119.55702964\n",
      "Epoch 1250/2000 - training_loss: 14602.71746940\n",
      "Epoch 1251/2000 - training_loss: 10703.91301263\n",
      "Epoch 1252/2000 - training_loss: 8411.65529575\n",
      "Epoch 1253/2000 - training_loss: 6946.93112406\n",
      "Epoch 1254/2000 - training_loss: 5956.43968673\n",
      "Epoch 1255/2000 - training_loss: 5262.06195179\n",
      "Epoch 1256/2000 - training_loss: 4759.95979779\n",
      "Epoch 1257/2000 - training_loss: 4391.03938725\n",
      "Epoch 1258/2000 - training_loss: 4118.18494167\n",
      "Epoch 1259/2000 - training_loss: 3916.34502091\n",
      "Epoch 1260/2000 - training_loss: 3769.64379450\n",
      "Epoch 1261/2000 - training_loss: 3666.30444608\n",
      "Epoch 1262/2000 - training_loss: 3597.82303704\n",
      "Epoch 1263/2000 - training_loss: 3558.43895131\n",
      "Epoch 1264/2000 - training_loss: 3543.86370785\n",
      "Epoch 1265/2000 - training_loss: 3550.68165647\n",
      "Epoch 1266/2000 - training_loss: 3576.41767759\n",
      "Epoch 1267/2000 - training_loss: 3619.17939286\n",
      "Epoch 1268/2000 - training_loss: 3677.36593416\n",
      "Epoch 1269/2000 - training_loss: 3749.65360673\n",
      "Epoch 1270/2000 - training_loss: 3835.12323932\n",
      "Epoch 1271/2000 - training_loss: 3932.58361199\n",
      "Epoch 1272/2000 - training_loss: 4041.97270998\n",
      "Epoch 1273/2000 - training_loss: 4161.41660675\n",
      "Epoch 1274/2000 - training_loss: 4289.91382945\n",
      "Epoch 1275/2000 - training_loss: 4428.28182830\n",
      "Epoch 1276/2000 - training_loss: 4573.81825365\n",
      "Epoch 1277/2000 - training_loss: 4724.43835080\n",
      "Epoch 1278/2000 - training_loss: 4881.95781425\n",
      "Epoch 1279/2000 - training_loss: 5041.36549064\n",
      "Epoch 1280/2000 - training_loss: 5202.11863487\n",
      "Epoch 1281/2000 - training_loss: 5362.32997297\n",
      "Epoch 1282/2000 - training_loss: 5518.26924648\n",
      "Epoch 1283/2000 - training_loss: 5667.46839435\n",
      "Epoch 1284/2000 - training_loss: 5808.77486163\n",
      "Epoch 1285/2000 - training_loss: 5936.81519688\n",
      "Epoch 1286/2000 - training_loss: 6047.23887538\n",
      "Epoch 1287/2000 - training_loss: 6141.09888167\n",
      "Epoch 1288/2000 - training_loss: 6211.65536280\n",
      "Epoch 1289/2000 - training_loss: 6258.33424756\n",
      "Epoch 1290/2000 - training_loss: 6279.78768450\n",
      "Epoch 1291/2000 - training_loss: 6274.30633388\n",
      "Epoch 1292/2000 - training_loss: 6243.35298636\n",
      "Epoch 1293/2000 - training_loss: 6186.55706010\n",
      "Epoch 1294/2000 - training_loss: 6105.35373074\n",
      "Epoch 1295/2000 - training_loss: 6005.82092878\n",
      "Epoch 1296/2000 - training_loss: 5886.87134993\n",
      "Epoch 1297/2000 - training_loss: 5754.91236661\n",
      "Epoch 1298/2000 - training_loss: 5611.65834024\n",
      "Epoch 1299/2000 - training_loss: 5463.40453015\n",
      "Epoch 1300/2000 - training_loss: 5309.23452541\n",
      "Epoch 1301/2000 - training_loss: 5155.26083393\n",
      "Epoch 1302/2000 - training_loss: 5003.34693238\n",
      "Epoch 1303/2000 - training_loss: 4855.84744939\n",
      "Epoch 1304/2000 - training_loss: 4713.95043169\n",
      "Epoch 1305/2000 - training_loss: 4579.31259897\n",
      "Epoch 1306/2000 - training_loss: 4452.86804726\n",
      "Epoch 1307/2000 - training_loss: 4334.86948525\n",
      "Epoch 1308/2000 - training_loss: 4225.35178715\n",
      "Epoch 1309/2000 - training_loss: 4125.66311523\n",
      "Epoch 1310/2000 - training_loss: 4034.92828503\n",
      "Epoch 1311/2000 - training_loss: 3952.79745170\n",
      "Epoch 1312/2000 - training_loss: 3879.75651925\n",
      "Epoch 1313/2000 - training_loss: 3814.82762973\n",
      "Epoch 1314/2000 - training_loss: 3758.15203801\n",
      "Epoch 1315/2000 - training_loss: 3709.20256703\n",
      "Epoch 1316/2000 - training_loss: 3667.63442193\n",
      "Epoch 1317/2000 - training_loss: 3632.85289047\n",
      "Epoch 1318/2000 - training_loss: 3604.46862161\n",
      "Epoch 1319/2000 - training_loss: 3582.21478256\n",
      "Epoch 1320/2000 - training_loss: 3565.42989439\n",
      "Epoch 1321/2000 - training_loss: 3553.79366470\n",
      "Epoch 1322/2000 - training_loss: 3546.74267949\n",
      "Epoch 1323/2000 - training_loss: 3543.72320111\n",
      "Epoch 1324/2000 - training_loss: 3544.31619930\n",
      "Epoch 1325/2000 - training_loss: 3547.89511453\n",
      "Epoch 1326/2000 - training_loss: 3554.01097170\n",
      "Epoch 1327/2000 - training_loss: 3561.76443696\n",
      "Epoch 1328/2000 - training_loss: 3570.96956456\n",
      "Epoch 1329/2000 - training_loss: 3580.73902785\n",
      "Epoch 1330/2000 - training_loss: 3590.43942205\n",
      "Epoch 1331/2000 - training_loss: 3599.63253421\n",
      "Epoch 1332/2000 - training_loss: 3607.58836198\n",
      "Epoch 1333/2000 - training_loss: 3614.10451857\n",
      "Epoch 1334/2000 - training_loss: 3618.59074858\n",
      "Epoch 1335/2000 - training_loss: 3620.88882161\n",
      "Epoch 1336/2000 - training_loss: 3620.73241138\n",
      "Epoch 1337/2000 - training_loss: 3618.54075457\n",
      "Epoch 1338/2000 - training_loss: 3613.85288455\n",
      "Epoch 1339/2000 - training_loss: 3607.45914809\n",
      "Epoch 1340/2000 - training_loss: 3599.35079784\n",
      "Epoch 1341/2000 - training_loss: 3590.31882222\n",
      "Epoch 1342/2000 - training_loss: 3580.81012972\n",
      "Epoch 1343/2000 - training_loss: 3571.46135334\n",
      "Epoch 1344/2000 - training_loss: 3562.80214999\n",
      "Epoch 1345/2000 - training_loss: 3555.52278800\n",
      "Epoch 1346/2000 - training_loss: 3549.68447489\n",
      "Epoch 1347/2000 - training_loss: 3545.78421435\n",
      "Epoch 1348/2000 - training_loss: 3543.80392367\n",
      "Epoch 1349/2000 - training_loss: 3543.84440228\n",
      "Epoch 1350/2000 - training_loss: 3545.33188914\n",
      "Epoch 1351/2000 - training_loss: 3548.08584389\n",
      "Epoch 1352/2000 - training_loss: 3551.52515621\n",
      "Epoch 1353/2000 - training_loss: 3555.02059669\n",
      "Epoch 1354/2000 - training_loss: 3557.77248602\n",
      "Epoch 1355/2000 - training_loss: 3559.62131991\n",
      "Epoch 1356/2000 - training_loss: 3560.04371115\n",
      "Epoch 1357/2000 - training_loss: 3559.07880529\n",
      "Epoch 1358/2000 - training_loss: 3556.74262030\n",
      "Epoch 1359/2000 - training_loss: 3553.70405423\n",
      "Epoch 1360/2000 - training_loss: 3550.22772622\n",
      "Epoch 1361/2000 - training_loss: 3547.21646646\n",
      "Epoch 1362/2000 - training_loss: 3544.82581900\n",
      "Epoch 1363/2000 - training_loss: 3543.68752822\n",
      "Epoch 1364/2000 - training_loss: 3543.85484549\n",
      "Epoch 1365/2000 - training_loss: 3544.95841772\n",
      "Epoch 1366/2000 - training_loss: 3546.61482878\n",
      "Epoch 1367/2000 - training_loss: 3548.41455227\n",
      "Epoch 1368/2000 - training_loss: 3549.53766445\n",
      "Epoch 1369/2000 - training_loss: 3549.60976793\n",
      "Epoch 1370/2000 - training_loss: 3548.80025059\n",
      "Epoch 1371/2000 - training_loss: 3547.20517634\n",
      "Epoch 1372/2000 - training_loss: 3545.44029875\n",
      "Epoch 1373/2000 - training_loss: 3544.14035877\n",
      "Epoch 1374/2000 - training_loss: 3543.63429182\n",
      "Epoch 1375/2000 - training_loss: 3543.97965867\n",
      "Epoch 1376/2000 - training_loss: 3545.06027139\n",
      "Epoch 1377/2000 - training_loss: 3545.97170982\n",
      "Epoch 1378/2000 - training_loss: 3546.53228356\n",
      "Epoch 1379/2000 - training_loss: 3546.23570879\n",
      "Epoch 1380/2000 - training_loss: 3545.35983168\n",
      "Epoch 1381/2000 - training_loss: 3544.25944595\n",
      "Epoch 1382/2000 - training_loss: 3543.63091473\n",
      "Epoch 1383/2000 - training_loss: 3543.72600310\n",
      "Epoch 1384/2000 - training_loss: 3544.42538561\n",
      "Epoch 1385/2000 - training_loss: 3545.01999387\n",
      "Epoch 1386/2000 - training_loss: 3545.27190410\n",
      "Epoch 1387/2000 - training_loss: 3544.68251582\n",
      "Epoch 1388/2000 - training_loss: 3543.94954903\n",
      "Epoch 1389/2000 - training_loss: 3543.55527766\n",
      "Epoch 1390/2000 - training_loss: 3543.89638487\n",
      "Epoch 1391/2000 - training_loss: 3544.33464408\n",
      "Epoch 1392/2000 - training_loss: 3544.66952622\n",
      "Epoch 1393/2000 - training_loss: 3544.34279080\n",
      "Epoch 1394/2000 - training_loss: 3543.80591257\n",
      "Epoch 1395/2000 - training_loss: 3543.53210955\n",
      "Epoch 1396/2000 - training_loss: 3543.95553450\n",
      "Epoch 1397/2000 - training_loss: 3544.23353773\n",
      "Epoch 1398/2000 - training_loss: 3544.10107465\n",
      "Epoch 1399/2000 - training_loss: 3543.82488712\n",
      "Epoch 1400/2000 - training_loss: 3543.63027364\n",
      "Epoch 1401/2000 - training_loss: 3543.95262175\n",
      "Epoch 1402/2000 - training_loss: 3544.11161339\n",
      "Epoch 1403/2000 - training_loss: 3543.75576414\n",
      "Epoch 1404/2000 - training_loss: 3543.67460555\n",
      "Epoch 1405/2000 - training_loss: 3543.70657829\n",
      "Epoch 1406/2000 - training_loss: 3544.00727026\n",
      "Epoch 1407/2000 - training_loss: 3543.76189779\n",
      "Epoch 1408/2000 - training_loss: 3543.58009114\n",
      "Epoch 1409/2000 - training_loss: 3543.71580176\n",
      "Epoch 1410/2000 - training_loss: 3543.75576414\n",
      "Epoch 1411/2000 - training_loss: 3543.64677687\n",
      "Epoch 1412/2000 - training_loss: 3543.60262977\n",
      "Epoch 1413/2000 - training_loss: 3543.71131475\n",
      "Epoch 1414/2000 - training_loss: 3543.69322237\n",
      "Epoch 1415/2000 - training_loss: 3543.53026948\n",
      "Epoch 1416/2000 - training_loss: 3543.82664403\n",
      "Epoch 1417/2000 - training_loss: 3543.67525775\n",
      "Epoch 1418/2000 - training_loss: 3543.55729092\n",
      "Epoch 1419/2000 - training_loss: 3543.68585561\n",
      "Epoch 1420/2000 - training_loss: 3543.62833172\n",
      "Epoch 1421/2000 - training_loss: 3543.67076281\n",
      "Epoch 1422/2000 - training_loss: 3543.67952830\n",
      "Epoch 1423/2000 - training_loss: 3543.62446305\n",
      "Epoch 1424/2000 - training_loss: 3543.69322237\n",
      "Epoch 1425/2000 - training_loss: 3543.63389381\n",
      "Epoch 1426/2000 - training_loss: 3543.67525775\n",
      "Epoch 1427/2000 - training_loss: 3543.63229540\n",
      "Epoch 1428/2000 - training_loss: 3543.64561916\n",
      "Epoch 1429/2000 - training_loss: 3543.68805462\n",
      "Epoch 1430/2000 - training_loss: 3543.64677687\n",
      "Epoch 1431/2000 - training_loss: 3543.53026948\n",
      "Epoch 1432/2000 - training_loss: 3543.67125623\n",
      "Epoch 1433/2000 - training_loss: 3543.59647651\n",
      "Epoch 1434/2000 - training_loss: 3543.64677687\n",
      "Epoch 1435/2000 - training_loss: 3543.59146458\n",
      "Epoch 1436/2000 - training_loss: 3543.67666929\n",
      "Epoch 1437/2000 - training_loss: 3543.67525775\n",
      "Epoch 1438/2000 - training_loss: 3543.67134236\n",
      "Epoch 1439/2000 - training_loss: 3543.64160729\n",
      "Epoch 1440/2000 - training_loss: 3543.63473533\n",
      "Epoch 1441/2000 - training_loss: 3543.69358480\n",
      "Epoch 1442/2000 - training_loss: 3543.71580176\n",
      "Epoch 1443/2000 - training_loss: 3543.85993558\n",
      "Epoch 1444/2000 - training_loss: 3544.21085173\n",
      "Epoch 1445/2000 - training_loss: 3545.31293418\n",
      "Epoch 1446/2000 - training_loss: 3551.16952458\n",
      "Epoch 1447/2000 - training_loss: 3583.82650668\n",
      "Epoch 1448/2000 - training_loss: 3744.63842220\n",
      "Epoch 1449/2000 - training_loss: 4401.26005069\n",
      "Epoch 1450/2000 - training_loss: 8149.62281508\n",
      "Epoch 1451/2000 - training_loss: 3984.73266521\n",
      "Epoch 1452/2000 - training_loss: 8629.59132756\n",
      "Epoch 1453/2000 - training_loss: 276872.43636831\n",
      "Epoch 1454/2000 - training_loss: 68143.45444061\n",
      "Epoch 1455/2000 - training_loss: 10338.21757259\n",
      "Epoch 1456/2000 - training_loss: 3692.30936577\n",
      "Epoch 1457/2000 - training_loss: 128776.14127040\n",
      "Epoch 1458/2000 - training_loss: 5793.74680587\n",
      "Epoch 1459/2000 - training_loss: 11925.66604846\n",
      "Epoch 1460/2000 - training_loss: 153787.82581667\n",
      "Epoch 1461/2000 - training_loss: 113321.06294697\n",
      "Epoch 1462/2000 - training_loss: 3558.05223326\n",
      "Epoch 1463/2000 - training_loss: 7073.06726061\n",
      "Epoch 1464/2000 - training_loss: 1433097.09957176\n",
      "Epoch 1465/2000 - training_loss: 3044.69307790\n",
      "Epoch 1466/2000 - training_loss: 3522.68562349\n",
      "Epoch 1467/2000 - training_loss: 6224.91000612\n",
      "Epoch 1468/2000 - training_loss: 3244.04520132\n",
      "Epoch 1469/2000 - training_loss: 3685.17012566\n",
      "Epoch 1470/2000 - training_loss: 26218.01464498\n",
      "Epoch 1471/2000 - training_loss: 2640.99691067\n",
      "Epoch 1472/2000 - training_loss: 2875.76288819\n",
      "Epoch 1473/2000 - training_loss: 11136.09284202\n",
      "Epoch 1474/2000 - training_loss: 8576.87245600\n",
      "Epoch 1475/2000 - training_loss: 6124.34609319\n",
      "Epoch 1476/2000 - training_loss: 12729.04049381\n",
      "Epoch 1477/2000 - training_loss: 2768.90585286\n",
      "Epoch 1478/2000 - training_loss: 7849.68079971\n",
      "Epoch 1479/2000 - training_loss: 40180.99866865\n",
      "Epoch 1480/2000 - training_loss: 1164401.95256060\n",
      "Epoch 1481/2000 - training_loss: 5131.40346871\n",
      "Epoch 1482/2000 - training_loss: 818894.19659637\n",
      "Epoch 1483/2000 - training_loss: 28638.01879508\n",
      "Epoch 1484/2000 - training_loss: 12205.01795847\n",
      "Epoch 1485/2000 - training_loss: 5902.61023000\n",
      "Epoch 1486/2000 - training_loss: 4590.32226371\n",
      "Epoch 1487/2000 - training_loss: 2390.23651234\n",
      "Epoch 1488/2000 - training_loss: 431401.43505574\n",
      "Epoch 1489/2000 - training_loss: 163437201.15798330\n",
      "Epoch 1490/2000 - training_loss: 57365.29001568\n",
      "Epoch 1491/2000 - training_loss: 47827.36279678\n",
      "Epoch 1492/2000 - training_loss: 7781.84162784\n",
      "Epoch 1493/2000 - training_loss: 2430.86202166\n",
      "Epoch 1494/2000 - training_loss: 5587.09701708\n",
      "Epoch 1495/2000 - training_loss: 7079.91015803\n",
      "Epoch 1496/2000 - training_loss: 12207.10107093\n",
      "Epoch 1497/2000 - training_loss: 23537.43132315\n",
      "Epoch 1498/2000 - training_loss: 33183.13185882\n",
      "Epoch 1499/2000 - training_loss: 4395.05191462\n",
      "Epoch 1500/2000 - training_loss: 2727.30957717\n",
      "Epoch 1501/2000 - training_loss: 3041.12392161\n",
      "Epoch 1502/2000 - training_loss: 252626.62685339\n",
      "Epoch 1503/2000 - training_loss: 2806.74432800\n",
      "Epoch 1504/2000 - training_loss: 6317.76691883\n",
      "Epoch 1505/2000 - training_loss: 76674.91081258\n",
      "Epoch 1506/2000 - training_loss: 75226.51453862\n",
      "Epoch 1507/2000 - training_loss: 3823.18873328\n",
      "Epoch 1508/2000 - training_loss: 6999.92458580\n",
      "Epoch 1509/2000 - training_loss: 9275.26814642\n",
      "Epoch 1510/2000 - training_loss: 73896.71886041\n",
      "Epoch 1511/2000 - training_loss: 39704.58071525\n",
      "Epoch 1512/2000 - training_loss: 10074.42479485\n",
      "Epoch 1513/2000 - training_loss: 4974.52488227\n",
      "Epoch 1514/2000 - training_loss: 6187.96832145\n",
      "Epoch 1515/2000 - training_loss: 32817.52756394\n",
      "Epoch 1516/2000 - training_loss: 13875.17416412\n",
      "Epoch 1517/2000 - training_loss: 3400.75464629\n",
      "Epoch 1518/2000 - training_loss: 2868.49874850\n",
      "Epoch 1519/2000 - training_loss: 4901.90821079\n",
      "Epoch 1520/2000 - training_loss: 48685.11444237\n",
      "Epoch 1521/2000 - training_loss: 15718.50488367\n",
      "Epoch 1522/2000 - training_loss: 4042.51672432\n",
      "Epoch 1523/2000 - training_loss: 6313.70325074\n",
      "Epoch 1524/2000 - training_loss: 2316361.61913186\n",
      "Epoch 1525/2000 - training_loss: 8055.15240352\n",
      "Epoch 1526/2000 - training_loss: 55435.48424609\n",
      "Epoch 1527/2000 - training_loss: 3594.51435209\n",
      "Epoch 1528/2000 - training_loss: 462700.36521831\n",
      "Epoch 1529/2000 - training_loss: 2777.71465601\n",
      "Epoch 1530/2000 - training_loss: 28708.02834110\n",
      "Epoch 1531/2000 - training_loss: 4619.03031621\n",
      "Epoch 1532/2000 - training_loss: 3721.83403399\n",
      "Epoch 1533/2000 - training_loss: 65698.69280002\n",
      "Epoch 1534/2000 - training_loss: 5097.21360263\n",
      "Epoch 1535/2000 - training_loss: 4315.04305915\n",
      "Epoch 1536/2000 - training_loss: 102418.28401428\n",
      "Epoch 1537/2000 - training_loss: 301292.55097610\n",
      "Epoch 1538/2000 - training_loss: 4410.77781537\n",
      "Epoch 1539/2000 - training_loss: 12795.70463033\n",
      "Epoch 1540/2000 - training_loss: 16800.72293791\n",
      "Epoch 1541/2000 - training_loss: 6242.79246381\n",
      "Epoch 1542/2000 - training_loss: 101870.76360115\n",
      "Epoch 1543/2000 - training_loss: 6820.09416861\n",
      "Epoch 1544/2000 - training_loss: 261633.98790246\n",
      "Epoch 1545/2000 - training_loss: 9776.61464735\n",
      "Epoch 1546/2000 - training_loss: 3839.71479182\n",
      "Epoch 1547/2000 - training_loss: 4902.37530551\n",
      "Epoch 1548/2000 - training_loss: 39127.57833535\n",
      "Epoch 1549/2000 - training_loss: 21012.32016354\n",
      "Epoch 1550/2000 - training_loss: 33367.37470595\n",
      "Epoch 1551/2000 - training_loss: 80507.52939193\n",
      "Epoch 1552/2000 - training_loss: 7906.89931443\n",
      "Epoch 1553/2000 - training_loss: 3858.11639183\n",
      "Epoch 1554/2000 - training_loss: 3054.39301525\n",
      "Epoch 1555/2000 - training_loss: 3237.57736454\n",
      "Epoch 1556/2000 - training_loss: 4444.30725496\n",
      "Epoch 1557/2000 - training_loss: 8585.40051472\n",
      "Epoch 1558/2000 - training_loss: 38814.64433972\n",
      "Epoch 1559/2000 - training_loss: 213054.08061360\n",
      "Epoch 1560/2000 - training_loss: 776628.15990496\n",
      "Epoch 1561/2000 - training_loss: 194210.29853662\n",
      "Epoch 1562/2000 - training_loss: 72673.94368676\n",
      "Epoch 1563/2000 - training_loss: 76963.04325094\n",
      "Epoch 1564/2000 - training_loss: 243647.67780723\n",
      "Epoch 1565/2000 - training_loss: 189840.14134191\n",
      "Epoch 1566/2000 - training_loss: 76530.51724721\n",
      "Epoch 1567/2000 - training_loss: 68096.64120609\n",
      "Epoch 1568/2000 - training_loss: 108211.70817044\n",
      "Epoch 1569/2000 - training_loss: 199772.82901885\n",
      "Epoch 1570/2000 - training_loss: 134395.21384284\n",
      "Epoch 1571/2000 - training_loss: 76473.17421881\n",
      "Epoch 1572/2000 - training_loss: 66334.23859667\n",
      "Epoch 1573/2000 - training_loss: 93949.39767974\n",
      "Epoch 1574/2000 - training_loss: 150561.69474953\n",
      "Epoch 1575/2000 - training_loss: 96950.35520080\n",
      "Epoch 1576/2000 - training_loss: 67166.63620150\n",
      "Epoch 1577/2000 - training_loss: 71282.48756336\n",
      "Epoch 1578/2000 - training_loss: 98773.06523912\n",
      "Epoch 1579/2000 - training_loss: 116369.62334349\n",
      "Epoch 1580/2000 - training_loss: 87948.85954840\n",
      "Epoch 1581/2000 - training_loss: 67235.59422897\n",
      "Epoch 1582/2000 - training_loss: 70056.19324934\n",
      "Epoch 1583/2000 - training_loss: 93251.38871493\n",
      "Epoch 1584/2000 - training_loss: 94377.78781733\n",
      "Epoch 1585/2000 - training_loss: 70752.27376025\n",
      "Epoch 1586/2000 - training_loss: 66462.73318963\n",
      "Epoch 1587/2000 - training_loss: 79624.73536544\n",
      "Epoch 1588/2000 - training_loss: 89343.00152874\n",
      "Epoch 1589/2000 - training_loss: 75699.50475303\n",
      "Epoch 1590/2000 - training_loss: 65604.22153358\n",
      "Epoch 1591/2000 - training_loss: 72410.77467819\n",
      "Epoch 1592/2000 - training_loss: 83130.32193581\n",
      "Epoch 1593/2000 - training_loss: 71744.11163577\n",
      "Epoch 1594/2000 - training_loss: 65679.26462730\n",
      "Epoch 1595/2000 - training_loss: 73688.51517002\n",
      "Epoch 1596/2000 - training_loss: 77565.79067902\n",
      "Epoch 1597/2000 - training_loss: 68165.35217170\n",
      "Epoch 1598/2000 - training_loss: 66697.91599465\n",
      "Epoch 1599/2000 - training_loss: 74966.04012649\n",
      "Epoch 1600/2000 - training_loss: 70208.32840634\n",
      "Epoch 1601/2000 - training_loss: 65719.14637964\n",
      "Epoch 1602/2000 - training_loss: 71792.32183216\n",
      "Epoch 1603/2000 - training_loss: 70803.03128301\n",
      "Epoch 1604/2000 - training_loss: 65524.72100637\n",
      "Epoch 1605/2000 - training_loss: 70402.95678706\n",
      "Epoch 1606/2000 - training_loss: 69169.14982958\n",
      "Epoch 1607/2000 - training_loss: 65780.80013141\n",
      "Epoch 1608/2000 - training_loss: 70439.69832479\n",
      "Epoch 1609/2000 - training_loss: 67030.61224549\n",
      "Epoch 1610/2000 - training_loss: 66886.59114012\n",
      "Epoch 1611/2000 - training_loss: 69336.15160827\n",
      "Epoch 1612/2000 - training_loss: 65558.06125750\n",
      "Epoch 1613/2000 - training_loss: 69162.82681725\n",
      "Epoch 1614/2000 - training_loss: 66002.59233921\n",
      "Epoch 1615/2000 - training_loss: 67946.50621394\n",
      "Epoch 1616/2000 - training_loss: 66455.38816606\n",
      "Epoch 1617/2000 - training_loss: 67221.29642730\n",
      "Epoch 1618/2000 - training_loss: 66670.87952788\n",
      "Epoch 1619/2000 - training_loss: 66955.57697870\n",
      "Epoch 1620/2000 - training_loss: 66211.31330382\n",
      "Epoch 1621/2000 - training_loss: 67221.29642730\n",
      "Epoch 1622/2000 - training_loss: 65673.92385983\n",
      "Epoch 1623/2000 - training_loss: 67548.15639166\n",
      "Epoch 1624/2000 - training_loss: 65899.27985289\n",
      "Epoch 1625/2000 - training_loss: 66351.22392430\n",
      "Epoch 1626/2000 - training_loss: 67285.44649078\n",
      "Epoch 1627/2000 - training_loss: 65897.74967682\n",
      "Epoch 1628/2000 - training_loss: 65816.88225720\n",
      "Epoch 1629/2000 - training_loss: 67162.30457745\n",
      "Epoch 1630/2000 - training_loss: 67908.43412892\n",
      "Epoch 1631/2000 - training_loss: 67516.69759293\n",
      "Epoch 1632/2000 - training_loss: 67548.17173955\n",
      "Epoch 1633/2000 - training_loss: 68256.86261544\n",
      "Epoch 1634/2000 - training_loss: 73903.22878921\n",
      "Epoch 1635/2000 - training_loss: 139708.65941393\n",
      "Epoch 1636/2000 - training_loss: 4748.12481208\n",
      "Epoch 1637/2000 - training_loss: 3215.87511969\n",
      "Epoch 1638/2000 - training_loss: 67987.01309785\n",
      "Epoch 1639/2000 - training_loss: 28466.53721585\n",
      "Epoch 1640/2000 - training_loss: 4292.64276864\n",
      "Epoch 1641/2000 - training_loss: 2895.98757768\n",
      "Epoch 1642/2000 - training_loss: 4208.22620770\n",
      "Epoch 1643/2000 - training_loss: 20076.00187179\n",
      "Epoch 1644/2000 - training_loss: 166640.13857168\n",
      "Epoch 1645/2000 - training_loss: 32134.93664747\n",
      "Epoch 1646/2000 - training_loss: 4500.93937442\n",
      "Epoch 1647/2000 - training_loss: 20637.86691044\n",
      "Epoch 1648/2000 - training_loss: 23298.92019529\n",
      "Epoch 1649/2000 - training_loss: 23292.81998220\n",
      "Epoch 1650/2000 - training_loss: 3280.37942428\n",
      "Epoch 1651/2000 - training_loss: 18202.87953069\n",
      "Epoch 1652/2000 - training_loss: 7205.08466882\n",
      "Epoch 1653/2000 - training_loss: 3567.67434568\n",
      "Epoch 1654/2000 - training_loss: 398414.31421180\n",
      "Epoch 1655/2000 - training_loss: 3460.81575026\n",
      "Epoch 1656/2000 - training_loss: 10847.85408821\n",
      "Epoch 1657/2000 - training_loss: 40689.90115889\n",
      "Epoch 1658/2000 - training_loss: 10609.61286708\n",
      "Epoch 1659/2000 - training_loss: 12378.86542368\n",
      "Epoch 1660/2000 - training_loss: 3625.52639392\n",
      "Epoch 1661/2000 - training_loss: 11144.14366954\n",
      "Epoch 1662/2000 - training_loss: 35313.16984274\n",
      "Epoch 1663/2000 - training_loss: 1180706.76880161\n",
      "Epoch 1664/2000 - training_loss: 4360.71184975\n",
      "Epoch 1665/2000 - training_loss: 28695.71853366\n",
      "Epoch 1666/2000 - training_loss: 4933.39552730\n",
      "Epoch 1667/2000 - training_loss: 125328.07920890\n",
      "Epoch 1668/2000 - training_loss: 3627.09184671\n",
      "Epoch 1669/2000 - training_loss: 2633603.41345278\n",
      "Epoch 1670/2000 - training_loss: 3020.16713048\n",
      "Epoch 1671/2000 - training_loss: 8680.59728009\n",
      "Epoch 1672/2000 - training_loss: 3106.80301021\n",
      "Epoch 1673/2000 - training_loss: 300921.22293336\n",
      "Epoch 1674/2000 - training_loss: 24233.54386377\n",
      "Epoch 1675/2000 - training_loss: 369733.90845400\n",
      "Epoch 1676/2000 - training_loss: 8501.95062671\n",
      "Epoch 1677/2000 - training_loss: 6689.33426591\n",
      "Epoch 1678/2000 - training_loss: 5148.18777529\n",
      "Epoch 1679/2000 - training_loss: 4173.78007333\n",
      "Epoch 1680/2000 - training_loss: 6806.51805002\n",
      "Epoch 1681/2000 - training_loss: 138713.63200119\n",
      "Epoch 1682/2000 - training_loss: 3574.76506388\n",
      "Epoch 1683/2000 - training_loss: 11923470.70999236\n",
      "Epoch 1684/2000 - training_loss: 4298.23193911\n",
      "Epoch 1685/2000 - training_loss: 6915.18396374\n",
      "Epoch 1686/2000 - training_loss: 8523438.60229006\n",
      "Epoch 1687/2000 - training_loss: 148121.91708245\n",
      "Epoch 1688/2000 - training_loss: 25235.89056922\n",
      "Epoch 1689/2000 - training_loss: 6683.18483535\n",
      "Epoch 1690/2000 - training_loss: 4043.41253553\n",
      "Epoch 1691/2000 - training_loss: 3412.69826817\n",
      "Epoch 1692/2000 - training_loss: 3616.93176784\n",
      "Epoch 1693/2000 - training_loss: 4750.40047438\n",
      "Epoch 1694/2000 - training_loss: 8183.43838257\n",
      "Epoch 1695/2000 - training_loss: 22964.23528456\n",
      "Epoch 1696/2000 - training_loss: 407475.21845096\n",
      "Epoch 1697/2000 - training_loss: 893820.94213530\n",
      "Epoch 1698/2000 - training_loss: 14980.71348278\n",
      "Epoch 1699/2000 - training_loss: 39034139.49619108\n",
      "Epoch 1700/2000 - training_loss: 5124.87304804\n",
      "Epoch 1701/2000 - training_loss: 84115.90663403\n",
      "Epoch 1702/2000 - training_loss: 115585.21936267\n",
      "Epoch 1703/2000 - training_loss: 32864.49354773\n",
      "Epoch 1704/2000 - training_loss: 3667.69177674\n",
      "Epoch 1705/2000 - training_loss: 6532.33808435\n",
      "Epoch 1706/2000 - training_loss: 17939.41239007\n",
      "Epoch 1707/2000 - training_loss: 12638.36368576\n",
      "Epoch 1708/2000 - training_loss: 86529.25928040\n",
      "Epoch 1709/2000 - training_loss: 28790.12781653\n",
      "Epoch 1710/2000 - training_loss: 1346302.95207070\n",
      "Epoch 1711/2000 - training_loss: 17959.31644037\n",
      "Epoch 1712/2000 - training_loss: 5577.11674001\n",
      "Epoch 1713/2000 - training_loss: 3693.24627735\n",
      "Epoch 1714/2000 - training_loss: 3938398.02993650\n",
      "Epoch 1715/2000 - training_loss: 17181.20420818\n",
      "Epoch 1716/2000 - training_loss: 349702.71180376\n",
      "Epoch 1717/2000 - training_loss: 67221.57420023\n",
      "Epoch 1718/2000 - training_loss: 10947.71506374\n",
      "Epoch 1719/2000 - training_loss: 11604.72134493\n",
      "Epoch 1720/2000 - training_loss: 6722.29570088\n",
      "Epoch 1721/2000 - training_loss: 19689.06763418\n",
      "Epoch 1722/2000 - training_loss: 37160.21270678\n",
      "Epoch 1723/2000 - training_loss: 6767.76900991\n",
      "Epoch 1724/2000 - training_loss: 4373.16308389\n",
      "Epoch 1725/2000 - training_loss: 59491.78525570\n",
      "Epoch 1726/2000 - training_loss: 20636871.15443591\n",
      "Epoch 1727/2000 - training_loss: 4338.28806721\n",
      "Epoch 1728/2000 - training_loss: 18919.99714416\n",
      "Epoch 1729/2000 - training_loss: 3915.36412522\n",
      "Epoch 1730/2000 - training_loss: 233760.25487748\n",
      "Epoch 1731/2000 - training_loss: 4416.67894572\n",
      "Epoch 1732/2000 - training_loss: 6379.75902890\n",
      "Epoch 1733/2000 - training_loss: 52493.10478352\n",
      "Epoch 1734/2000 - training_loss: 12918.02174798\n",
      "Epoch 1735/2000 - training_loss: 4214.18440233\n",
      "Epoch 1736/2000 - training_loss: 10157.43190799\n",
      "Epoch 1737/2000 - training_loss: 175576.51360899\n",
      "Epoch 1738/2000 - training_loss: 15459.93431253\n",
      "Epoch 1739/2000 - training_loss: 101433.09614479\n",
      "Epoch 1740/2000 - training_loss: 7092.95039526\n",
      "Epoch 1741/2000 - training_loss: 5341.43598368\n",
      "Epoch 1742/2000 - training_loss: 9457.74390966\n",
      "Epoch 1743/2000 - training_loss: 65830.83858819\n",
      "Epoch 1744/2000 - training_loss: 101026.16353465\n",
      "Epoch 1745/2000 - training_loss: 13177.04574414\n",
      "Epoch 1746/2000 - training_loss: 7941.96958524\n",
      "Epoch 1747/2000 - training_loss: 9699.29445349\n",
      "Epoch 1748/2000 - training_loss: 25773.59957514\n",
      "Epoch 1749/2000 - training_loss: 1065236.78826514\n",
      "Epoch 1750/2000 - training_loss: 64993.88320511\n",
      "Epoch 1751/2000 - training_loss: 16465.41938444\n",
      "Epoch 1752/2000 - training_loss: 9408.85500580\n",
      "Epoch 1753/2000 - training_loss: 7561.63735818\n",
      "Epoch 1754/2000 - training_loss: 7463.02622911\n",
      "Epoch 1755/2000 - training_loss: 8567.70037446\n",
      "Epoch 1756/2000 - training_loss: 11218.48713542\n",
      "Epoch 1757/2000 - training_loss: 16838.13309808\n",
      "Epoch 1758/2000 - training_loss: 29977.44629398\n",
      "Epoch 1759/2000 - training_loss: 70124.31051094\n",
      "Epoch 1760/2000 - training_loss: 308529.66563029\n",
      "Epoch 1761/2000 - training_loss: 239336732.38902560\n",
      "Epoch 1762/2000 - training_loss: 5878.31634822\n",
      "Epoch 1763/2000 - training_loss: 12112.62453668\n",
      "Epoch 1764/2000 - training_loss: 71665.20384924\n",
      "Epoch 1765/2000 - training_loss: 12470.61166572\n",
      "Epoch 1766/2000 - training_loss: 69601.78327579\n",
      "Epoch 1767/2000 - training_loss: 120297.28372113\n",
      "Epoch 1768/2000 - training_loss: 15547.12469782\n",
      "Epoch 1769/2000 - training_loss: 7626.28647695\n",
      "Epoch 1770/2000 - training_loss: 5453.11772340\n",
      "Epoch 1771/2000 - training_loss: 4643.55086641\n",
      "Epoch 1772/2000 - training_loss: 4355.71752174\n",
      "Epoch 1773/2000 - training_loss: 4331.10546693\n",
      "Epoch 1774/2000 - training_loss: 4469.00941341\n",
      "Epoch 1775/2000 - training_loss: 4724.11167466\n",
      "Epoch 1776/2000 - training_loss: 5073.24369731\n",
      "Epoch 1777/2000 - training_loss: 5505.07666351\n",
      "Epoch 1778/2000 - training_loss: 6012.54435834\n",
      "Epoch 1779/2000 - training_loss: 6591.74919062\n",
      "Epoch 1780/2000 - training_loss: 7242.36987340\n",
      "Epoch 1781/2000 - training_loss: 7960.25421787\n",
      "Epoch 1782/2000 - training_loss: 8745.50459425\n",
      "Epoch 1783/2000 - training_loss: 9598.76056381\n",
      "Epoch 1784/2000 - training_loss: 10513.13516217\n",
      "Epoch 1785/2000 - training_loss: 11489.86630163\n",
      "Epoch 1786/2000 - training_loss: 12529.44118382\n",
      "Epoch 1787/2000 - training_loss: 13622.88145650\n",
      "Epoch 1788/2000 - training_loss: 14775.40941878\n",
      "Epoch 1789/2000 - training_loss: 15981.31920575\n",
      "Epoch 1790/2000 - training_loss: 17233.56151588\n",
      "Epoch 1791/2000 - training_loss: 18526.90510117\n",
      "Epoch 1792/2000 - training_loss: 19856.97579257\n",
      "Epoch 1793/2000 - training_loss: 21222.47274136\n",
      "Epoch 1794/2000 - training_loss: 22633.10040390\n",
      "Epoch 1795/2000 - training_loss: 24057.05930882\n",
      "Epoch 1796/2000 - training_loss: 25497.93247168\n",
      "Epoch 1797/2000 - training_loss: 26965.29259183\n",
      "Epoch 1798/2000 - training_loss: 28444.27425825\n",
      "Epoch 1799/2000 - training_loss: 29939.95001148\n",
      "Epoch 1800/2000 - training_loss: 31424.02896290\n",
      "Epoch 1801/2000 - training_loss: 32919.02657849\n",
      "Epoch 1802/2000 - training_loss: 34395.74440325\n",
      "Epoch 1803/2000 - training_loss: 35884.92624239\n",
      "Epoch 1804/2000 - training_loss: 37359.07104186\n",
      "Epoch 1805/2000 - training_loss: 38784.80085532\n",
      "Epoch 1806/2000 - training_loss: 40221.57761531\n",
      "Epoch 1807/2000 - training_loss: 41620.84332088\n",
      "Epoch 1808/2000 - training_loss: 43034.42039611\n",
      "Epoch 1809/2000 - training_loss: 44387.33362632\n",
      "Epoch 1810/2000 - training_loss: 45737.36021131\n",
      "Epoch 1811/2000 - training_loss: 47052.42722357\n",
      "Epoch 1812/2000 - training_loss: 48323.07546980\n",
      "Epoch 1813/2000 - training_loss: 49566.18933163\n",
      "Epoch 1814/2000 - training_loss: 50804.07704604\n",
      "Epoch 1815/2000 - training_loss: 51972.67878566\n",
      "Epoch 1816/2000 - training_loss: 53122.80784190\n",
      "Epoch 1817/2000 - training_loss: 54218.60417630\n",
      "Epoch 1818/2000 - training_loss: 55317.61391699\n",
      "Epoch 1819/2000 - training_loss: 56384.73303865\n",
      "Epoch 1820/2000 - training_loss: 57380.26266733\n",
      "Epoch 1821/2000 - training_loss: 58368.06218826\n",
      "Epoch 1822/2000 - training_loss: 59309.65363338\n",
      "Epoch 1823/2000 - training_loss: 60237.98907242\n",
      "Epoch 1824/2000 - training_loss: 61112.54339259\n",
      "Epoch 1825/2000 - training_loss: 61967.90645482\n",
      "Epoch 1826/2000 - training_loss: 62721.94614741\n",
      "Epoch 1827/2000 - training_loss: 63531.41405374\n",
      "Epoch 1828/2000 - training_loss: 64357.32217148\n",
      "Epoch 1829/2000 - training_loss: 65030.59698993\n",
      "Epoch 1830/2000 - training_loss: 65757.91326920\n",
      "Epoch 1831/2000 - training_loss: 66366.72231467\n",
      "Epoch 1832/2000 - training_loss: 67028.70768677\n",
      "Epoch 1833/2000 - training_loss: 67655.84367671\n",
      "Epoch 1834/2000 - training_loss: 68201.17113334\n",
      "Epoch 1835/2000 - training_loss: 68799.47205941\n",
      "Epoch 1836/2000 - training_loss: 69312.34696108\n",
      "Epoch 1837/2000 - training_loss: 69783.72460501\n",
      "Epoch 1838/2000 - training_loss: 70260.31263957\n",
      "Epoch 1839/2000 - training_loss: 70789.90386646\n",
      "Epoch 1840/2000 - training_loss: 71228.26123037\n",
      "Epoch 1841/2000 - training_loss: 71671.10011450\n",
      "Epoch 1842/2000 - training_loss: 72067.89258322\n",
      "Epoch 1843/2000 - training_loss: 72418.32682675\n",
      "Epoch 1844/2000 - training_loss: 72822.07358728\n",
      "Epoch 1845/2000 - training_loss: 73178.04734570\n",
      "Epoch 1846/2000 - training_loss: 73485.59069639\n",
      "Epoch 1847/2000 - training_loss: 73846.38019096\n",
      "Epoch 1848/2000 - training_loss: 74106.39067724\n",
      "Epoch 1849/2000 - training_loss: 74367.41381946\n",
      "Epoch 1850/2000 - training_loss: 74682.27062087\n",
      "Epoch 1851/2000 - training_loss: 74946.90580094\n",
      "Epoch 1852/2000 - training_loss: 75212.44422835\n",
      "Epoch 1853/2000 - training_loss: 75426.45368784\n",
      "Epoch 1854/2000 - training_loss: 75640.78158992\n",
      "Epoch 1855/2000 - training_loss: 75802.51374873\n",
      "Epoch 1856/2000 - training_loss: 76018.55544219\n",
      "Epoch 1857/2000 - training_loss: 76181.88604710\n",
      "Epoch 1858/2000 - training_loss: 76399.63792325\n",
      "Epoch 1859/2000 - training_loss: 76563.52506062\n",
      "Epoch 1860/2000 - training_loss: 76728.38850118\n",
      "Epoch 1861/2000 - training_loss: 76893.76050410\n",
      "Epoch 1862/2000 - training_loss: 77059.42996778\n",
      "Epoch 1863/2000 - training_loss: 77115.02979234\n",
      "Epoch 1864/2000 - training_loss: 77225.79759684\n",
      "Epoch 1865/2000 - training_loss: 77392.66082952\n",
      "Epoch 1866/2000 - training_loss: 77504.52495318\n",
      "Epoch 1867/2000 - training_loss: 77560.52335524\n",
      "Epoch 1868/2000 - training_loss: 77728.43250220\n",
      "Epoch 1869/2000 - training_loss: 77784.54690676\n",
      "Epoch 1870/2000 - training_loss: 77896.90909589\n",
      "Epoch 1871/2000 - training_loss: 77953.28074852\n",
      "Epoch 1872/2000 - training_loss: 78009.61166013\n",
      "Epoch 1873/2000 - training_loss: 78066.26956729\n",
      "Epoch 1874/2000 - training_loss: 78122.70482637\n",
      "Epoch 1875/2000 - training_loss: 78179.56001807\n",
      "Epoch 1876/2000 - training_loss: 78179.57615308\n",
      "Epoch 1877/2000 - training_loss: 78292.76152426\n",
      "Epoch 1878/2000 - training_loss: 78292.69774914\n",
      "Epoch 1879/2000 - training_loss: 78292.69774914\n",
      "Epoch 1880/2000 - training_loss: 78349.62136669\n",
      "Epoch 1881/2000 - training_loss: 78349.62136669\n",
      "Epoch 1882/2000 - training_loss: 78406.32653140\n",
      "Epoch 1883/2000 - training_loss: 78406.32653140\n",
      "Epoch 1884/2000 - training_loss: 78406.32653140\n",
      "Epoch 1885/2000 - training_loss: 78406.32653140\n",
      "Epoch 1886/2000 - training_loss: 78406.32653140\n",
      "Epoch 1887/2000 - training_loss: 78406.32653140\n",
      "Epoch 1888/2000 - training_loss: 78349.62136669\n",
      "Epoch 1889/2000 - training_loss: 78349.62136669\n",
      "Epoch 1890/2000 - training_loss: 78292.69774914\n",
      "Epoch 1891/2000 - training_loss: 78292.76152426\n",
      "Epoch 1892/2000 - training_loss: 78236.10390699\n",
      "Epoch 1893/2000 - training_loss: 78179.57615308\n",
      "Epoch 1894/2000 - training_loss: 78179.56001807\n",
      "Epoch 1895/2000 - training_loss: 78066.27485497\n",
      "Epoch 1896/2000 - training_loss: 78066.26956729\n",
      "Epoch 1897/2000 - training_loss: 78009.61166013\n",
      "Epoch 1898/2000 - training_loss: 77953.28074852\n",
      "Epoch 1899/2000 - training_loss: 77840.60479511\n",
      "Epoch 1900/2000 - training_loss: 77784.54690676\n",
      "Epoch 1901/2000 - training_loss: 77672.42372754\n",
      "Epoch 1902/2000 - training_loss: 77560.52335524\n",
      "Epoch 1903/2000 - training_loss: 77448.39852988\n",
      "Epoch 1904/2000 - training_loss: 77336.96763628\n",
      "Epoch 1905/2000 - training_loss: 77225.87420624\n",
      "Epoch 1906/2000 - training_loss: 77115.05491658\n",
      "Epoch 1907/2000 - training_loss: 77004.15455127\n",
      "Epoch 1908/2000 - training_loss: 76838.67115722\n",
      "Epoch 1909/2000 - training_loss: 76673.50772166\n",
      "Epoch 1910/2000 - training_loss: 76563.58789646\n",
      "Epoch 1911/2000 - training_loss: 76399.63792325\n",
      "Epoch 1912/2000 - training_loss: 76181.95268057\n",
      "Epoch 1913/2000 - training_loss: 76018.63291366\n",
      "Epoch 1914/2000 - training_loss: 75748.57935587\n",
      "Epoch 1915/2000 - training_loss: 75587.03328706\n",
      "Epoch 1916/2000 - training_loss: 75372.98832269\n",
      "Epoch 1917/2000 - training_loss: 75159.12873127\n",
      "Epoch 1918/2000 - training_loss: 74893.93900263\n",
      "Epoch 1919/2000 - training_loss: 74682.35494476\n",
      "Epoch 1920/2000 - training_loss: 74315.20162717\n",
      "Epoch 1921/2000 - training_loss: 74106.37678366\n",
      "Epoch 1922/2000 - training_loss: 73794.70183937\n",
      "Epoch 1923/2000 - training_loss: 73434.24866095\n",
      "Epoch 1924/2000 - training_loss: 73127.05230994\n",
      "Epoch 1925/2000 - training_loss: 72822.07358728\n",
      "Epoch 1926/2000 - training_loss: 72418.32682675\n",
      "Epoch 1927/2000 - training_loss: 72067.89258322\n",
      "Epoch 1928/2000 - training_loss: 71671.10011450\n",
      "Epoch 1929/2000 - training_loss: 71228.26123037\n",
      "Epoch 1930/2000 - training_loss: 70789.90386646\n",
      "Epoch 1931/2000 - training_loss: 70356.14583753\n",
      "Epoch 1932/2000 - training_loss: 69831.12235488\n",
      "Epoch 1933/2000 - training_loss: 69359.17214171\n",
      "Epoch 1934/2000 - training_loss: 68845.70131404\n",
      "Epoch 1935/2000 - training_loss: 68292.46310723\n",
      "Epoch 1936/2000 - training_loss: 67746.14947639\n",
      "Epoch 1937/2000 - training_loss: 67162.22861798\n",
      "Epoch 1938/2000 - training_loss: 66541.97159653\n",
      "Epoch 1939/2000 - training_loss: 65887.41161313\n",
      "Epoch 1940/2000 - training_loss: 65158.16851035\n",
      "Epoch 1941/2000 - training_loss: 64482.94009481\n",
      "Epoch 1942/2000 - training_loss: 63736.41144003\n",
      "Epoch 1943/2000 - training_loss: 62962.99794771\n",
      "Epoch 1944/2000 - training_loss: 62164.85332700\n",
      "Epoch 1945/2000 - training_loss: 61343.93705924\n",
      "Epoch 1946/2000 - training_loss: 60464.45859605\n",
      "Epoch 1947/2000 - training_loss: 59530.64888597\n",
      "Epoch 1948/2000 - training_loss: 58619.41689516\n",
      "Epoch 1949/2000 - training_loss: 57624.47288320\n",
      "Epoch 1950/2000 - training_loss: 56588.32414210\n",
      "Epoch 1951/2000 - training_loss: 55548.24096265\n",
      "Epoch 1952/2000 - training_loss: 54409.82892748\n",
      "Epoch 1953/2000 - training_loss: 53246.21743045\n",
      "Epoch 1954/2000 - training_loss: 52061.78374189\n",
      "Epoch 1955/2000 - training_loss: 50861.43024783\n",
      "Epoch 1956/2000 - training_loss: 49593.74838099\n",
      "Epoch 1957/2000 - training_loss: 48296.38684702\n",
      "Epoch 1958/2000 - training_loss: 47001.62957357\n",
      "Epoch 1959/2000 - training_loss: 45664.39345930\n",
      "Epoch 1960/2000 - training_loss: 44294.96631526\n",
      "Epoch 1961/2000 - training_loss: 42880.74823999\n",
      "Epoch 1962/2000 - training_loss: 41516.78097480\n",
      "Epoch 1963/2000 - training_loss: 40142.74414119\n",
      "Epoch 1964/2000 - training_loss: 38747.65829387\n",
      "Epoch 1965/2000 - training_loss: 37359.17217182\n",
      "Epoch 1966/2000 - training_loss: 36000.12897915\n",
      "Epoch 1967/2000 - training_loss: 34641.95667768\n",
      "Epoch 1968/2000 - training_loss: 33321.98325692\n",
      "Epoch 1969/2000 - training_loss: 32000.51222588\n",
      "Epoch 1970/2000 - training_loss: 30724.38726457\n",
      "Epoch 1971/2000 - training_loss: 29470.58476391\n",
      "Epoch 1972/2000 - training_loss: 28243.77335619\n",
      "Epoch 1973/2000 - training_loss: 27037.29232053\n",
      "Epoch 1974/2000 - training_loss: 25866.39257094\n",
      "Epoch 1975/2000 - training_loss: 24733.24613917\n",
      "Epoch 1976/2000 - training_loss: 23639.76171813\n",
      "Epoch 1977/2000 - training_loss: 22564.76871425\n",
      "Epoch 1978/2000 - training_loss: 21534.35898084\n",
      "Epoch 1979/2000 - training_loss: 20536.40611700\n",
      "Epoch 1980/2000 - training_loss: 19579.28950552\n",
      "Epoch 1981/2000 - training_loss: 18652.37398523\n",
      "Epoch 1982/2000 - training_loss: 17763.71038363\n",
      "Epoch 1983/2000 - training_loss: 16909.32064358\n",
      "Epoch 1984/2000 - training_loss: 16090.75425290\n",
      "Epoch 1985/2000 - training_loss: 15304.93010308\n",
      "Epoch 1986/2000 - training_loss: 14553.11713411\n",
      "Epoch 1987/2000 - training_loss: 13836.19439492\n",
      "Epoch 1988/2000 - training_loss: 13157.43296609\n",
      "Epoch 1989/2000 - training_loss: 12510.73749673\n",
      "Epoch 1990/2000 - training_loss: 11891.38819583\n",
      "Epoch 1991/2000 - training_loss: 11309.47332950\n",
      "Epoch 1992/2000 - training_loss: 10757.55503809\n",
      "Epoch 1993/2000 - training_loss: 10231.59288717\n",
      "Epoch 1994/2000 - training_loss: 9737.50614978\n",
      "Epoch 1995/2000 - training_loss: 9269.45164391\n",
      "Epoch 1996/2000 - training_loss: 8831.39895929\n",
      "Epoch 1997/2000 - training_loss: 8418.62123275\n",
      "Epoch 1998/2000 - training_loss: 8031.52643644\n",
      "Epoch 1999/2000 - training_loss: 7667.44072292\n",
      "Epoch 2000/2000 - training_loss: 7327.69030365\n"
     ]
    }
   ],
   "source": [
    "N = 2000\n",
    "loss = np.zeros(N)\n",
    "for epoch in range (N):\n",
    "    train_loss = train_step(X_seq, X, PINN, train=True)\n",
    "    loss[epoch] = train_loss\n",
    "    print(f\"Epoch {epoch + 1}/{N} - training_loss: {train_loss:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2, x3 = DataPreprocessing(data, ff=0.005)\n",
    "x_seq = transform_sequence(tf.concat([tf.reshape(x1[:,1], (-1,1)),x2], axis = 1), 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(994, 8, 1), dtype=float32, numpy=\n",
       "array([[[0.99706596],\n",
       "        [0.9999963 ],\n",
       "        [0.99999833],\n",
       "        ...,\n",
       "        [0.99999785],\n",
       "        [0.99999785],\n",
       "        [0.99999785]],\n",
       "\n",
       "       [[0.9970758 ],\n",
       "        [0.9999963 ],\n",
       "        [0.99999833],\n",
       "        ...,\n",
       "        [0.99999785],\n",
       "        [0.99999785],\n",
       "        [0.99999785]],\n",
       "\n",
       "       [[0.99708575],\n",
       "        [0.9999964 ],\n",
       "        [0.99999833],\n",
       "        ...,\n",
       "        [0.99999785],\n",
       "        [0.99999785],\n",
       "        [0.99999785]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.9995572 ],\n",
       "        [0.999998  ],\n",
       "        [0.99999845],\n",
       "        ...,\n",
       "        [0.99999785],\n",
       "        [0.99999785],\n",
       "        [0.99999785]],\n",
       "\n",
       "       [[0.9995577 ],\n",
       "        [0.999998  ],\n",
       "        [0.99999845],\n",
       "        ...,\n",
       "        [0.99999785],\n",
       "        [0.99999785],\n",
       "        [0.99999785]],\n",
       "\n",
       "       [[0.99955803],\n",
       "        [0.999998  ],\n",
       "        [0.99999845],\n",
       "        ...,\n",
       "        [0.99999785],\n",
       "        [0.99999785],\n",
       "        [0.99999785]]], dtype=float32)>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_pred = PINN(x_seq)\n",
    "f_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.9998629>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(f_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
